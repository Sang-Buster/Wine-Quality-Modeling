[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Introduction to Statistical Learning",
    "section": "",
    "text": "Welcome ðŸ‘‹! This is Group 6â€™s Final Project, and here is our detailed work\nConsider the wine quality dataset from UCI Machine Learning Respository1. We will focus only on the data concerning white wines (and not red wines). Dichotomize the quality variable as good, which takes the value 1 if quality \\(\\geq\\) 7 axnd the value 0, otherwise. We will take good as response and all the 11 physiochemical characteristics of the wines in the data as predictors.\n\n\nUse 10-fold cross-validation for estimating the test error rates below and compute the estimates using caret package with seed set to 1234 before each computation.\n\nFit a KNN with K chosen optimally using test error rate. Report error rate, sensitivity, specificity, and AUC for the optimal KNN based on the training data. Also, report its estimated test error rate.\nRepeat (a) using logistic regression.\nRepeat (a) using LDA.\nRepeat (a) using QDA.\nCompare the results in (a)-(d). Which classifier would you recommend? Justify your answer.\n\n\n\n\n\nKNN\nGLM/logit/glmnet\nLDA\nQDA\nRegression/Multi-Regression\nNaive Bayes\nDecision Tree\nClassification Tree\nRegression Tree\nClassification and Regression Trees (CART)\nIterative Dichotomiser 3 (ID3)\nC4.5\nRandom Forest\nClassification\nRegression\nBagged\nBoosted\nGradient Boosted Trees (GBT)\nExtremely Randomized Trees (ExtraTrees)\nSVM\n\n\n\n\nThis is a dataset of wine quality containing 4898 observations of 12 variables. The variables are:\n\nfixed.acidity: The amount of fixed acid in the wine (\\(g/dm^3\\))\nvolatile.acidity: The amount of volatile acid in the wine (\\(g/dm^4\\))\ncitric.acid: The amount of citric acid in the wine (\\(g/dm^3\\))\nresidual.sugar: The amount of residual sugar in the wine (\\(g/dm^3\\))\nchlorides: The amount of salt in the wine (\\(g/dm^3\\))\nfree.sulfur.dioxide: The amount of free sulfur dioxide in the wine (\\(mg/dm^3\\))\ntotal.sulfur.dioxide: The amount of total sulfur dioxide in the wine (\\(mg/dm^3\\))\ndensity: The density of the wine (\\(g/dm^3\\))\npH: The \\(pH\\) value of the wine\nsulphates: The amount of sulphates in the wine (\\(g/dm^3\\))\nalcohol: The alcohol content of the wine (\\(\\% vol\\))\nquality: The quality score of the wine (0-10)\n\nAfter removing the duplicate rows from our data set, we are left with 3961 observations of the above 11 variables minus quality column variable, and introduced a new variable good as our response:\n\ngood: A binary variable indicating whether the wine is good (quality \\(\\geq\\) 7) or not (quality \\(&lt;\\) 7)."
  },
  {
    "objectID": "index.html#preamble",
    "href": "index.html#preamble",
    "title": "Introduction to Statistical Learning",
    "section": "",
    "text": "Welcome ðŸ‘‹! This is Group 6â€™s Final Project, and here is our detailed work\nConsider the wine quality dataset from UCI Machine Learning Respository1. We will focus only on the data concerning white wines (and not red wines). Dichotomize the quality variable as good, which takes the value 1 if quality \\(\\geq\\) 7 axnd the value 0, otherwise. We will take good as response and all the 11 physiochemical characteristics of the wines in the data as predictors.\n\n\nUse 10-fold cross-validation for estimating the test error rates below and compute the estimates using caret package with seed set to 1234 before each computation.\n\nFit a KNN with K chosen optimally using test error rate. Report error rate, sensitivity, specificity, and AUC for the optimal KNN based on the training data. Also, report its estimated test error rate.\nRepeat (a) using logistic regression.\nRepeat (a) using LDA.\nRepeat (a) using QDA.\nCompare the results in (a)-(d). Which classifier would you recommend? Justify your answer.\n\n\n\n\n\nKNN\nGLM/logit/glmnet\nLDA\nQDA\nRegression/Multi-Regression\nNaive Bayes\nDecision Tree\nClassification Tree\nRegression Tree\nClassification and Regression Trees (CART)\nIterative Dichotomiser 3 (ID3)\nC4.5\nRandom Forest\nClassification\nRegression\nBagged\nBoosted\nGradient Boosted Trees (GBT)\nExtremely Randomized Trees (ExtraTrees)\nSVM\n\n\n\n\nThis is a dataset of wine quality containing 4898 observations of 12 variables. The variables are:\n\nfixed.acidity: The amount of fixed acid in the wine (\\(g/dm^3\\))\nvolatile.acidity: The amount of volatile acid in the wine (\\(g/dm^4\\))\ncitric.acid: The amount of citric acid in the wine (\\(g/dm^3\\))\nresidual.sugar: The amount of residual sugar in the wine (\\(g/dm^3\\))\nchlorides: The amount of salt in the wine (\\(g/dm^3\\))\nfree.sulfur.dioxide: The amount of free sulfur dioxide in the wine (\\(mg/dm^3\\))\ntotal.sulfur.dioxide: The amount of total sulfur dioxide in the wine (\\(mg/dm^3\\))\ndensity: The density of the wine (\\(g/dm^3\\))\npH: The \\(pH\\) value of the wine\nsulphates: The amount of sulphates in the wine (\\(g/dm^3\\))\nalcohol: The alcohol content of the wine (\\(\\% vol\\))\nquality: The quality score of the wine (0-10)\n\nAfter removing the duplicate rows from our data set, we are left with 3961 observations of the above 11 variables minus quality column variable, and introduced a new variable good as our response:\n\ngood: A binary variable indicating whether the wine is good (quality \\(\\geq\\) 7) or not (quality \\(&lt;\\) 7)."
  },
  {
    "objectID": "index.html#footnotes",
    "href": "index.html#footnotes",
    "title": "Introduction to Statistical Learning",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nP. Cortez, A. Cerdeira, F. Almeida, T. Matos and J. Reis. Modeling wine preferences by data mining from physicochemical properties. In Decision Support Systems, Elsevier, 47(4):547-553, 2009.â†©ï¸Ž"
  },
  {
    "objectID": "src/analysis.html",
    "href": "src/analysis.html",
    "title": "Exploratory Analysis",
    "section": "",
    "text": "Show/Hide Code\nwine.data &lt;- read.csv(\"dataset\\\\winequality-white.csv\", sep=\";\", header = T)\n\nwine.data &lt;-  wine.data %&gt;% mutate(good = ifelse(quality&gt;=7, 1, 0))\n\nhead(wine.data)\n\n\n  fixed.acidity volatile.acidity citric.acid residual.sugar chlorides\n1           7.0             0.27        0.36           20.7     0.045\n2           6.3             0.30        0.34            1.6     0.049\n3           8.1             0.28        0.40            6.9     0.050\n4           7.2             0.23        0.32            8.5     0.058\n5           7.2             0.23        0.32            8.5     0.058\n6           8.1             0.28        0.40            6.9     0.050\n  free.sulfur.dioxide total.sulfur.dioxide density   pH sulphates alcohol\n1                  45                  170  1.0010 3.00      0.45     8.8\n2                  14                  132  0.9940 3.30      0.49     9.5\n3                  30                   97  0.9951 3.26      0.44    10.1\n4                  47                  186  0.9956 3.19      0.40     9.9\n5                  47                  186  0.9956 3.19      0.40     9.9\n6                  30                   97  0.9951 3.26      0.44    10.1\n  quality good\n1       6    0\n2       6    0\n3       6    0\n4       6    0\n5       6    0\n6       6    0"
  },
  {
    "objectID": "src/analysis.html#data-import",
    "href": "src/analysis.html#data-import",
    "title": "Exploratory Analysis",
    "section": "",
    "text": "Show/Hide Code\nwine.data &lt;- read.csv(\"dataset\\\\winequality-white.csv\", sep=\";\", header = T)\n\nwine.data &lt;-  wine.data %&gt;% mutate(good = ifelse(quality&gt;=7, 1, 0))\n\nhead(wine.data)\n\n\n  fixed.acidity volatile.acidity citric.acid residual.sugar chlorides\n1           7.0             0.27        0.36           20.7     0.045\n2           6.3             0.30        0.34            1.6     0.049\n3           8.1             0.28        0.40            6.9     0.050\n4           7.2             0.23        0.32            8.5     0.058\n5           7.2             0.23        0.32            8.5     0.058\n6           8.1             0.28        0.40            6.9     0.050\n  free.sulfur.dioxide total.sulfur.dioxide density   pH sulphates alcohol\n1                  45                  170  1.0010 3.00      0.45     8.8\n2                  14                  132  0.9940 3.30      0.49     9.5\n3                  30                   97  0.9951 3.26      0.44    10.1\n4                  47                  186  0.9956 3.19      0.40     9.9\n5                  47                  186  0.9956 3.19      0.40     9.9\n6                  30                   97  0.9951 3.26      0.44    10.1\n  quality good\n1       6    0\n2       6    0\n3       6    0\n4       6    0\n5       6    0\n6       6    0"
  },
  {
    "objectID": "src/analysis.html#data-analysis",
    "href": "src/analysis.html#data-analysis",
    "title": "Exploratory Analysis",
    "section": "Data Analysis",
    "text": "Data Analysis\n\n\nShow/Hide Code\ndim(wine.data)\n\n\n[1] 4898   13\n\n\nShow/Hide Code\n# Removing duplicate Rows\nwine.data &lt;- wine.data[!duplicated(wine.data), ]\n\ndim(wine.data)\n\n\n[1] 3961   13\n\n\nShow/Hide Code\nstr(wine.data)\n\n\n'data.frame':   3961 obs. of  13 variables:\n $ fixed.acidity       : num  7 6.3 8.1 7.2 6.2 8.1 8.1 8.6 7.9 6.6 ...\n $ volatile.acidity    : num  0.27 0.3 0.28 0.23 0.32 0.22 0.27 0.23 0.18 0.16 ...\n $ citric.acid         : num  0.36 0.34 0.4 0.32 0.16 0.43 0.41 0.4 0.37 0.4 ...\n $ residual.sugar      : num  20.7 1.6 6.9 8.5 7 1.5 1.45 4.2 1.2 1.5 ...\n $ chlorides           : num  0.045 0.049 0.05 0.058 0.045 0.044 0.033 0.035 0.04 0.044 ...\n $ free.sulfur.dioxide : num  45 14 30 47 30 28 11 17 16 48 ...\n $ total.sulfur.dioxide: num  170 132 97 186 136 129 63 109 75 143 ...\n $ density             : num  1.001 0.994 0.995 0.996 0.995 ...\n $ pH                  : num  3 3.3 3.26 3.19 3.18 3.22 2.99 3.14 3.18 3.54 ...\n $ sulphates           : num  0.45 0.49 0.44 0.4 0.47 0.45 0.56 0.53 0.63 0.52 ...\n $ alcohol             : num  8.8 9.5 10.1 9.9 9.6 11 12 9.7 10.8 12.4 ...\n $ quality             : int  6 6 6 6 6 6 5 5 5 7 ...\n $ good                : num  0 0 0 0 0 0 0 0 0 1 ...\n\n\nShow/Hide Code\nsummary(wine.data)\n\n\n fixed.acidity    volatile.acidity  citric.acid     residual.sugar  \n Min.   : 3.800   Min.   :0.0800   Min.   :0.0000   Min.   : 0.600  \n 1st Qu.: 6.300   1st Qu.:0.2100   1st Qu.:0.2700   1st Qu.: 1.600  \n Median : 6.800   Median :0.2600   Median :0.3200   Median : 4.700  \n Mean   : 6.839   Mean   :0.2805   Mean   :0.3343   Mean   : 5.915  \n 3rd Qu.: 7.300   3rd Qu.:0.3300   3rd Qu.:0.3900   3rd Qu.: 8.900  \n Max.   :14.200   Max.   :1.1000   Max.   :1.6600   Max.   :65.800  \n   chlorides       free.sulfur.dioxide total.sulfur.dioxide    density      \n Min.   :0.00900   Min.   :  2.00      Min.   :  9.0        Min.   :0.9871  \n 1st Qu.:0.03500   1st Qu.: 23.00      1st Qu.:106.0        1st Qu.:0.9916  \n Median :0.04200   Median : 33.00      Median :133.0        Median :0.9935  \n Mean   :0.04591   Mean   : 34.89      Mean   :137.2        Mean   :0.9938  \n 3rd Qu.:0.05000   3rd Qu.: 45.00      3rd Qu.:166.0        3rd Qu.:0.9957  \n Max.   :0.34600   Max.   :289.00      Max.   :440.0        Max.   :1.0390  \n       pH          sulphates         alcohol         quality     \n Min.   :2.720   Min.   :0.2200   Min.   : 8.00   Min.   :3.000  \n 1st Qu.:3.090   1st Qu.:0.4100   1st Qu.: 9.50   1st Qu.:5.000  \n Median :3.180   Median :0.4800   Median :10.40   Median :6.000  \n Mean   :3.195   Mean   :0.4904   Mean   :10.59   Mean   :5.855  \n 3rd Qu.:3.290   3rd Qu.:0.5500   3rd Qu.:11.40   3rd Qu.:6.000  \n Max.   :3.820   Max.   :1.0800   Max.   :14.20   Max.   :9.000  \n      good       \n Min.   :0.0000  \n 1st Qu.:0.0000  \n Median :0.0000  \n Mean   :0.2083  \n 3rd Qu.:0.0000  \n Max.   :1.0000  \n\n\nShow/Hide Code\n# Check for NAs in dataset\nsum(is.na(wine.data))\n\n\n[1] 0\n\n\nShow/Hide Code\n# Counts at each combination of response's factor levels\ntable(wine.data$quality)\n\n\n\n   3    4    5    6    7    8    9 \n  20  153 1175 1788  689  131    5"
  },
  {
    "objectID": "src/analysis.html#data-histograms",
    "href": "src/analysis.html#data-histograms",
    "title": "Exploratory Analysis",
    "section": "Data Histograms",
    "text": "Data Histograms\n\n\nShow/Hide Code\nwine.colnames &lt;- colnames(wine.data[, 1:12])\nnum_plots     &lt;- length(wine.colnames)\nnum_rows      &lt;- ceiling(num_plots/3)\n\n\n# Create an empty list to store plots\ngrid_arr      &lt;- list()\n\n\n# Loop over each column name in the wine.colnames vector\nfor(i in 1:num_plots) {\n  # Create a ggplot object for the current column using aes\n  plt &lt;- ggplot(data = wine.data, aes_string(x = wine.colnames[i])) +\n    geom_histogram(binwidth = diff(range(wine.data[[wine.colnames[i]]]))/30, \n                   color = \"black\", fill = \"slategray3\") +\n    labs(x = wine.colnames[i], y = \"Frequency\") +\n    theme_bw()\n  \n  # Add the current plot to the grid_arr list\n  grid_arr[[i]] &lt;- plt\n}\n\ngrid_arr &lt;- do.call(gridExtra::grid.arrange, c(grid_arr, ncol = 3))"
  },
  {
    "objectID": "src/analysis.html#data-relationships",
    "href": "src/analysis.html#data-relationships",
    "title": "Exploratory Analysis",
    "section": "Data Relationships",
    "text": "Data Relationships\n\n\nShow/Hide Code\nreshape2::melt(wine.data[, 1:12], \"quality\") %&gt;% \n  ggplot(aes(value, quality, color = variable)) +  \n  geom_point() + \n  geom_smooth(aes(value,quality, colour=variable), method=lm, se=FALSE)+\n  facet_wrap(.~variable, scales = \"free\")\n\n\n\n\n\nShow/Hide Code\n# Collinearity between Attributes\ncor(wine.data) %&gt;% \n  corrplot::corrplot(method = 'number',  type = \"lower\", tl.col = \"steelblue\", number.cex = 0.5)\n\n\n\n\n\nShow/Hide Code\n# Remove quality from dataset, use good as response \nwine.data$quality &lt;- NULL"
  },
  {
    "objectID": "src/analysis.html#data-split",
    "href": "src/analysis.html#data-split",
    "title": "Exploratory Analysis",
    "section": "Data Split",
    "text": "Data Split\n\n\nShow/Hide Code\nset.seed(123)\n# Splitting the dataset into train and test (7/10th for train remaining for test)\ninTrain &lt;- caret::createDataPartition(wine.data$good, p = 7/10, list = F)\ntrain &lt;- wine.data[inTrain,]\ntest &lt;- wine.data[-inTrain,]\n\n\n# Convert the outcome variable to a factor with two levels\ntrain$good &lt;- as.factor(train$good)\ntest$good &lt;- as.factor(test$good)\n\n# Data savings for model building in the next\nsave(wine.data, file = \"dataset\\\\wine.data.Rdata\")\nsave(train, file = \"dataset\\\\train.Rdata\")\nsave(test, file = \"dataset\\\\test.Rdata\")"
  },
  {
    "objectID": "src/knn.html",
    "href": "src/knn.html",
    "title": "K Nearest Neighbor Classifier",
    "section": "",
    "text": "Show/Hide Code\n#--------------------#\n#-----K-fold CV------#\n#--------------------#\n\nset.seed(1234)\n# Define the training control object for 10-fold cross-validation\ntrain_control &lt;- trainControl(method = \"cv\", number = 10)\n\n# Train the KNN model using 10-fold cross-validation\n# tuneLength argument to specify the range of values of K to be considered for tuning\nset.seed(1234)\nknn_model &lt;- train(good ~ ., \n                   data = train, \n                   method = \"knn\", \n                   trControl = train_control)\n\n# Save the model into .Rdata for future import \nsave(knn_model, file = \"dataset\\\\knn.model_kfoldCV.Rdata\")\n\n\n#--------------------------#\n#-----K-fold CV (Mod)------#\n#--------------------------#\n\nset.seed(1234)\ntrain_control &lt;- trainControl(method = \"cv\", number = 10)\n\nset.seed(1234)\nknn_model &lt;- train(good ~ ., \n                   data = train, \n                   method = \"knn\", \n                   trControl = train_control, \n                   tuneLength = 10)\n\n# Save the model into .Rdata for future import \nsave(knn_model, file = \"dataset\\\\knn.model_kfoldCV_mod.Rdata\")\n\n\n#--------------------#\n#----Hold-out CV-----#\n#--------------------#\n\nset.seed(1234)\ntrain_control &lt;- trainControl(method = \"none\",)\n\nset.seed(1234)\nknn_model &lt;- train(good ~ ., \n                   data = train, \n                   method = \"knn\")\n\nsave(knn_model, file = \"dataset\\\\knn.model_holdoutCV.Rdata\")\n\n\n#--------------------------#\n#----Hold-out CV (Mod)-----#\n#--------------------------#\n\nset.seed(1234)\ntrain_control &lt;- trainControl(method = \"none\",)\n\nset.seed(1234)\nknn_model &lt;- train(good ~ ., \n                   data = train, \n                   method = \"knn\",\n                   tuneGrid = expand.grid(k=1:30))\n\nsave(knn_model, file = \"dataset\\\\knn.model_holdoutCV_mod.Rdata\")\n\n\n#--------------------#\n#-------LOOCV--------#\n#--------------------#\n\nset.seed(1234)\ntrain_control &lt;- trainControl(method = \"LOOCV\")\n\nset.seed(1234)\nknn_model &lt;- train(good ~ ., \n                   data = train, \n                   method = \"knn\", \n                   trControl = train_control)\n\nsave(knn_model, file = \"dataset\\\\knn.model_looCV.Rdata\")\n\n\n#--------------------------#\n#-------LOOCV (Mod)--------#\n#--------------------------#\n\nset.seed(1234)\ntrain_control &lt;- trainControl(method = \"LOOCV\")\n\nset.seed(1234)\nknn_model &lt;- train(good ~ ., \n                   data = train, \n                   method = \"knn\", \n                   trControl = train_control,\n                   tuneLength = 10,\n                   tuneGrid = expand.grid(k = 1:20))\n\nsave(knn_model, file = \"dataset\\\\knn.model_looCV_mod.Rdata\")\n\n\n#--------------------#\n#----Repeated CV-----#\n#--------------------#\n\nset.seed(1234)\ntrain_control &lt;- trainControl(method = \"repeatedcv\", number = 10, repeats = 5)\n\nset.seed(1234)\nknn_model &lt;- train(good ~ ., \n                   data = train, \n                   method = \"knn\", \n                   trControl = train_control)\n\nsave(knn_model, file = \"dataset\\\\knn.model_repeatedCV.Rdata\")\n\n\n#--------------------------#\n#----Repeated CV (Mod)-----#\n#--------------------------#\n\nset.seed(1234)\ntrain_control &lt;- trainControl(method = \"repeatedcv\", number = 10, repeats = 5)\n\nkknn.grid &lt;- expand.grid(kmax = c(3, 5, 7 ,9, 11), distance = c(1, 2, 3),\n                         kernel = c(\"rectangular\", \"gaussian\", \"cos\"))\n\nset.seed(1234)\nknn_model &lt;- train(good ~ ., \n                   data = train, \n                   method = \"kknn\",\n                   trControl = train_control, \n                   tuneGrid = kknn.grid,\n                   preProcess = c(\"center\", \"scale\"))\n\nsave(knn_model, file = \"dataset\\\\knn.model_repeatedCV_mod.Rdata\")"
  },
  {
    "objectID": "src/knn.html#model-construction",
    "href": "src/knn.html#model-construction",
    "title": "K Nearest Neighbor Classifier",
    "section": "",
    "text": "Show/Hide Code\n#--------------------#\n#-----K-fold CV------#\n#--------------------#\n\nset.seed(1234)\n# Define the training control object for 10-fold cross-validation\ntrain_control &lt;- trainControl(method = \"cv\", number = 10)\n\n# Train the KNN model using 10-fold cross-validation\n# tuneLength argument to specify the range of values of K to be considered for tuning\nset.seed(1234)\nknn_model &lt;- train(good ~ ., \n                   data = train, \n                   method = \"knn\", \n                   trControl = train_control)\n\n# Save the model into .Rdata for future import \nsave(knn_model, file = \"dataset\\\\knn.model_kfoldCV.Rdata\")\n\n\n#--------------------------#\n#-----K-fold CV (Mod)------#\n#--------------------------#\n\nset.seed(1234)\ntrain_control &lt;- trainControl(method = \"cv\", number = 10)\n\nset.seed(1234)\nknn_model &lt;- train(good ~ ., \n                   data = train, \n                   method = \"knn\", \n                   trControl = train_control, \n                   tuneLength = 10)\n\n# Save the model into .Rdata for future import \nsave(knn_model, file = \"dataset\\\\knn.model_kfoldCV_mod.Rdata\")\n\n\n#--------------------#\n#----Hold-out CV-----#\n#--------------------#\n\nset.seed(1234)\ntrain_control &lt;- trainControl(method = \"none\",)\n\nset.seed(1234)\nknn_model &lt;- train(good ~ ., \n                   data = train, \n                   method = \"knn\")\n\nsave(knn_model, file = \"dataset\\\\knn.model_holdoutCV.Rdata\")\n\n\n#--------------------------#\n#----Hold-out CV (Mod)-----#\n#--------------------------#\n\nset.seed(1234)\ntrain_control &lt;- trainControl(method = \"none\",)\n\nset.seed(1234)\nknn_model &lt;- train(good ~ ., \n                   data = train, \n                   method = \"knn\",\n                   tuneGrid = expand.grid(k=1:30))\n\nsave(knn_model, file = \"dataset\\\\knn.model_holdoutCV_mod.Rdata\")\n\n\n#--------------------#\n#-------LOOCV--------#\n#--------------------#\n\nset.seed(1234)\ntrain_control &lt;- trainControl(method = \"LOOCV\")\n\nset.seed(1234)\nknn_model &lt;- train(good ~ ., \n                   data = train, \n                   method = \"knn\", \n                   trControl = train_control)\n\nsave(knn_model, file = \"dataset\\\\knn.model_looCV.Rdata\")\n\n\n#--------------------------#\n#-------LOOCV (Mod)--------#\n#--------------------------#\n\nset.seed(1234)\ntrain_control &lt;- trainControl(method = \"LOOCV\")\n\nset.seed(1234)\nknn_model &lt;- train(good ~ ., \n                   data = train, \n                   method = \"knn\", \n                   trControl = train_control,\n                   tuneLength = 10,\n                   tuneGrid = expand.grid(k = 1:20))\n\nsave(knn_model, file = \"dataset\\\\knn.model_looCV_mod.Rdata\")\n\n\n#--------------------#\n#----Repeated CV-----#\n#--------------------#\n\nset.seed(1234)\ntrain_control &lt;- trainControl(method = \"repeatedcv\", number = 10, repeats = 5)\n\nset.seed(1234)\nknn_model &lt;- train(good ~ ., \n                   data = train, \n                   method = \"knn\", \n                   trControl = train_control)\n\nsave(knn_model, file = \"dataset\\\\knn.model_repeatedCV.Rdata\")\n\n\n#--------------------------#\n#----Repeated CV (Mod)-----#\n#--------------------------#\n\nset.seed(1234)\ntrain_control &lt;- trainControl(method = \"repeatedcv\", number = 10, repeats = 5)\n\nkknn.grid &lt;- expand.grid(kmax = c(3, 5, 7 ,9, 11), distance = c(1, 2, 3),\n                         kernel = c(\"rectangular\", \"gaussian\", \"cos\"))\n\nset.seed(1234)\nknn_model &lt;- train(good ~ ., \n                   data = train, \n                   method = \"kknn\",\n                   trControl = train_control, \n                   tuneGrid = kknn.grid,\n                   preProcess = c(\"center\", \"scale\"))\n\nsave(knn_model, file = \"dataset\\\\knn.model_repeatedCV_mod.Rdata\")"
  },
  {
    "objectID": "src/knn.html#k-fold-cv",
    "href": "src/knn.html#k-fold-cv",
    "title": "K Nearest Neighbor Classifier",
    "section": "K-fold CV",
    "text": "K-fold CV\n\n\nShow/Hide Code\n# Data Import\nload(\"dataset\\\\wine.data.Rdata\")\nload(\"dataset\\\\train.Rdata\")\nload(\"dataset\\\\test.Rdata\")\n\n# Model Import\nload(\"dataset\\\\model\\\\knn.model_kfoldCV.Rdata\")\n\n# Make predictions on the test data using the trained model and calculate the test error rate\nknn.predictions &lt;- predict(knn_model, newdata = test)\n\nconfusionMatrix(knn.predictions, test$good)\n\n\nConfusion Matrix and Statistics\n\n          Reference\nPrediction   0   1\n         0 873 194\n         1  76  45\n                                          \n               Accuracy : 0.7727          \n                 95% CI : (0.7478, 0.7963)\n    No Information Rate : 0.7988          \n    P-Value [Acc &gt; NIR] : 0.9878          \n                                          \n                  Kappa : 0.1327          \n                                          \n Mcnemar's Test P-Value : 1.076e-12       \n                                          \n            Sensitivity : 0.9199          \n            Specificity : 0.1883          \n         Pos Pred Value : 0.8182          \n         Neg Pred Value : 0.3719          \n             Prevalence : 0.7988          \n         Detection Rate : 0.7348          \n   Detection Prevalence : 0.8981          \n      Balanced Accuracy : 0.5541          \n                                          \n       'Positive' Class : 0               \n                                          \n\n\nShow/Hide Code\n# Convert predictions to a numeric vector\nknn.predictions &lt;- as.numeric(knn.predictions)\n\n# Calculate the AUC using the performance() and auc() functions:\npred_obj &lt;- prediction(knn.predictions, test$good)\nauc_val &lt;- performance(pred_obj, \"auc\")@y.values[[1]]\nauc_val\n\n\n[1] 0.5541001\n\n\nShow/Hide Code\n# Performance plot for TP and FP\nroc_obj &lt;- performance(pred_obj, \"tpr\", \"fpr\")\nplot(roc_obj, colorize = TRUE, lwd = 2,\n     xlab = \"False Positive Rate\", \n     ylab = \"True Positive Rate\",\n     main = \"ROC Curves from K-fold CV\")\npoints(auc_val, 1 - auc_val, \n       col = \"steelblue\", \n       pch = 21)\nabline(a = 0, b = 1)\n\n\nShow/Hide Code\nknn.kfoldCV.ROC.plot&lt;- recordPlot()\n\n# Accuracy and Kappa value plot\nknn.accu.kappa.plot &lt;- function(knn.model) {\n  p &lt;- ggplot(data=data.frame(k = knn.model$results$k,\n                              Accuracy = knn.model$results$Accuracy,\n                              Kappa = knn.model$results$Kappa)) +\n    geom_point(aes(x = k, y = Accuracy, color = \"Accuracy\")) +\n    geom_point(aes(x = k, y = Kappa, color = \"Kappa\")) +\n    geom_line(aes(x = k, y = Accuracy, linetype = \"Accuracy\", color = \"Accuracy\")) +\n    geom_line(aes(x = k, y = Kappa, linetype = \"Kappa\", color = \"Kappa\")) +\n    scale_color_manual(values = c(\"#98c379\", \"#e06c75\"),\n                       guide = guide_legend(override.aes = list(linetype = c(1, 0)) )) +\n    scale_linetype_manual(values=c(\"solid\", \"dotted\"),\n                          guide = guide_legend(override.aes = list(color = c(\"#98c379\", \"#e06c75\")))) +\n    labs(x = \"K value\", \n         y = \"Accuracy / Kappa\") +\n    ylim(0, 1) +\n    theme_bw() +\n    theme(plot.title = element_text(hjust = 0.5)) +\n    guides(color = guide_legend(title = \"Metric\"),\n           linetype = guide_legend(title = \"Metric\"))\n  return(p)\n}\n\nknn.kfoldCV.plot &lt;- knn.accu.kappa.plot(knn_model) + \n  geom_text(aes(x = k, y = Accuracy, label = round(Accuracy, 3)), vjust = -1) +\n  geom_text(aes(x = k, y = Kappa, label = round(Kappa, 3)), vjust = -1) +\n  ggtitle(\"KNN Model Performance (10-Fold CV)\")\n\n\n\nTuned\n\n\nShow/Hide Code\nload(\"dataset\\\\model\\\\knn.model_kfoldCV_mod.Rdata\")\n\nknn.predictions &lt;- predict(knn_model, newdata = test)\n\nconfusionMatrix(knn.predictions, test$good)\n\n\nConfusion Matrix and Statistics\n\n          Reference\nPrediction   0   1\n         0 927 215\n         1  22  24\n                                          \n               Accuracy : 0.8005          \n                 95% CI : (0.7766, 0.8229)\n    No Information Rate : 0.7988          \n    P-Value [Acc &gt; NIR] : 0.4596          \n                                          \n                  Kappa : 0.1107          \n                                          \n Mcnemar's Test P-Value : &lt;2e-16          \n                                          \n            Sensitivity : 0.9768          \n            Specificity : 0.1004          \n         Pos Pred Value : 0.8117          \n         Neg Pred Value : 0.5217          \n             Prevalence : 0.7988          \n         Detection Rate : 0.7803          \n   Detection Prevalence : 0.9613          \n      Balanced Accuracy : 0.5386          \n                                          \n       'Positive' Class : 0               \n                                          \n\n\nShow/Hide Code\nknn.predictions &lt;- as.numeric(knn.predictions)\npred_obj &lt;- prediction(knn.predictions, test$good)\nauc_val &lt;- performance(pred_obj, \"auc\")@y.values[[1]]\nauc_val\n\n\n[1] 0.5386181\n\n\nShow/Hide Code\nroc_obj &lt;- performance(pred_obj, \"tpr\", \"fpr\")\ninvisible(plot(roc_obj, colorize = TRUE, lwd = 2,\n               xlab = \"False Positive Rate\", \n               ylab = \"True Positive Rate\",\n               main = \"ROC Curves from Tuned K-fold CV\"))\npoints(auc_val, 1 - auc_val, \n       col = \"steelblue\", \n       pch = 21)\nabline(a = 0, b = 1)\n\n\nShow/Hide Code\nknn.kfoldCV_mod.ROC.plot &lt;- recordPlot()\n\nknn.kfoldCV_mod.plot &lt;- knn.accu.kappa.plot(knn_model) +\n  geom_text(aes(x = k, y = Accuracy, label = round(Accuracy, 3)), vjust = -1) +\n  geom_text(aes(x = k, y = Kappa, label = round(Kappa, 3)), vjust = -1) +\n  ggtitle(\"KNN Model Performance (Tuned 10-Fold CV)\")"
  },
  {
    "objectID": "src/knn.html#hold-out-cv-validation-set-approach",
    "href": "src/knn.html#hold-out-cv-validation-set-approach",
    "title": "K Nearest Neighbor Classifier",
    "section": "Hold-out CV (Validation Set Approach)",
    "text": "Hold-out CV (Validation Set Approach)\n\n\nShow/Hide Code\nload(\"dataset\\\\model\\\\knn.model_holdoutCV.Rdata\")\n\nknn.predictions &lt;- predict(knn_model, newdata = test)\n\nconfusionMatrix(knn.predictions, test$good)\n\n\nConfusion Matrix and Statistics\n\n          Reference\nPrediction   0   1\n         0 886 201\n         1  63  38\n                                         \n               Accuracy : 0.7778         \n                 95% CI : (0.753, 0.8011)\n    No Information Rate : 0.7988         \n    P-Value [Acc &gt; NIR] : 0.9663         \n                                         \n                  Kappa : 0.1181         \n                                         \n Mcnemar's Test P-Value : &lt;2e-16         \n                                         \n            Sensitivity : 0.9336         \n            Specificity : 0.1590         \n         Pos Pred Value : 0.8151         \n         Neg Pred Value : 0.3762         \n             Prevalence : 0.7988         \n         Detection Rate : 0.7458         \n   Detection Prevalence : 0.9150         \n      Balanced Accuracy : 0.5463         \n                                         \n       'Positive' Class : 0              \n                                         \n\n\nShow/Hide Code\nknn.predictions &lt;- as.numeric(knn.predictions)\npred_obj &lt;- prediction(knn.predictions, test$good)\nauc_val &lt;- performance(pred_obj, \"auc\")@y.values[[1]]\nauc_val\n\n\n[1] 0.5463051\n\n\nShow/Hide Code\nroc_obj &lt;- performance(pred_obj, \"tpr\", \"fpr\")\nplot(roc_obj, colorize = TRUE, lwd = 2,\n     xlab = \"False Positive Rate\", \n     ylab = \"True Positive Rate\",\n     main = \"ROC Curves from Hold-out CV\")\npoints(auc_val, 1 - auc_val, \n       col = \"steelblue\", \n       pch = 21)\nabline(a = 0, b = 1)\n\n\nShow/Hide Code\nknn.holdoutCV.ROC.plot &lt;- recordPlot()\n\nknn.holdoutCV.plot &lt;- knn.accu.kappa.plot(knn_model) +\n  geom_text(aes(x = k, y = Accuracy, label = round(Accuracy, 3)), vjust = -1) +\n  geom_text(aes(x = k, y = Kappa, label = round(Kappa, 3)), vjust = -1) +\n  ggtitle(\"KNN Model Performance (Hold-out CV)\")\n\n\n\nTuned\n\n\nShow/Hide Code\nload(\"dataset\\\\model\\\\knn.model_holdoutCV_mod.Rdata\")\n\nknn.predictions &lt;- predict(knn_model, newdata = test)\n\nconfusionMatrix(knn.predictions, test$good)\n\n\nConfusion Matrix and Statistics\n\n          Reference\nPrediction   0   1\n         0 943 228\n         1   6  11\n                                          \n               Accuracy : 0.803           \n                 95% CI : (0.7793, 0.8253)\n    No Information Rate : 0.7988          \n    P-Value [Acc &gt; NIR] : 0.3748          \n                                          \n                  Kappa : 0.0608          \n                                          \n Mcnemar's Test P-Value : &lt;2e-16          \n                                          \n            Sensitivity : 0.99368         \n            Specificity : 0.04603         \n         Pos Pred Value : 0.80529         \n         Neg Pred Value : 0.64706         \n             Prevalence : 0.79882         \n         Detection Rate : 0.79377         \n   Detection Prevalence : 0.98569         \n      Balanced Accuracy : 0.51985         \n                                          \n       'Positive' Class : 0               \n                                          \n\n\nShow/Hide Code\nknn.predictions &lt;- as.numeric(knn.predictions)\npred_obj &lt;- prediction(knn.predictions, test$good)\nauc_val &lt;- performance(pred_obj, \"auc\")@y.values[[1]]\nauc_val\n\n\n[1] 0.5198513\n\n\nShow/Hide Code\nroc_obj &lt;- performance(pred_obj, \"tpr\", \"fpr\")\nplot(roc_obj, colorize = TRUE, lwd = 2,\n     xlab = \"False Positive Rate\", \n     ylab = \"True Positive Rate\",\n     main = \"ROC Curves from Tuned Hold-out CV\")\npoints(auc_val, 1 - auc_val, \n       col = \"steelblue\", \n       pch = 21)\nabline(a = 0, b = 1)\n\n\nShow/Hide Code\nknn.holdoutCV_mod.ROC.plot &lt;- recordPlot()\n\nknn.holdoutCV_mod.plot &lt;- knn.accu.kappa.plot(knn_model) + \n  geom_text(aes(x = k, y = Accuracy, label = round(Accuracy, 3)), hjust = -0.3, angle=90) +\n  geom_text(aes(x = k, y = Kappa, label = round(Kappa, 3)), hjust=-0.3, angle=90) +\n  ggtitle(\"KNN Model Performance (Tuned Hold-out CV)\")"
  },
  {
    "objectID": "src/knn.html#loocv",
    "href": "src/knn.html#loocv",
    "title": "K Nearest Neighbor Classifier",
    "section": "LOOCV",
    "text": "LOOCV\n\n\nShow/Hide Code\nload(\"dataset\\\\model\\\\knn.model_looCV.Rdata\")\n\nknn.predictions &lt;- predict(knn_model, newdata = test)\nconfusionMatrix(knn.predictions, test$good)\n\n\nConfusion Matrix and Statistics\n\n          Reference\nPrediction   0   1\n         0 915 170\n         1  34  69\n                                          \n               Accuracy : 0.8283          \n                 95% CI : (0.8056, 0.8493)\n    No Information Rate : 0.7988          \n    P-Value [Acc &gt; NIR] : 0.00558         \n                                          \n                  Kappa : 0.3213          \n                                          \n Mcnemar's Test P-Value : &lt; 2e-16         \n                                          \n            Sensitivity : 0.9642          \n            Specificity : 0.2887          \n         Pos Pred Value : 0.8433          \n         Neg Pred Value : 0.6699          \n             Prevalence : 0.7988          \n         Detection Rate : 0.7702          \n   Detection Prevalence : 0.9133          \n      Balanced Accuracy : 0.6264          \n                                          \n       'Positive' Class : 0               \n                                          \n\n\nShow/Hide Code\nknn.predictions &lt;- as.numeric(knn.predictions)\npred_obj &lt;- prediction(knn.predictions, test$good)\nauc_val &lt;- performance(pred_obj, \"auc\")@y.values[[1]]\nauc_val\n\n\n[1] 0.6264379\n\n\nShow/Hide Code\nroc_obj &lt;- performance(pred_obj, \"tpr\", \"fpr\")\nplot(roc_obj, colorize = TRUE, lwd = 2,\n     xlab = \"False Positive Rate\", \n     ylab = \"True Positive Rate\",\n     main = 'ROC Curves from LOOCV')\npoints(auc_val, 1 - auc_val, \n       col = \"steelblue\", \n       pch = 21)\nabline(a = 0, b = 1)\n\n\nShow/Hide Code\nknn.looCV.ROC.plot &lt;- recordPlot()\n\nknn.looCV.plot &lt;- knn.accu.kappa.plot(knn_model) + \n  geom_text(aes(x = k, y = Accuracy, label = round(Accuracy, 3)), vjust = -1) +\n  geom_text(aes(x = k, y = Kappa, label = round(Kappa, 3)), vjust = -1) +\n  ggtitle(\"KNN Model Performance (LOOCV)\")\n\n\n\nTuned\n\n\nShow/Hide Code\nload(\"dataset\\\\model\\\\knn.model_looCV_mod.Rdata\")\n\nknn.predictions &lt;- predict(knn_model, newdata = test)\nconfusionMatrix(knn.predictions, test$good)\n\n\nConfusion Matrix and Statistics\n\n          Reference\nPrediction   0   1\n         0 927 215\n         1  22  24\n                                          \n               Accuracy : 0.8005          \n                 95% CI : (0.7766, 0.8229)\n    No Information Rate : 0.7988          \n    P-Value [Acc &gt; NIR] : 0.4596          \n                                          \n                  Kappa : 0.1107          \n                                          \n Mcnemar's Test P-Value : &lt;2e-16          \n                                          \n            Sensitivity : 0.9768          \n            Specificity : 0.1004          \n         Pos Pred Value : 0.8117          \n         Neg Pred Value : 0.5217          \n             Prevalence : 0.7988          \n         Detection Rate : 0.7803          \n   Detection Prevalence : 0.9613          \n      Balanced Accuracy : 0.5386          \n                                          \n       'Positive' Class : 0               \n                                          \n\n\nShow/Hide Code\nknn.predictions &lt;- as.numeric(knn.predictions)\npred_obj &lt;- prediction(knn.predictions, test$good)\nauc_val &lt;- performance(pred_obj, \"auc\")@y.values[[1]]\nauc_val\n\n\n[1] 0.5386181\n\n\nShow/Hide Code\nroc_obj &lt;- performance(pred_obj, \"tpr\", \"fpr\")\nplot(roc_obj, colorize = TRUE, lwd = 2,\n     xlab = \"False Positive Rate\", \n     ylab = \"True Positive Rate\",\n     main = \"ROC Curves from Tuned LOOCV\")\npoints(auc_val, 1 - auc_val, \n       col = \"steelblue\", \n       pch = 21)\nabline(a = 0, b = 1)\n\n\nShow/Hide Code\nknn.looCV_mod.ROC.plot &lt;- recordPlot()\n\nknn.looCV_mod.plot &lt;- knn.accu.kappa.plot(knn_model) + \n  geom_text(aes(x = k, y = Accuracy, label = round(Accuracy, 3)), hjust = -0.3, angle=90) +\n  geom_text(aes(x = k, y = Kappa, label = round(Kappa, 3)), hjust = -0.3, angle=90) +\n  ggtitle(\"KNN Model Performance (Tuned LOOCV)\")"
  },
  {
    "objectID": "src/knn.html#repeated-cv",
    "href": "src/knn.html#repeated-cv",
    "title": "K Nearest Neighbor Classifier",
    "section": "Repeated CV",
    "text": "Repeated CV\n\n\nShow/Hide Code\nload(\"dataset\\\\model\\\\knn.model_repeatedCV.Rdata\")\n\nknn.predictions &lt;- predict(knn_model, newdata = test)\n\nconfusionMatrix(knn.predictions, test$good)\n\n\nConfusion Matrix and Statistics\n\n          Reference\nPrediction   0   1\n         0 864 126\n         1  85 113\n                                          \n               Accuracy : 0.8224          \n                 95% CI : (0.7994, 0.8437)\n    No Information Rate : 0.7988          \n    P-Value [Acc &gt; NIR] : 0.022056        \n                                          \n                  Kappa : 0.4095          \n                                          \n Mcnemar's Test P-Value : 0.005892        \n                                          \n            Sensitivity : 0.9104          \n            Specificity : 0.4728          \n         Pos Pred Value : 0.8727          \n         Neg Pred Value : 0.5707          \n             Prevalence : 0.7988          \n         Detection Rate : 0.7273          \n   Detection Prevalence : 0.8333          \n      Balanced Accuracy : 0.6916          \n                                          \n       'Positive' Class : 0               \n                                          \n\n\nShow/Hide Code\nknn.predictions &lt;- as.numeric(knn.predictions)\npred_obj &lt;- prediction(knn.predictions, test$good)\nauc_val &lt;- performance(pred_obj, \"auc\")@y.values[[1]]\nauc_val\n\n\n[1] 0.6916177\n\n\nShow/Hide Code\nroc_obj &lt;- performance(pred_obj, \"tpr\", \"fpr\")\nplot(roc_obj, colorize = TRUE, lwd = 2,\n     xlab = \"False Positive Rate\", \n     ylab = \"True Positive Rate\",\n     main = \"ROC Curves from Repeated CV\")\npoints(auc_val, 1 - auc_val, \n       col = \"steelblue\", \n       pch = 21)\nabline(a = 0, b = 1)\n\n\nShow/Hide Code\nknn.repeatedCV.ROC.plot &lt;- recordPlot()\n\ndf &lt;- knn_model$results\nknn.repeatedCV.plot &lt;- ggplot(data=df, aes(x = kmax, y = Accuracy)) +\n  geom_point(aes(color = \"Accuracy\")) +\n  geom_point(aes(color = \"Kappa\")) +\n  geom_line(aes(linetype = \"Accuracy\", color = \"Accuracy\")) +\n  geom_line(aes(y = Kappa, linetype = \"Kappa\", color = \"Kappa\")) +\n  geom_text(aes(label = round(Accuracy, 3)), vjust = -1) +\n  geom_text(aes(y = Kappa, label = round(Kappa, 3)), vjust = -1) +\n  scale_color_manual(values = c(\"#98c379\", \"#e06c75\"),\n                     guide = guide_legend(override.aes = list(linetype = c(1, 0)) )) +\n  scale_linetype_manual(values=c(\"solid\", \"dotted\"),\n                        guide = guide_legend(override.aes = list(color = c(\"#98c379\", \"#e06c75\")))) +\n  labs(x = \"K value\", \n       y = \"Accuracy / Kappa\",\n       title = \"KNN Model Performance (Repeated CV)\") +\n  ylim(0, 1) +\n  theme_bw() +\n  theme(plot.title = element_text(hjust = 0.5)) +\n  guides(color = guide_legend(title = \"Metric\"),\n         linetype = guide_legend(title = \"Metric\"))\n\n\n\nTuned\n\n\nShow/Hide Code\nload(\"dataset\\\\model\\\\knn.model_repeatedCV_mod.Rdata\")\n\nknn.predictions &lt;- predict(knn_model, newdata = test)\n\nconfusionMatrix(knn.predictions, test$good)\n\n\nConfusion Matrix and Statistics\n\n          Reference\nPrediction   0   1\n         0 906  90\n         1  43 149\n                                          \n               Accuracy : 0.888           \n                 95% CI : (0.8687, 0.9054)\n    No Information Rate : 0.7988          \n    P-Value [Acc &gt; NIR] : &lt; 2.2e-16       \n                                          \n                  Kappa : 0.624           \n                                          \n Mcnemar's Test P-Value : 6.643e-05       \n                                          \n            Sensitivity : 0.9547          \n            Specificity : 0.6234          \n         Pos Pred Value : 0.9096          \n         Neg Pred Value : 0.7760          \n             Prevalence : 0.7988          \n         Detection Rate : 0.7626          \n   Detection Prevalence : 0.8384          \n      Balanced Accuracy : 0.7891          \n                                          \n       'Positive' Class : 0               \n                                          \n\n\nShow/Hide Code\nknn.predictions &lt;- as.numeric(knn.predictions)\npred_obj &lt;- prediction(knn.predictions, test$good)\nauc_val &lt;- performance(pred_obj, \"auc\")@y.values[[1]]\nauc_val\n\n\n[1] 0.7890601\n\n\nShow/Hide Code\nroc_obj &lt;- performance(pred_obj, \"tpr\", \"fpr\")\n\nplot(roc_obj, colorize = TRUE, lwd = 2,\n     xlab = \"False Positive Rate\", \n     ylab = \"True Positive Rate\",\n     main = \"ROC Curves from Tuned Repeated CV\")\npoints(auc_val, 1 - auc_val, \n       col = \"steelblue\", \n       pch = 21)\nabline(a = 0, b = 1)\n\n\nShow/Hide Code\nknn.repeatedCV_mod.ROC.plot &lt;- recordPlot()\n\nknn.repeatedCV_mod.plot &lt;- ggplot(knn_model) +\n  labs(x = \"K value\", \n       y = \"Accuracy\", \n       title = \"KNN Model Performance (Tuned Repeated CV)\") +\n  theme_bw() +\n  theme(plot.title = element_text(hjust = 0.5))"
  },
  {
    "objectID": "src/knn.html#summary",
    "href": "src/knn.html#summary",
    "title": "K Nearest Neighbor Classifier",
    "section": "Summary",
    "text": "Summary\n\n\nShow/Hide Code\nggarrange(knn.kfoldCV.plot,\n          knn.kfoldCV_mod.plot,\n          knn.holdoutCV.plot,\n          knn.holdoutCV_mod.plot,\n          knn.looCV.plot,\n          knn.looCV_mod.plot,\n          knn.repeatedCV.plot,\n          knn.repeatedCV_mod.plot,\n          ncol = 2, nrow = 4)\n\n\n\n\n\n\n\nShow/Hide Code\ncowplot::plot_grid(knn.kfoldCV.ROC.plot, knn.kfoldCV_mod.ROC.plot,\n                   ncol = 2, align = \"hv\", scale = 0.8)\n\n\n\n\n\nShow/Hide Code\ncowplot::plot_grid(knn.holdoutCV.ROC.plot, knn.holdoutCV_mod.ROC.plot,\n                   ncol = 2, align = \"hv\", scale = 0.8)\n\n\n\n\n\nShow/Hide Code\ncowplot::plot_grid(knn.looCV.ROC.plot, knn.looCV_mod.ROC.plot,\n                   ncol = 2, align = \"hv\", scale = 0.8)\n\n\n\n\n\nShow/Hide Code\ncowplot::plot_grid(knn.repeatedCV.ROC.plot, knn.repeatedCV_mod.ROC.plot,\n                   ncol = 2, align = \"hv\", scale = 0.8)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nResampling Method\nError Rate\nSensitivity\nSpecificity\nAUC\n\n\n\n\nK-Fold CV\n0.2273\n0.9199\n0.1883\n0.5541001\n\n\nK-Fold CV (Tuned)\n0.1995\n0.9768\n0.1004\n0.5386181\n\n\nHold-out CV\n0.2222\n0.9336\n0.1590\n0.5463051\n\n\nHold-out CV (Tuned)\n0.2022\n0.9926\n0.0251\n0.5088642\n\n\nLOOCV\n0.1717\n0.9642\n0.2887\n0.6264379\n\n\nLOOCV (Tuned)\n0.1995\n0.9768\n0.1004\n0.5386181\n\n\nRepeated CV\n0.1776\n0.9104\n0.4728\n0.6916177\n\n\nRepeated CV (Tuned)\n0.1120\n0.9547\n0.6234\n0.7890601"
  },
  {
    "objectID": "src/lda.html",
    "href": "src/lda.html",
    "title": "Quadratic Discriminant Analysis",
    "section": "",
    "text": "Show/Hide Code\n#------------#\n#----LDA-----#\n#------------#\nset.seed(1234)\ntrain_control &lt;- trainControl(method = \"cv\", number = 10)\n\nset.seed(1234)\nlda_model &lt;- train(good ~ ., \n                   data = train, \n                   method = \"lda\", \n                   trControl = train_control)\n\nsave(lda_model, file = \"dataset\\\\lda.model_kfoldCV.Rdata\")\n\n\n#------------------#\n#----LDA (Mod)-----#\n#------------------#"
  },
  {
    "objectID": "src/lda.html#model-construction",
    "href": "src/lda.html#model-construction",
    "title": "Quadratic Discriminant Analysis",
    "section": "",
    "text": "Show/Hide Code\n#------------#\n#----LDA-----#\n#------------#\nset.seed(1234)\ntrain_control &lt;- trainControl(method = \"cv\", number = 10)\n\nset.seed(1234)\nlda_model &lt;- train(good ~ ., \n                   data = train, \n                   method = \"lda\", \n                   trControl = train_control)\n\nsave(lda_model, file = \"dataset\\\\lda.model_kfoldCV.Rdata\")\n\n\n#------------------#\n#----LDA (Mod)-----#\n#------------------#"
  },
  {
    "objectID": "src/lda.html#k-fold-cv",
    "href": "src/lda.html#k-fold-cv",
    "title": "Quadratic Discriminant Analysis",
    "section": "K-fold CV",
    "text": "K-fold CV\n\n\nShow/Hide Code\n# Data Import\nload(\"dataset\\\\wine.data.Rdata\")\nload(\"dataset\\\\train.Rdata\")\nload(\"dataset\\\\test.Rdata\")\n\n# Model Import\nload(\"dataset\\\\model\\\\lda.model_kfoldCV.Rdata\")\n\nlda.predictions &lt;- predict(lda_model, newdata = test)\n\nconfusionMatrix(lda.predictions, test$good)\n\n\nConfusion Matrix and Statistics\n\n          Reference\nPrediction   0   1\n         0 881 160\n         1  68  79\n                                          \n               Accuracy : 0.8081          \n                 95% CI : (0.7845, 0.8301)\n    No Information Rate : 0.7988          \n    P-Value [Acc &gt; NIR] : 0.2246          \n                                          \n                  Kappa : 0.3024          \n                                          \n Mcnemar's Test P-Value : 1.674e-09       \n                                          \n            Sensitivity : 0.9283          \n            Specificity : 0.3305          \n         Pos Pred Value : 0.8463          \n         Neg Pred Value : 0.5374          \n             Prevalence : 0.7988          \n         Detection Rate : 0.7416          \n   Detection Prevalence : 0.8763          \n      Balanced Accuracy : 0.6294          \n                                          \n       'Positive' Class : 0               \n                                          \n\n\nShow/Hide Code\nlda.predictions &lt;- as.numeric(lda.predictions)\npred_obj &lt;- prediction(lda.predictions, test$good)\nauc_val &lt;- performance(pred_obj, \"auc\")@y.values[[1]]\nauc_val\n\n\n[1] 0.6294448\n\n\nShow/Hide Code\nroc_obj &lt;- performance(pred_obj, \"tpr\", \"fpr\")\nplot(roc_obj, colorize = TRUE, lwd = 2,\n     xlab = \"False Positive Rate\", \n     ylab = \"True Positive Rate\",\n     main = \"ROC Curves from Repeated CV\")\npoints(auc_val, 1 - auc_val, \n       col = \"steelblue\", \n       pch = 21)\nabline(a = 0, b = 1)\n\n\nShow/Hide Code\nlda.kfoldCV.ROC.plot &lt;- recordPlot()\n\n\n\nTuned\n\n\n\n\n\n\n\n\n\n\nResampling Method\nError Rate\nSensitivity\nSpecificity\nAUC\n\n\n\n\nLDA\n0.1919\n0.9283\n0.3305\n0.6294448\n\n\nLDA (Tuned)\n0.xxxx\n0.xxxx\n0.xxxx\n0.xxxxxxx"
  },
  {
    "objectID": "src/logit.html",
    "href": "src/logit.html",
    "title": "Logistic Regression",
    "section": "",
    "text": "Show/Hide Code\n#----------------------------#\n#----Logistic Regression-----#\n#----------------------------#\n\nset.seed(1234)\n# Define the training control object for 10-fold cross-validation\ntrain_control &lt;- trainControl(method = \"cv\", number = 10)\n\n# Train the logistic regression model using 10-fold cross-validation\nset.seed(1234)\nlogit_model &lt;- train(good ~ ., \n                     data = train, \n                     method = \"glm\", \n                     family = \"binomial\",\n                     trControl = train_control)\n\nsave(logit_model, file = \"dataset\\\\logit.model_kfoldCV.Rdata\")\n\n\n#----------------------------------#\n#----Logistic Regression (Mod)-----#\n#----------------------------------#"
  },
  {
    "objectID": "src/logit.html#model-construction",
    "href": "src/logit.html#model-construction",
    "title": "Logistic Regression",
    "section": "",
    "text": "Show/Hide Code\n#----------------------------#\n#----Logistic Regression-----#\n#----------------------------#\n\nset.seed(1234)\n# Define the training control object for 10-fold cross-validation\ntrain_control &lt;- trainControl(method = \"cv\", number = 10)\n\n# Train the logistic regression model using 10-fold cross-validation\nset.seed(1234)\nlogit_model &lt;- train(good ~ ., \n                     data = train, \n                     method = \"glm\", \n                     family = \"binomial\",\n                     trControl = train_control)\n\nsave(logit_model, file = \"dataset\\\\logit.model_kfoldCV.Rdata\")\n\n\n#----------------------------------#\n#----Logistic Regression (Mod)-----#\n#----------------------------------#"
  },
  {
    "objectID": "src/logit.html#k-fold-cv",
    "href": "src/logit.html#k-fold-cv",
    "title": "Logistic Regression",
    "section": "K-fold CV",
    "text": "K-fold CV\n\n\nShow/Hide Code\n# Data Import\nload(\"dataset\\\\wine.data.Rdata\")\nload(\"dataset\\\\train.Rdata\")\nload(\"dataset\\\\test.Rdata\")\n\n# Model Import\nload(\"dataset\\\\model\\\\logit.model_kfoldCV.Rdata\")\n\nlogit.predictions &lt;- predict(logit_model, newdata = test)\n\nconfusionMatrix(logit.predictions, test$good)\n\n\nConfusion Matrix and Statistics\n\n          Reference\nPrediction   0   1\n         0 887 169\n         1  62  70\n                                          \n               Accuracy : 0.8056          \n                 95% CI : (0.7819, 0.8277)\n    No Information Rate : 0.7988          \n    P-Value [Acc &gt; NIR] : 0.2954          \n                                          \n                  Kappa : 0.2733          \n                                          \n Mcnemar's Test P-Value : 3.074e-12       \n                                          \n            Sensitivity : 0.9347          \n            Specificity : 0.2929          \n         Pos Pred Value : 0.8400          \n         Neg Pred Value : 0.5303          \n             Prevalence : 0.7988          \n         Detection Rate : 0.7466          \n   Detection Prevalence : 0.8889          \n      Balanced Accuracy : 0.6138          \n                                          \n       'Positive' Class : 0               \n                                          \n\n\nShow/Hide Code\nlogit.predictions &lt;- as.numeric(logit.predictions)\npred_obj &lt;- prediction(logit.predictions, test$good)\nauc_val  &lt;- performance(pred_obj, \"auc\")@y.values[[1]]\nauc_val\n\n\n[1] 0.6137776\n\n\nShow/Hide Code\nroc_obj &lt;- performance(pred_obj, \"tpr\", \"fpr\")\nplot(roc_obj, colorize = TRUE, lwd = 2,\n     xlab = \"False Positive Rate\", \n     ylab = \"True Positive Rate\",\n     main = \"ROC Curves from Repeated CV\")\npoints(auc_val, 1 - auc_val, \n       col = \"steelblue\", \n       pch = 21)\nabline(a = 0, b = 1)\n\n\nShow/Hide Code\nlogit.kfoldCV.ROC.plot &lt;- recordPlot()\n\n\n\nTuned\n\n\nShow/Hide Code\nglm.model &lt;- glm(good ~ ., data= train,family=\"binomial\")\nglm.fit= stepAIC(glm.model, direction = 'backward')\n\n\nStart:  AIC=2228.76\ngood ~ fixed.acidity + volatile.acidity + citric.acid + residual.sugar + \n    chlorides + free.sulfur.dioxide + total.sulfur.dioxide + \n    density + pH + sulphates + alcohol\n\n                       Df Deviance    AIC\n- citric.acid           1   2205.4 2227.4\n- alcohol               1   2205.7 2227.7\n&lt;none&gt;                      2204.8 2228.8\n- total.sulfur.dioxide  1   2206.9 2228.9\n- chlorides             1   2215.2 2237.2\n- free.sulfur.dioxide   1   2216.8 2238.8\n- sulphates             1   2224.5 2246.5\n- fixed.acidity         1   2230.8 2252.8\n- volatile.acidity      1   2231.5 2253.5\n- density               1   2236.3 2258.3\n- residual.sugar        1   2243.8 2265.8\n- pH                    1   2259.9 2281.9\n\nStep:  AIC=2227.37\ngood ~ fixed.acidity + volatile.acidity + residual.sugar + chlorides + \n    free.sulfur.dioxide + total.sulfur.dioxide + density + pH + \n    sulphates + alcohol\n\n                       Df Deviance    AIC\n- alcohol               1   2206.2 2226.2\n&lt;none&gt;                      2205.4 2227.4\n- total.sulfur.dioxide  1   2207.7 2227.7\n- chlorides             1   2215.9 2235.9\n- free.sulfur.dioxide   1   2217.4 2237.4\n- sulphates             1   2225.0 2245.0\n- fixed.acidity         1   2230.8 2250.8\n- volatile.acidity      1   2231.7 2251.7\n- density               1   2237.6 2257.6\n- residual.sugar        1   2244.7 2264.7\n- pH                    1   2260.8 2280.8\n\nStep:  AIC=2226.16\ngood ~ fixed.acidity + volatile.acidity + residual.sugar + chlorides + \n    free.sulfur.dioxide + total.sulfur.dioxide + density + pH + \n    sulphates\n\n                       Df Deviance    AIC\n- total.sulfur.dioxide  1   2208.0 2226.0\n&lt;none&gt;                      2206.2 2226.2\n- chlorides             1   2216.4 2234.4\n- free.sulfur.dioxide   1   2217.6 2235.6\n- sulphates             1   2231.3 2249.3\n- volatile.acidity      1   2231.7 2249.7\n- fixed.acidity         1   2272.7 2290.7\n- pH                    1   2316.6 2334.6\n- residual.sugar        1   2383.1 2401.1\n- density               1   2512.7 2530.7\n\nStep:  AIC=2226.01\ngood ~ fixed.acidity + volatile.acidity + residual.sugar + chlorides + \n    free.sulfur.dioxide + density + pH + sulphates\n\n                      Df Deviance    AIC\n&lt;none&gt;                     2208.0 2226.0\n- free.sulfur.dioxide  1   2218.1 2234.1\n- chlorides            1   2218.6 2234.6\n- sulphates            1   2232.6 2248.6\n- volatile.acidity     1   2238.0 2254.0\n- fixed.acidity        1   2274.6 2290.6\n- pH                   1   2317.9 2333.9\n- residual.sugar       1   2390.9 2406.9\n- density              1   2560.1 2576.1\n\n\nShow/Hide Code\n# Make predictions on test data and construct a confusion matrix\nlogit.predictions &lt;- predict(glm.fit, newdata = test,type = \"response\")\nlogit.predictions &lt;- factor(ifelse(logit.predictions &gt; 0.7, 1, 0),\n                            levels = c(0, 1))\nconfusionMatrix(logit.predictions, test$good)\n\n\nConfusion Matrix and Statistics\n\n          Reference\nPrediction   0   1\n         0 939 218\n         1  10  21\n                                          \n               Accuracy : 0.8081          \n                 95% CI : (0.7845, 0.8301)\n    No Information Rate : 0.7988          \n    P-Value [Acc &gt; NIR] : 0.2246          \n                                          \n                  Kappa : 0.1147          \n                                          \n Mcnemar's Test P-Value : &lt;2e-16          \n                                          \n            Sensitivity : 0.98946         \n            Specificity : 0.08787         \n         Pos Pred Value : 0.81158         \n         Neg Pred Value : 0.67742         \n             Prevalence : 0.79882         \n         Detection Rate : 0.79040         \n   Detection Prevalence : 0.97391         \n      Balanced Accuracy : 0.53866         \n                                          \n       'Positive' Class : 0               \n                                          \n\n\nShow/Hide Code\nlogit.predictions &lt;- as.numeric(logit.predictions)\npred_obj &lt;- prediction(logit.predictions, test$good)\nauc_val &lt;- performance(pred_obj, \"auc\")@y.values[[1]]\nauc_val\n\n\n[1] 0.5386644\n\n\nShow/Hide Code\nroc_obj &lt;- performance(pred_obj, \"tpr\", \"fpr\")\nplot(roc_obj, colorize = TRUE, lwd = 2,\n     xlab = \"False Positive Rate\", \n     ylab = \"True Positive Rate\",\n     main = \"ROC Curves from Repeated CV\")\npoints(auc_val, 1 - auc_val, \n       col = \"steelblue\", \n       pch = 21)\nabline(a = 0, b = 1)\n\n\nShow/Hide Code\nlogit.kfoldCV.ROC.plot &lt;- recordPlot()\n\n\n\n\n\n\n\n\n\n\n\n\nResampling Method\nError Rate\nSensitivity\nSpecificity\nAUC\n\n\n\n\nLogistic Regression\n0.1944\n0.9347\n0.2929\n0.6137776\n\n\nLogistic Regression (Tuned)\n0.1919\n0.98946\n0.08787\n0.5386644"
  },
  {
    "objectID": "src/qda.html",
    "href": "src/qda.html",
    "title": "Linear Discriminant Analysis",
    "section": "",
    "text": "Show/Hide Code\n#------------#\n#----QDA-----#\n#------------#\nset.seed(1234)\ntrain_control &lt;- trainControl(method = \"cv\", number = 10)\n\nset.seed(1234)\nqda_model &lt;- train(good ~ ., \n                   data = train, \n                   method = \"qda\", \n                   trControl = train_control)\n\nsave(qda_model, file = \"dataset\\\\qda.model_kfoldCV.Rdata\")\n\n#------------------#\n#----QDA (Mod)-----#\n#------------------#"
  },
  {
    "objectID": "src/qda.html#model-construction",
    "href": "src/qda.html#model-construction",
    "title": "Linear Discriminant Analysis",
    "section": "",
    "text": "Show/Hide Code\n#------------#\n#----QDA-----#\n#------------#\nset.seed(1234)\ntrain_control &lt;- trainControl(method = \"cv\", number = 10)\n\nset.seed(1234)\nqda_model &lt;- train(good ~ ., \n                   data = train, \n                   method = \"qda\", \n                   trControl = train_control)\n\nsave(qda_model, file = \"dataset\\\\qda.model_kfoldCV.Rdata\")\n\n#------------------#\n#----QDA (Mod)-----#\n#------------------#"
  },
  {
    "objectID": "src/qda.html#k-fold-cv",
    "href": "src/qda.html#k-fold-cv",
    "title": "Linear Discriminant Analysis",
    "section": "K-fold CV",
    "text": "K-fold CV\n\n\nShow/Hide Code\n# Data Import\nload(\"dataset\\\\wine.data.Rdata\")\nload(\"dataset\\\\train.Rdata\")\nload(\"dataset\\\\test.Rdata\")\n\n# Model Import\nload(\"dataset\\\\model\\\\qda.model_kfoldCV.Rdata\")\n\nqda.predictions &lt;- predict(qda_model, newdata = test)\n\nconfusionMatrix(qda.predictions, test$good)\n\n\nConfusion Matrix and Statistics\n\n          Reference\nPrediction   0   1\n         0 704  59\n         1 245 180\n                                          \n               Accuracy : 0.7441          \n                 95% CI : (0.7183, 0.7687)\n    No Information Rate : 0.7988          \n    P-Value [Acc &gt; NIR] : 1               \n                                          \n                  Kappa : 0.3834          \n                                          \n Mcnemar's Test P-Value : &lt;2e-16          \n                                          \n            Sensitivity : 0.7418          \n            Specificity : 0.7531          \n         Pos Pred Value : 0.9227          \n         Neg Pred Value : 0.4235          \n             Prevalence : 0.7988          \n         Detection Rate : 0.5926          \n   Detection Prevalence : 0.6423          \n      Balanced Accuracy : 0.7475          \n                                          \n       'Positive' Class : 0               \n                                          \n\n\nShow/Hide Code\nqda.predictions &lt;- as.numeric(qda.predictions)\npred_obj &lt;- prediction(qda.predictions, test$good)\nauc_val &lt;- performance(pred_obj, \"auc\")@y.values[[1]]\nauc_val\n\n\n[1] 0.7474858\n\n\nShow/Hide Code\nroc_obj &lt;- performance(pred_obj, \"tpr\", \"fpr\")\nplot(roc_obj, colorize = TRUE, lwd = 2,\n     xlab = \"False Positive Rate\", \n     ylab = \"True Positive Rate\",\n     main = \"ROC Curves from Repeated CV\")\npoints(auc_val, 1 - auc_val, \n       col = \"steelblue\", \n       pch = 21)\nabline(a = 0, b = 1)\n\n\nShow/Hide Code\nqda.kfoldCV.ROC.plot &lt;- recordPlot()\n\n\n\nTuned\n\n\n\n\n\n\n\n\n\n\nResampling Method\nError Rate\nSensitivity\nSpecificity\nAUC\n\n\n\n\nQDA\n0.2559\n0.7418\n0.7531\n0.7474858\n\n\nQDA (Tuned)\n0.xxxx\n0.xxxx\n0.xxxx\n0.xxxxxxx"
  },
  {
    "objectID": "src/qda.html#summary",
    "href": "src/qda.html#summary",
    "title": "Linear Discriminant Analysis",
    "section": "Summary",
    "text": "Summary\n\n\nShow/Hide Code\ncowplot::plot_grid(knn.kfoldCV.ROC.plot,\n                   logit.kfoldCV.ROC.plot,\n                   lda.kfoldCV.ROC.plot,\n                   qda.kfoldCV.ROC.plot, \n                   ncol = 2, align = \"hv\", scale = 0.8)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nResampling Method\nError Rate\nSensitivity\nSpecificity\nAUC\n\n\n\n\nLogistic Regression\n0.1944\n0.9347\n0.2929\n0.6137776\n\n\nLogistic Regression (Tuned)\n0.1919\n0.98946\n0.08787\n0.5386644\n\n\n\n\n\n\n\n\n\nLDA\n0.1919\n0.9283\n0.3305\n0.6294448\n\n\nLDA (Tuned)\n0.xxxx\n0.xxxx\n0.xxxx\n0.xxxxxxx\n\n\n\n\n\n\n\n\n\nQDA\n0.2559\n0.7418\n0.7531\n0.7474858\n\n\nQDA (Tuned)\n0.xxxx\n0.xxxx\n0.xxxx\n0.xxxxxxx"
  }
]