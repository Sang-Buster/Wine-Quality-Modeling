[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Introduction to Statistical Learning",
    "section": "",
    "text": "Consider the wine quality dataset from UCI Machine Learning Respository1. We will focus only on the data concerning white wines (and not red wines). Dichotomize the quality variable as good, which takes the value 1 if quality \\(\\geq\\) 7 axnd the value 0, otherwise. We will take good as response and all the 11 physiochemical characteristics of the wines in the data as predictors.\n\n\nUse 10-fold cross-validation for estimating the test error rates below and compute the estimates using caret package with seed set to 1234 before each computation.\n\nFit a KNN with K chosen optimally using test error rate. Report error rate, sensitivity, specificity, and AUC for the optimal KNN based on the training data. Also, report its estimated test error rate.\nRepeat (a) using logistic regression.\nRepeat (a) using LDA.\nRepeat (a) using QDA.\nCompare the results in (a)-(d). Which classifier would you recommend? Justify your answer.\n\n\n\n\n\nKNN\nGLM/logit/glmnet\nLDA\nQDA\nNaive Bayes\nDecision Tree (CART Algorithm)\nRandom Forest (Classification)\nBagging (Bootstrap Aggregation)\nBoosting (Gradient Boosting Machine (GBM))\neXtreme Gradient Boosting (XGBoost)\nExtremely Randomized Trees (ExtraTrees)\nSVM\nNeural Networks (NNET)\n\n\n\n\nThis is a dataset of wine quality containing 4898 observations of 12 variables. The variables are:\n\nfixed.acidity: The amount of fixed acid in the wine (\\(g/dm^3\\))\nvolatile.acidity: The amount of volatile acid in the wine (\\(g/dm^4\\))\ncitric.acid: The amount of citric acid in the wine (\\(g/dm^3\\))\nresidual.sugar: The amount of residual sugar in the wine (\\(g/dm^3\\))\nchlorides: The amount of salt in the wine (\\(g/dm^3\\))\nfree.sulfur.dioxide: The amount of free sulfur dioxide in the wine (\\(mg/dm^3\\))\ntotal.sulfur.dioxide: The amount of total sulfur dioxide in the wine (\\(mg/dm^3\\))\ndensity: The density of the wine (\\(g/dm^3\\))\npH: The \\(pH\\) value of the wine\nsulphates: The amount of sulphates in the wine (\\(g/dm^3\\))\nalcohol: The alcohol content of the wine (\\(\\% vol\\))\nquality: The quality score of the wine (0-10)\n\nAfter removing the duplicate rows from our data set, we are left with 3961 observations of the above 11 variables minus quality column variable, and introduced a new variable good as our response:\n\ngood: A binary variable indicating whether the wine is good (quality \\(\\geq\\) 7) or not (quality \\(&lt;\\) 7)."
  },
  {
    "objectID": "index.html#preamble",
    "href": "index.html#preamble",
    "title": "Introduction to Statistical Learning",
    "section": "",
    "text": "Consider the wine quality dataset from UCI Machine Learning Respository1. We will focus only on the data concerning white wines (and not red wines). Dichotomize the quality variable as good, which takes the value 1 if quality \\(\\geq\\) 7 axnd the value 0, otherwise. We will take good as response and all the 11 physiochemical characteristics of the wines in the data as predictors.\n\n\nUse 10-fold cross-validation for estimating the test error rates below and compute the estimates using caret package with seed set to 1234 before each computation.\n\nFit a KNN with K chosen optimally using test error rate. Report error rate, sensitivity, specificity, and AUC for the optimal KNN based on the training data. Also, report its estimated test error rate.\nRepeat (a) using logistic regression.\nRepeat (a) using LDA.\nRepeat (a) using QDA.\nCompare the results in (a)-(d). Which classifier would you recommend? Justify your answer.\n\n\n\n\n\nKNN\nGLM/logit/glmnet\nLDA\nQDA\nNaive Bayes\nDecision Tree (CART Algorithm)\nRandom Forest (Classification)\nBagging (Bootstrap Aggregation)\nBoosting (Gradient Boosting Machine (GBM))\neXtreme Gradient Boosting (XGBoost)\nExtremely Randomized Trees (ExtraTrees)\nSVM\nNeural Networks (NNET)\n\n\n\n\nThis is a dataset of wine quality containing 4898 observations of 12 variables. The variables are:\n\nfixed.acidity: The amount of fixed acid in the wine (\\(g/dm^3\\))\nvolatile.acidity: The amount of volatile acid in the wine (\\(g/dm^4\\))\ncitric.acid: The amount of citric acid in the wine (\\(g/dm^3\\))\nresidual.sugar: The amount of residual sugar in the wine (\\(g/dm^3\\))\nchlorides: The amount of salt in the wine (\\(g/dm^3\\))\nfree.sulfur.dioxide: The amount of free sulfur dioxide in the wine (\\(mg/dm^3\\))\ntotal.sulfur.dioxide: The amount of total sulfur dioxide in the wine (\\(mg/dm^3\\))\ndensity: The density of the wine (\\(g/dm^3\\))\npH: The \\(pH\\) value of the wine\nsulphates: The amount of sulphates in the wine (\\(g/dm^3\\))\nalcohol: The alcohol content of the wine (\\(\\% vol\\))\nquality: The quality score of the wine (0-10)\n\nAfter removing the duplicate rows from our data set, we are left with 3961 observations of the above 11 variables minus quality column variable, and introduced a new variable good as our response:\n\ngood: A binary variable indicating whether the wine is good (quality \\(\\geq\\) 7) or not (quality \\(&lt;\\) 7)."
  },
  {
    "objectID": "index.html#footnotes",
    "href": "index.html#footnotes",
    "title": "Introduction to Statistical Learning",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nP. Cortez, A. Cerdeira, F. Almeida, T. Matos and J. Reis. Modeling wine preferences by data mining from physicochemical properties. In Decision Support Systems, Elsevier, 47(4):547-553, 2009.↩︎"
  },
  {
    "objectID": "src/analysis.html",
    "href": "src/analysis.html",
    "title": "Exploratory Analysis",
    "section": "",
    "text": "Show/Hide Code\nwine.data &lt;- read.csv(\"dataset\\\\winequality-white.csv\", sep=\";\", header = T)\n\n# Removing duplicate Rows, mutate our categorical response good\nwine.data_cleaned &lt;-  wine.data %&gt;% mutate(good = ifelse(quality&gt;=7, 1, 0)) %&gt;% distinct() %&gt;% dplyr::select(c(1:11, 13))\n\nhead(wine.data)\n\n\n  fixed.acidity volatile.acidity citric.acid residual.sugar chlorides\n1           7.0             0.27        0.36           20.7     0.045\n2           6.3             0.30        0.34            1.6     0.049\n3           8.1             0.28        0.40            6.9     0.050\n4           7.2             0.23        0.32            8.5     0.058\n5           7.2             0.23        0.32            8.5     0.058\n6           8.1             0.28        0.40            6.9     0.050\n  free.sulfur.dioxide total.sulfur.dioxide density   pH sulphates alcohol\n1                  45                  170  1.0010 3.00      0.45     8.8\n2                  14                  132  0.9940 3.30      0.49     9.5\n3                  30                   97  0.9951 3.26      0.44    10.1\n4                  47                  186  0.9956 3.19      0.40     9.9\n5                  47                  186  0.9956 3.19      0.40     9.9\n6                  30                   97  0.9951 3.26      0.44    10.1\n  quality\n1       6\n2       6\n3       6\n4       6\n5       6\n6       6\n\n\nShow/Hide Code\nhead(wine.data_cleaned)\n\n\n  fixed.acidity volatile.acidity citric.acid residual.sugar chlorides\n1           7.0             0.27        0.36           20.7     0.045\n2           6.3             0.30        0.34            1.6     0.049\n3           8.1             0.28        0.40            6.9     0.050\n4           7.2             0.23        0.32            8.5     0.058\n5           6.2             0.32        0.16            7.0     0.045\n6           8.1             0.22        0.43            1.5     0.044\n  free.sulfur.dioxide total.sulfur.dioxide density   pH sulphates alcohol good\n1                  45                  170  1.0010 3.00      0.45     8.8    0\n2                  14                  132  0.9940 3.30      0.49     9.5    0\n3                  30                   97  0.9951 3.26      0.44    10.1    0\n4                  47                  186  0.9956 3.19      0.40     9.9    0\n5                  30                  136  0.9949 3.18      0.47     9.6    0\n6                  28                  129  0.9938 3.22      0.45    11.0    0"
  },
  {
    "objectID": "src/analysis.html#data-import",
    "href": "src/analysis.html#data-import",
    "title": "Exploratory Analysis",
    "section": "",
    "text": "Show/Hide Code\nwine.data &lt;- read.csv(\"dataset\\\\winequality-white.csv\", sep=\";\", header = T)\n\n# Removing duplicate Rows, mutate our categorical response good\nwine.data_cleaned &lt;-  wine.data %&gt;% mutate(good = ifelse(quality&gt;=7, 1, 0)) %&gt;% distinct() %&gt;% dplyr::select(c(1:11, 13))\n\nhead(wine.data)\n\n\n  fixed.acidity volatile.acidity citric.acid residual.sugar chlorides\n1           7.0             0.27        0.36           20.7     0.045\n2           6.3             0.30        0.34            1.6     0.049\n3           8.1             0.28        0.40            6.9     0.050\n4           7.2             0.23        0.32            8.5     0.058\n5           7.2             0.23        0.32            8.5     0.058\n6           8.1             0.28        0.40            6.9     0.050\n  free.sulfur.dioxide total.sulfur.dioxide density   pH sulphates alcohol\n1                  45                  170  1.0010 3.00      0.45     8.8\n2                  14                  132  0.9940 3.30      0.49     9.5\n3                  30                   97  0.9951 3.26      0.44    10.1\n4                  47                  186  0.9956 3.19      0.40     9.9\n5                  47                  186  0.9956 3.19      0.40     9.9\n6                  30                   97  0.9951 3.26      0.44    10.1\n  quality\n1       6\n2       6\n3       6\n4       6\n5       6\n6       6\n\n\nShow/Hide Code\nhead(wine.data_cleaned)\n\n\n  fixed.acidity volatile.acidity citric.acid residual.sugar chlorides\n1           7.0             0.27        0.36           20.7     0.045\n2           6.3             0.30        0.34            1.6     0.049\n3           8.1             0.28        0.40            6.9     0.050\n4           7.2             0.23        0.32            8.5     0.058\n5           6.2             0.32        0.16            7.0     0.045\n6           8.1             0.22        0.43            1.5     0.044\n  free.sulfur.dioxide total.sulfur.dioxide density   pH sulphates alcohol good\n1                  45                  170  1.0010 3.00      0.45     8.8    0\n2                  14                  132  0.9940 3.30      0.49     9.5    0\n3                  30                   97  0.9951 3.26      0.44    10.1    0\n4                  47                  186  0.9956 3.19      0.40     9.9    0\n5                  30                  136  0.9949 3.18      0.47     9.6    0\n6                  28                  129  0.9938 3.22      0.45    11.0    0"
  },
  {
    "objectID": "src/analysis.html#data-analysis",
    "href": "src/analysis.html#data-analysis",
    "title": "Exploratory Analysis",
    "section": "Data Analysis",
    "text": "Data Analysis\n\n\nShow/Hide Code\ndim(wine.data)\n\n\n[1] 4898   12\n\n\nShow/Hide Code\ndim(wine.data_cleaned)\n\n\n[1] 3961   12\n\n\nShow/Hide Code\nstr(wine.data)\n\n\n'data.frame':   4898 obs. of  12 variables:\n $ fixed.acidity       : num  7 6.3 8.1 7.2 7.2 8.1 6.2 7 6.3 8.1 ...\n $ volatile.acidity    : num  0.27 0.3 0.28 0.23 0.23 0.28 0.32 0.27 0.3 0.22 ...\n $ citric.acid         : num  0.36 0.34 0.4 0.32 0.32 0.4 0.16 0.36 0.34 0.43 ...\n $ residual.sugar      : num  20.7 1.6 6.9 8.5 8.5 6.9 7 20.7 1.6 1.5 ...\n $ chlorides           : num  0.045 0.049 0.05 0.058 0.058 0.05 0.045 0.045 0.049 0.044 ...\n $ free.sulfur.dioxide : num  45 14 30 47 47 30 30 45 14 28 ...\n $ total.sulfur.dioxide: num  170 132 97 186 186 97 136 170 132 129 ...\n $ density             : num  1.001 0.994 0.995 0.996 0.996 ...\n $ pH                  : num  3 3.3 3.26 3.19 3.19 3.26 3.18 3 3.3 3.22 ...\n $ sulphates           : num  0.45 0.49 0.44 0.4 0.4 0.44 0.47 0.45 0.49 0.45 ...\n $ alcohol             : num  8.8 9.5 10.1 9.9 9.9 10.1 9.6 8.8 9.5 11 ...\n $ quality             : int  6 6 6 6 6 6 6 6 6 6 ...\n\n\nShow/Hide Code\nstr(wine.data_cleaned)\n\n\n'data.frame':   3961 obs. of  12 variables:\n $ fixed.acidity       : num  7 6.3 8.1 7.2 6.2 8.1 8.1 8.6 7.9 6.6 ...\n $ volatile.acidity    : num  0.27 0.3 0.28 0.23 0.32 0.22 0.27 0.23 0.18 0.16 ...\n $ citric.acid         : num  0.36 0.34 0.4 0.32 0.16 0.43 0.41 0.4 0.37 0.4 ...\n $ residual.sugar      : num  20.7 1.6 6.9 8.5 7 1.5 1.45 4.2 1.2 1.5 ...\n $ chlorides           : num  0.045 0.049 0.05 0.058 0.045 0.044 0.033 0.035 0.04 0.044 ...\n $ free.sulfur.dioxide : num  45 14 30 47 30 28 11 17 16 48 ...\n $ total.sulfur.dioxide: num  170 132 97 186 136 129 63 109 75 143 ...\n $ density             : num  1.001 0.994 0.995 0.996 0.995 ...\n $ pH                  : num  3 3.3 3.26 3.19 3.18 3.22 2.99 3.14 3.18 3.54 ...\n $ sulphates           : num  0.45 0.49 0.44 0.4 0.47 0.45 0.56 0.53 0.63 0.52 ...\n $ alcohol             : num  8.8 9.5 10.1 9.9 9.6 11 12 9.7 10.8 12.4 ...\n $ good                : num  0 0 0 0 0 0 0 0 0 1 ...\n\n\nShow/Hide Code\nsummary(wine.data)\n\n\n fixed.acidity    volatile.acidity  citric.acid     residual.sugar  \n Min.   : 3.800   Min.   :0.0800   Min.   :0.0000   Min.   : 0.600  \n 1st Qu.: 6.300   1st Qu.:0.2100   1st Qu.:0.2700   1st Qu.: 1.700  \n Median : 6.800   Median :0.2600   Median :0.3200   Median : 5.200  \n Mean   : 6.855   Mean   :0.2782   Mean   :0.3342   Mean   : 6.391  \n 3rd Qu.: 7.300   3rd Qu.:0.3200   3rd Qu.:0.3900   3rd Qu.: 9.900  \n Max.   :14.200   Max.   :1.1000   Max.   :1.6600   Max.   :65.800  \n   chlorides       free.sulfur.dioxide total.sulfur.dioxide    density      \n Min.   :0.00900   Min.   :  2.00      Min.   :  9.0        Min.   :0.9871  \n 1st Qu.:0.03600   1st Qu.: 23.00      1st Qu.:108.0        1st Qu.:0.9917  \n Median :0.04300   Median : 34.00      Median :134.0        Median :0.9937  \n Mean   :0.04577   Mean   : 35.31      Mean   :138.4        Mean   :0.9940  \n 3rd Qu.:0.05000   3rd Qu.: 46.00      3rd Qu.:167.0        3rd Qu.:0.9961  \n Max.   :0.34600   Max.   :289.00      Max.   :440.0        Max.   :1.0390  \n       pH          sulphates         alcohol         quality     \n Min.   :2.720   Min.   :0.2200   Min.   : 8.00   Min.   :3.000  \n 1st Qu.:3.090   1st Qu.:0.4100   1st Qu.: 9.50   1st Qu.:5.000  \n Median :3.180   Median :0.4700   Median :10.40   Median :6.000  \n Mean   :3.188   Mean   :0.4898   Mean   :10.51   Mean   :5.878  \n 3rd Qu.:3.280   3rd Qu.:0.5500   3rd Qu.:11.40   3rd Qu.:6.000  \n Max.   :3.820   Max.   :1.0800   Max.   :14.20   Max.   :9.000  \n\n\nShow/Hide Code\nsummary(wine.data_cleaned)\n\n\n fixed.acidity    volatile.acidity  citric.acid     residual.sugar  \n Min.   : 3.800   Min.   :0.0800   Min.   :0.0000   Min.   : 0.600  \n 1st Qu.: 6.300   1st Qu.:0.2100   1st Qu.:0.2700   1st Qu.: 1.600  \n Median : 6.800   Median :0.2600   Median :0.3200   Median : 4.700  \n Mean   : 6.839   Mean   :0.2805   Mean   :0.3343   Mean   : 5.915  \n 3rd Qu.: 7.300   3rd Qu.:0.3300   3rd Qu.:0.3900   3rd Qu.: 8.900  \n Max.   :14.200   Max.   :1.1000   Max.   :1.6600   Max.   :65.800  \n   chlorides       free.sulfur.dioxide total.sulfur.dioxide    density      \n Min.   :0.00900   Min.   :  2.00      Min.   :  9.0        Min.   :0.9871  \n 1st Qu.:0.03500   1st Qu.: 23.00      1st Qu.:106.0        1st Qu.:0.9916  \n Median :0.04200   Median : 33.00      Median :133.0        Median :0.9935  \n Mean   :0.04591   Mean   : 34.89      Mean   :137.2        Mean   :0.9938  \n 3rd Qu.:0.05000   3rd Qu.: 45.00      3rd Qu.:166.0        3rd Qu.:0.9957  \n Max.   :0.34600   Max.   :289.00      Max.   :440.0        Max.   :1.0390  \n       pH          sulphates         alcohol           good       \n Min.   :2.720   Min.   :0.2200   Min.   : 8.00   Min.   :0.0000  \n 1st Qu.:3.090   1st Qu.:0.4100   1st Qu.: 9.50   1st Qu.:0.0000  \n Median :3.180   Median :0.4800   Median :10.40   Median :0.0000  \n Mean   :3.195   Mean   :0.4904   Mean   :10.59   Mean   :0.2083  \n 3rd Qu.:3.290   3rd Qu.:0.5500   3rd Qu.:11.40   3rd Qu.:0.0000  \n Max.   :3.820   Max.   :1.0800   Max.   :14.20   Max.   :1.0000  \n\n\nShow/Hide Code\n# Check for NAs in dataset\nsum(is.na(wine.data))\n\n\n[1] 0\n\n\nShow/Hide Code\n# Counts at each combination of response's factor levels\ntable(wine.data$quality)\n\n\n\n   3    4    5    6    7    8    9 \n  20  163 1457 2198  880  175    5"
  },
  {
    "objectID": "src/analysis.html#data-histograms",
    "href": "src/analysis.html#data-histograms",
    "title": "Exploratory Analysis",
    "section": "Data Histograms",
    "text": "Data Histograms\n\n\nShow/Hide Code\nwine.colnames &lt;- colnames(wine.data[, 1:12])\nnum_plots     &lt;- length(wine.colnames)\nnum_rows      &lt;- ceiling(num_plots/3)\n\n\n# Create an empty list to store plots\ngrid_arr      &lt;- list()\n\n\n# Loop over each column name in the wine.colnames vector\nfor(i in 1:num_plots) {\n  # Create a ggplot object for the current column using aes\n  plt &lt;- ggplot(data = wine.data, aes_string(x = wine.colnames[i])) +\n    geom_histogram(binwidth = diff(range(wine.data[[wine.colnames[i]]]))/30, \n                   color = \"black\", fill = \"slategray3\") +\n    labs(x = wine.colnames[i], y = \"Frequency\") +\n    theme_bw()\n  \n  # Add the current plot to the grid_arr list\n  grid_arr[[i]] &lt;- plt\n}\n\ngrid_arr &lt;- do.call(gridExtra::grid.arrange, c(grid_arr, ncol = 3))"
  },
  {
    "objectID": "src/analysis.html#data-relationships",
    "href": "src/analysis.html#data-relationships",
    "title": "Exploratory Analysis",
    "section": "Data Relationships",
    "text": "Data Relationships\n\n\nShow/Hide Code\nreshape2::melt(wine.data[, 1:12], \"quality\") %&gt;% \n  ggplot(aes(value, quality, color = variable)) +  \n  geom_point() + \n  geom_smooth(aes(value,quality, colour=variable), method=lm, se=FALSE)+\n  facet_wrap(.~variable, scales = \"free\")\n\n\n\n\n\n\n\n\n\nShow/Hide Code\n# Collinearity between Attributes\ncor(wine.data_cleaned) %&gt;% \n  corrplot::corrplot(method = 'number',  type = \"lower\", tl.col = \"steelblue\", number.cex = 0.5)"
  },
  {
    "objectID": "src/analysis.html#data-split",
    "href": "src/analysis.html#data-split",
    "title": "Exploratory Analysis",
    "section": "Data Split",
    "text": "Data Split\n\n\nShow/Hide Code\nset.seed(123)\n# Splitting the dataset into train and test (7/10th for train remaining for test)\ninTrain &lt;- caret::createDataPartition(wine.data_cleaned$good, p = 7/10, list = F)\ntrain &lt;- wine.data_cleaned[inTrain,]\ntest  &lt;- wine.data_cleaned[-inTrain,]\n\n\n# Convert the outcome variable to a factor with two levels\ntrain$good &lt;- as.factor(train$good)\ntest$good &lt;- as.factor(test$good)\n\n# Save data for building models in the next step\nsave(wine.data_cleaned, file = \"dataset\\\\wine.data_cleaned.Rdata\")\nsave(train, file = \"dataset\\\\train.Rdata\")\nsave(test, file = \"dataset\\\\test.Rdata\")"
  },
  {
    "objectID": "src/bagging.html",
    "href": "src/bagging.html",
    "title": "Bagging",
    "section": "",
    "text": "Show/Hide Code\n#----------------#\n#----Bagging-----#\n#----------------#\nset.seed(1234)\ntrain_control &lt;- trainControl(method = \"cv\", number = 10)\n\nset.seed(1234)\nbag_model &lt;- train(good ~ ., \n               data = train, \n               method = \"treebag\", \n               trControl = train_control)\n\nsave(bag_model, file = \"dataset\\\\model\\\\bag.model_kfoldCV.Rdata\")"
  },
  {
    "objectID": "src/bagging.html#model-construction",
    "href": "src/bagging.html#model-construction",
    "title": "Bagging",
    "section": "",
    "text": "Show/Hide Code\n#----------------#\n#----Bagging-----#\n#----------------#\nset.seed(1234)\ntrain_control &lt;- trainControl(method = \"cv\", number = 10)\n\nset.seed(1234)\nbag_model &lt;- train(good ~ ., \n               data = train, \n               method = \"treebag\", \n               trControl = train_control)\n\nsave(bag_model, file = \"dataset\\\\model\\\\bag.model_kfoldCV.Rdata\")"
  },
  {
    "objectID": "src/bagging.html#k-fold-cv",
    "href": "src/bagging.html#k-fold-cv",
    "title": "Bagging",
    "section": "K-fold CV",
    "text": "K-fold CV\n\n\nShow/Hide Code\n# Data Import\nload(\"dataset\\\\train.Rdata\")\nload(\"dataset\\\\test.Rdata\")\n\n# Model Import\nload(\"dataset\\\\model\\\\bag.model_kfoldCV.Rdata\")\n\nbag.predictions &lt;- predict(bag_model, newdata = test)\n\nconfusionMatrix(bag.predictions, test$good)\n\n\nConfusion Matrix and Statistics\n\n          Reference\nPrediction   0   1\n         0 882 118\n         1  79 109\n                                          \n               Accuracy : 0.8342          \n                 95% CI : (0.8118, 0.8549)\n    No Information Rate : 0.8089          \n    P-Value [Acc &gt; NIR] : 0.013640        \n                                          \n                  Kappa : 0.4259          \n                                          \n Mcnemar's Test P-Value : 0.006781        \n                                          \n            Sensitivity : 0.9178          \n            Specificity : 0.4802          \n         Pos Pred Value : 0.8820          \n         Neg Pred Value : 0.5798          \n             Prevalence : 0.8089          \n         Detection Rate : 0.7424          \n   Detection Prevalence : 0.8418          \n      Balanced Accuracy : 0.6990          \n                                          \n       'Positive' Class : 0               \n                                          \n\n\nShow/Hide Code\nbag.predictions &lt;- as.numeric(bag.predictions)\npred_obj &lt;- prediction(bag.predictions, test$good)\nauc_val &lt;- performance(pred_obj, \"auc\")@y.values[[1]]\nauc_val\n\n\n[1] 0.6989851\n\n\nShow/Hide Code\nroc_obj &lt;- performance(pred_obj, \"tpr\", \"fpr\")\nplot(roc_obj, colorize = TRUE, lwd = 2,\n     xlab = \"False Positive Rate\", \n     ylab = \"True Positive Rate\",\n     main = \"ROC Curves from Repeated CV\")\npoints(auc_val, 1 - auc_val, \n       col = \"steelblue\", \n       pch = 21)\nabline(a = 0, b = 1)\n\n\nShow/Hide Code\nbag.kfoldCV.ROC.plot &lt;- recordPlot()\n\n\n\nTuned\n\n\n\n\n\n\n\n\n\n\nResampling Method\nError Rate\nSensitivity\nSpecificity\nAUC\n\n\n\n\nBagging\n0.1658\n0.9178\n0.4802\n0.6989851\n\n\nBagging (Tuned)\n0.xxxx\n0.xxxx\n0.xxxx\n0.xxxxxxx"
  },
  {
    "objectID": "src/boosting.html",
    "href": "src/boosting.html",
    "title": "Boosting",
    "section": "",
    "text": "Show/Hide Code\n#--------------#\n#----boost-----#\n#--------------#\nset.seed(1234)\ntrain_control &lt;- trainControl(method = \"cv\", number = 10)\n\nset.seed(1234)\nboost_model &lt;- train(good ~ ., \n               data = train, \n               method = \"gbm\", \n               trControl = train_control)\n\nsave(boost_model, file = \"dataset\\\\model\\\\boost.model_kfoldCV.Rdata\")"
  },
  {
    "objectID": "src/boosting.html#model-construction",
    "href": "src/boosting.html#model-construction",
    "title": "Boosting",
    "section": "",
    "text": "Show/Hide Code\n#--------------#\n#----boost-----#\n#--------------#\nset.seed(1234)\ntrain_control &lt;- trainControl(method = \"cv\", number = 10)\n\nset.seed(1234)\nboost_model &lt;- train(good ~ ., \n               data = train, \n               method = \"gbm\", \n               trControl = train_control)\n\nsave(boost_model, file = \"dataset\\\\model\\\\boost.model_kfoldCV.Rdata\")"
  },
  {
    "objectID": "src/boosting.html#k-fold-cv",
    "href": "src/boosting.html#k-fold-cv",
    "title": "Boosting",
    "section": "K-fold CV",
    "text": "K-fold CV\n\n\nShow/Hide Code\n# Data Import\nload(\"dataset\\\\train.Rdata\")\nload(\"dataset\\\\test.Rdata\")\n\n# Model Import\nload(\"dataset\\\\model\\\\boost.model_kfoldCV.Rdata\")\n\nboost.predictions &lt;- predict(boost_model, newdata = test)\n\nconfusionMatrix(boost.predictions, test$good)\n\n\nConfusion Matrix and Statistics\n\n          Reference\nPrediction   0   1\n         0 889 136\n         1  72  91\n                                          \n               Accuracy : 0.8249          \n                 95% CI : (0.8021, 0.8461)\n    No Information Rate : 0.8089          \n    P-Value [Acc &gt; NIR] : 0.085           \n                                          \n                  Kappa : 0.3653          \n                                          \n Mcnemar's Test P-Value : 1.252e-05       \n                                          \n            Sensitivity : 0.9251          \n            Specificity : 0.4009          \n         Pos Pred Value : 0.8673          \n         Neg Pred Value : 0.5583          \n             Prevalence : 0.8089          \n         Detection Rate : 0.7483          \n   Detection Prevalence : 0.8628          \n      Balanced Accuracy : 0.6630          \n                                          \n       'Positive' Class : 0               \n                                          \n\n\nShow/Hide Code\nboost.predictions &lt;- as.numeric(boost.predictions)\npred_obj &lt;- prediction(boost.predictions, test$good)\nauc_val &lt;- performance(pred_obj, \"auc\")@y.values[[1]]\nauc_val\n\n\n[1] 0.6629796\n\n\nShow/Hide Code\nroc_obj &lt;- performance(pred_obj, \"tpr\", \"fpr\")\nplot(roc_obj, colorize = TRUE, lwd = 2,\n     xlab = \"False Positive Rate\", \n     ylab = \"True Positive Rate\",\n     main = \"ROC Curves from Repeated CV\")\npoints(auc_val, 1 - auc_val, \n       col = \"steelblue\", \n       pch = 21)\nabline(a = 0, b = 1)\n\n\nShow/Hide Code\nboost.kfoldCV.ROC.plot &lt;- recordPlot()\n\n\n\nTuned\n\n\n\n\n\n\n\n\n\n\nResampling Method\nError Rate\nSensitivity\nSpecificity\nAUC\n\n\n\n\nBoost\n0.1751\n0.9251\n0.4009\n0.6629796\n\n\nBoost (Tuned)\n0.xxxx\n0.xxxx\n0.xxxx\n0.xxxxxxx"
  },
  {
    "objectID": "src/decisionTree.html",
    "href": "src/decisionTree.html",
    "title": "Decision Tree",
    "section": "",
    "text": "The CART (Classification and Regression Trees) algorithm is a decision tree method. CART is a popular algorithm used for both classification and regression problems. For our classification task, it constructs a binary tree in which each internal node represents a test on a single feature, and each leaf node represents a class label or a numeric value. The splitting of nodes in the tree is based on a measure of impurity such as Gini impurity or entropy. The CART algorithm is often used in applications such as finance, marketing, and healthcare."
  },
  {
    "objectID": "src/decisionTree.html#model-construction",
    "href": "src/decisionTree.html#model-construction",
    "title": "Decision Tree",
    "section": "Model Construction",
    "text": "Model Construction\n\n\nShow/Hide Code\n#----------------------#\n#----Decision Tree-----#\n#----------------------#\nset.seed(1234)\ntrain_control &lt;- trainControl(method = \"cv\", number = 10)\n\nset.seed(1234)\ndc_model &lt;- train(good ~ ., \n               data = train, \n               method = \"rpart2\", \n               trControl = train_control,\n               na.action = na.omit)\n\nsave(dc_model, file = \"dataset\\\\model\\\\dc.model_kfoldCV.Rdata\")\n\n\n#----------------------------#\n#----Decision Tree (Mod)-----#\n#----------------------------#\nset.seed(1234)\ntrain_control &lt;- trainControl(method = \"cv\", number = 10)\n\nset.seed(1234)\ndc_model &lt;- train(good ~ ., \n               data = train, \n               method = \"rpart\", \n               trControl = train_control,\n               tuneLength = 5,\n               tuneGrid = data.frame(cp = seq(0.001, 0.1, by = 0.001)))\n\nsave(dc_model, file = \"dataset\\\\model\\\\dc.model_kfoldCV_mod.Rdata\")"
  },
  {
    "objectID": "src/decisionTree.html#k-fold-cv",
    "href": "src/decisionTree.html#k-fold-cv",
    "title": "Decision Tree",
    "section": "K-fold CV",
    "text": "K-fold CV\n\n\nShow/Hide Code\n# Data Import\nload(\"dataset\\\\wine.data.Rdata\")\nload(\"dataset\\\\train.Rdata\")\nload(\"dataset\\\\test.Rdata\")\n\n# Model Import\nload(\"dataset\\\\model\\\\dc.model_kfoldCV.Rdata\")\n\ndc.predictions &lt;- predict(dc_model, newdata = test)\n\nconfusionMatrix(dc.predictions, test$good)\n\n\nConfusion Matrix and Statistics\n\n          Reference\nPrediction   0   1\n         0 868 122\n         1  93 105\n                                          \n               Accuracy : 0.819           \n                 95% CI : (0.7959, 0.8405)\n    No Information Rate : 0.8089          \n    P-Value [Acc &gt; NIR] : 0.19862         \n                                          \n                  Kappa : 0.3845          \n                                          \n Mcnemar's Test P-Value : 0.05619         \n                                          \n            Sensitivity : 0.9032          \n            Specificity : 0.4626          \n         Pos Pred Value : 0.8768          \n         Neg Pred Value : 0.5303          \n             Prevalence : 0.8089          \n         Detection Rate : 0.7306          \n   Detection Prevalence : 0.8333          \n      Balanced Accuracy : 0.6829          \n                                          \n       'Positive' Class : 0               \n                                          \n\n\nShow/Hide Code\ndc.predictions &lt;- as.numeric(dc.predictions)\npred_obj &lt;- prediction(dc.predictions, test$good)\nauc_val &lt;- performance(pred_obj, \"auc\")@y.values[[1]]\nauc_val\n\n\n[1] 0.6828904\n\n\nShow/Hide Code\nroc_obj &lt;- performance(pred_obj, \"tpr\", \"fpr\")\nplot(roc_obj, colorize = TRUE, lwd = 2,\n     xlab = \"False Positive Rate\", \n     ylab = \"True Positive Rate\",\n     main = \"ROC Curves from Repeated CV\")\npoints(auc_val, 1 - auc_val, \n       col = \"steelblue\", \n       pch = 21)\nabline(a = 0, b = 1)\n\n\nShow/Hide Code\ndc.kfoldCV.ROC.plot &lt;- recordPlot()\n\n\n\nTuned\n\n\nShow/Hide Code\n# Model Import\nload(\"dataset\\\\model\\\\dc.model_kfoldCV_mod.Rdata\")\n\ndc.predictions &lt;- predict(dc_model, newdata = test)\n\nconfusionMatrix(dc.predictions, test$good)\n\n\nConfusion Matrix and Statistics\n\n          Reference\nPrediction   0   1\n         0 889 136\n         1  72  91\n                                          \n               Accuracy : 0.8249          \n                 95% CI : (0.8021, 0.8461)\n    No Information Rate : 0.8089          \n    P-Value [Acc &gt; NIR] : 0.085           \n                                          \n                  Kappa : 0.3653          \n                                          \n Mcnemar's Test P-Value : 1.252e-05       \n                                          \n            Sensitivity : 0.9251          \n            Specificity : 0.4009          \n         Pos Pred Value : 0.8673          \n         Neg Pred Value : 0.5583          \n             Prevalence : 0.8089          \n         Detection Rate : 0.7483          \n   Detection Prevalence : 0.8628          \n      Balanced Accuracy : 0.6630          \n                                          \n       'Positive' Class : 0               \n                                          \n\n\nShow/Hide Code\ndc.predictions &lt;- as.numeric(dc.predictions)\npred_obj &lt;- prediction(dc.predictions, test$good)\nauc_val &lt;- performance(pred_obj, \"auc\")@y.values[[1]]\nauc_val\n\n\n[1] 0.6629796\n\n\nShow/Hide Code\nroc_obj &lt;- performance(pred_obj, \"tpr\", \"fpr\")\nplot(roc_obj, colorize = TRUE, lwd = 2,\n     xlab = \"False Positive Rate\", \n     ylab = \"True Positive Rate\",\n     main = \"ROC Curves from Repeated CV\")\npoints(auc_val, 1 - auc_val, \n       col = \"steelblue\", \n       pch = 21)\nabline(a = 0, b = 1)\n\n\nShow/Hide Code\ndc.kfoldCV_mod.ROC.plot &lt;- recordPlot()\n\n\n\n\n\n\n\n\n\n\n\n\nResampling Method\nError Rate\nSensitivity\nSpecificity\nAUC\n\n\n\n\nCART\n0.181\n0.9032\n0.4626\n0.6828904\n\n\nCART (Tuned)\n0.1751\n0.9251\n0.4009\n0.6629796"
  },
  {
    "objectID": "src/knn.html",
    "href": "src/knn.html",
    "title": "K Nearest Neighbor Classifier",
    "section": "",
    "text": "Show/Hide Code\n#--------------------#\n#-----K-fold CV------#\n#--------------------#\n\nset.seed(1234)\n# Define the training control object for 10-fold cross-validation\ntrain_control &lt;- trainControl(method = \"cv\", number = 10)\n\n# Train the KNN model using 10-fold cross-validation\n# tuneLength argument to specify the range of values of K to be considered for tuning\nset.seed(1234)\nknn_model &lt;- train(good ~ ., \n                   data = train, \n                   method = \"knn\", \n                   trControl = train_control)\n\n# Save the model into .Rdata for future import \nsave(knn_model, file = \"dataset\\\\model\\\\knn.model_kfoldCV.Rdata\")\n\n\n#--------------------------#\n#-----K-fold CV (Mod)------#\n#--------------------------#\n\nset.seed(1234)\ntrain_control &lt;- trainControl(method = \"cv\", number = 10)\n\nset.seed(1234)\nknn_model &lt;- train(good ~ ., \n                   data = train, \n                   method = \"knn\", \n                   trControl = train_control, \n                   tuneLength = 10)\n\n# Save the model into .Rdata for future import \nsave(knn_model, file = \"dataset\\\\model\\\\knn.model_kfoldCV_mod.Rdata\")\n\n\n#--------------------#\n#----Hold-out CV-----#\n#--------------------#\n\nset.seed(1234)\ntrain_control &lt;- trainControl(method = \"none\",)\n\nset.seed(1234)\nknn_model &lt;- train(good ~ ., \n                   data = train, \n                   method = \"knn\")\n\nsave(knn_model, file = \"dataset\\\\model\\\\knn.model_holdoutCV.Rdata\")\n\n\n#--------------------------#\n#----Hold-out CV (Mod)-----#\n#--------------------------#\n\nset.seed(1234)\ntrain_control &lt;- trainControl(method = \"none\",)\n\nset.seed(1234)\nknn_model &lt;- train(good ~ ., \n                   data = train, \n                   method = \"knn\",\n                   tuneGrid = expand.grid(k=1:30))\n\nsave(knn_model, file = \"dataset\\\\model\\\\knn.model_holdoutCV_mod.Rdata\")\n\n\n#--------------------#\n#-------LOOCV--------#\n#--------------------#\n\nset.seed(1234)\ntrain_control &lt;- trainControl(method = \"LOOCV\")\n\nset.seed(1234)\nknn_model &lt;- train(good ~ ., \n                   data = train, \n                   method = \"knn\", \n                   trControl = train_control)\n\nsave(knn_model, file = \"dataset\\\\model\\\\knn.model_looCV.Rdata\")\n\n\n#--------------------------#\n#-------LOOCV (Mod)--------#\n#--------------------------#\n\nset.seed(1234)\ntrain_control &lt;- trainControl(method = \"LOOCV\")\n\nset.seed(1234)\nknn_model &lt;- train(good ~ ., \n                   data = train, \n                   method = \"knn\", \n                   trControl = train_control,\n                   tuneLength = 10,\n                   tuneGrid = expand.grid(k = 1:20))\n\nsave(knn_model, file = \"dataset\\\\model\\\\knn.model_looCV_mod.Rdata\")\n\n\n#--------------------#\n#----Repeated CV-----#\n#--------------------#\n\nset.seed(1234)\ntrain_control &lt;- trainControl(method = \"repeatedcv\", number = 10, repeats = 5)\n\nset.seed(1234)\nknn_model &lt;- train(good ~ ., \n                   data = train, \n                   method = \"knn\", \n                   trControl = train_control)\n\nsave(knn_model, file = \"dataset\\\\model\\\\knn.model_repeatedCV.Rdata\")\n\n\n#--------------------------#\n#----Repeated CV (Mod)-----#\n#--------------------------#\n\nset.seed(1234)\ntrain_control &lt;- trainControl(method = \"repeatedcv\", number = 10, repeats = 5)\n\nkknn.grid &lt;- expand.grid(kmax = c(3, 5, 7 ,9, 11), distance = c(1, 2, 3),\n                         kernel = c(\"rectangular\", \"gaussian\", \"cos\"))\n\nset.seed(1234)\nknn_model &lt;- train(good ~ ., \n                   data = train, \n                   method = \"kknn\",\n                   trControl = train_control, \n                   tuneGrid = kknn.grid,\n                   preProcess = c(\"center\", \"scale\"))\n\nsave(knn_model, file = \"dataset\\\\model\\\\knn.model_repeatedCV_mod.Rdata\")"
  },
  {
    "objectID": "src/knn.html#model-construction",
    "href": "src/knn.html#model-construction",
    "title": "K Nearest Neighbor Classifier",
    "section": "",
    "text": "Show/Hide Code\n#--------------------#\n#-----K-fold CV------#\n#--------------------#\n\nset.seed(1234)\n# Define the training control object for 10-fold cross-validation\ntrain_control &lt;- trainControl(method = \"cv\", number = 10)\n\n# Train the KNN model using 10-fold cross-validation\n# tuneLength argument to specify the range of values of K to be considered for tuning\nset.seed(1234)\nknn_model &lt;- train(good ~ ., \n                   data = train, \n                   method = \"knn\", \n                   trControl = train_control)\n\n# Save the model into .Rdata for future import \nsave(knn_model, file = \"dataset\\\\model\\\\knn.model_kfoldCV.Rdata\")\n\n\n#--------------------------#\n#-----K-fold CV (Mod)------#\n#--------------------------#\n\nset.seed(1234)\ntrain_control &lt;- trainControl(method = \"cv\", number = 10)\n\nset.seed(1234)\nknn_model &lt;- train(good ~ ., \n                   data = train, \n                   method = \"knn\", \n                   trControl = train_control, \n                   tuneLength = 10)\n\n# Save the model into .Rdata for future import \nsave(knn_model, file = \"dataset\\\\model\\\\knn.model_kfoldCV_mod.Rdata\")\n\n\n#--------------------#\n#----Hold-out CV-----#\n#--------------------#\n\nset.seed(1234)\ntrain_control &lt;- trainControl(method = \"none\",)\n\nset.seed(1234)\nknn_model &lt;- train(good ~ ., \n                   data = train, \n                   method = \"knn\")\n\nsave(knn_model, file = \"dataset\\\\model\\\\knn.model_holdoutCV.Rdata\")\n\n\n#--------------------------#\n#----Hold-out CV (Mod)-----#\n#--------------------------#\n\nset.seed(1234)\ntrain_control &lt;- trainControl(method = \"none\",)\n\nset.seed(1234)\nknn_model &lt;- train(good ~ ., \n                   data = train, \n                   method = \"knn\",\n                   tuneGrid = expand.grid(k=1:30))\n\nsave(knn_model, file = \"dataset\\\\model\\\\knn.model_holdoutCV_mod.Rdata\")\n\n\n#--------------------#\n#-------LOOCV--------#\n#--------------------#\n\nset.seed(1234)\ntrain_control &lt;- trainControl(method = \"LOOCV\")\n\nset.seed(1234)\nknn_model &lt;- train(good ~ ., \n                   data = train, \n                   method = \"knn\", \n                   trControl = train_control)\n\nsave(knn_model, file = \"dataset\\\\model\\\\knn.model_looCV.Rdata\")\n\n\n#--------------------------#\n#-------LOOCV (Mod)--------#\n#--------------------------#\n\nset.seed(1234)\ntrain_control &lt;- trainControl(method = \"LOOCV\")\n\nset.seed(1234)\nknn_model &lt;- train(good ~ ., \n                   data = train, \n                   method = \"knn\", \n                   trControl = train_control,\n                   tuneLength = 10,\n                   tuneGrid = expand.grid(k = 1:20))\n\nsave(knn_model, file = \"dataset\\\\model\\\\knn.model_looCV_mod.Rdata\")\n\n\n#--------------------#\n#----Repeated CV-----#\n#--------------------#\n\nset.seed(1234)\ntrain_control &lt;- trainControl(method = \"repeatedcv\", number = 10, repeats = 5)\n\nset.seed(1234)\nknn_model &lt;- train(good ~ ., \n                   data = train, \n                   method = \"knn\", \n                   trControl = train_control)\n\nsave(knn_model, file = \"dataset\\\\model\\\\knn.model_repeatedCV.Rdata\")\n\n\n#--------------------------#\n#----Repeated CV (Mod)-----#\n#--------------------------#\n\nset.seed(1234)\ntrain_control &lt;- trainControl(method = \"repeatedcv\", number = 10, repeats = 5)\n\nkknn.grid &lt;- expand.grid(kmax = c(3, 5, 7 ,9, 11), distance = c(1, 2, 3),\n                         kernel = c(\"rectangular\", \"gaussian\", \"cos\"))\n\nset.seed(1234)\nknn_model &lt;- train(good ~ ., \n                   data = train, \n                   method = \"kknn\",\n                   trControl = train_control, \n                   tuneGrid = kknn.grid,\n                   preProcess = c(\"center\", \"scale\"))\n\nsave(knn_model, file = \"dataset\\\\model\\\\knn.model_repeatedCV_mod.Rdata\")"
  },
  {
    "objectID": "src/knn.html#k-fold-cv",
    "href": "src/knn.html#k-fold-cv",
    "title": "K Nearest Neighbor Classifier",
    "section": "K-fold CV",
    "text": "K-fold CV\n\n\nShow/Hide Code\n# Data Import\nload(\"dataset\\\\train.Rdata\")\nload(\"dataset\\\\test.Rdata\")\n\n# Model Import\nload(\"dataset\\\\model\\\\knn.model_kfoldCV.Rdata\")\n\n# Make predictions on the test data using the trained model and calculate the test error rate\nknn.predictions &lt;- predict(knn_model, newdata = test)\n\nconfusionMatrix(knn.predictions, test$good)\n\n\nConfusion Matrix and Statistics\n\n          Reference\nPrediction   0   1\n         0 903 164\n         1  58  63\n                                          \n               Accuracy : 0.8131          \n                 95% CI : (0.7898, 0.8349)\n    No Information Rate : 0.8089          \n    P-Value [Acc &gt; NIR] : 0.3725          \n                                          \n                  Kappa : 0.2643          \n                                          \n Mcnemar's Test P-Value : 1.826e-12       \n                                          \n            Sensitivity : 0.9396          \n            Specificity : 0.2775          \n         Pos Pred Value : 0.8463          \n         Neg Pred Value : 0.5207          \n             Prevalence : 0.8089          \n         Detection Rate : 0.7601          \n   Detection Prevalence : 0.8981          \n      Balanced Accuracy : 0.6086          \n                                          \n       'Positive' Class : 0               \n                                          \n\n\nShow/Hide Code\n# Convert predictions to a numeric vector\nknn.predictions &lt;- as.numeric(knn.predictions)\n\n# Calculate the AUC using the performance() and auc() functions:\npred_obj &lt;- prediction(knn.predictions, test$good)\nauc_val &lt;- performance(pred_obj, \"auc\")@y.values[[1]]\nauc_val\n\n\n[1] 0.6085896\n\n\nShow/Hide Code\n# Performance plot for TP and FP\nroc_obj &lt;- performance(pred_obj, \"tpr\", \"fpr\")\nplot(roc_obj, colorize = TRUE, lwd = 2,\n     xlab = \"False Positive Rate\", \n     ylab = \"True Positive Rate\",\n     main = \"ROC Curves from 10-fold CV\")\npoints(auc_val, 1 - auc_val, \n       col = \"steelblue\", \n       pch = 21)\nabline(a = 0, b = 1)\n\n\nShow/Hide Code\nknn.kfoldCV.ROC.plot&lt;- recordPlot()\n\n# Accuracy and Kappa value plot\nknn.accu.kappa.plot &lt;- function(knn.model) {\n  p &lt;- ggplot(data=data.frame(k = knn.model$results$k,\n                              Accuracy = knn.model$results$Accuracy,\n                              Kappa = knn.model$results$Kappa)) +\n    geom_point(aes(x = k, y = Accuracy, color = \"Accuracy\")) +\n    geom_point(aes(x = k, y = Kappa, color = \"Kappa\")) +\n    geom_line(aes(x = k, y = Accuracy, linetype = \"Accuracy\", color = \"Accuracy\")) +\n    geom_line(aes(x = k, y = Kappa, linetype = \"Kappa\", color = \"Kappa\")) +\n    scale_color_manual(values = c(\"#98c379\", \"#e06c75\"),\n                       guide = guide_legend(override.aes = list(linetype = c(1, 0)) )) +\n    scale_linetype_manual(values=c(\"solid\", \"dotted\"),\n                          guide = guide_legend(override.aes = list(color = c(\"#98c379\", \"#e06c75\")))) +\n    labs(x = \"K value\", \n         y = \"Accuracy / Kappa\") +\n    ylim(0, 1) +\n    theme_bw() +\n    theme(plot.title = element_text(hjust = 0.5)) +\n    guides(color = guide_legend(title = \"Metric\"),\n           linetype = guide_legend(title = \"Metric\"))\n  return(p)\n}\n\nknn.kfoldCV.plot &lt;- knn.accu.kappa.plot(knn_model) + \n  geom_text(aes(x = k, y = Accuracy, label = round(Accuracy, 3)), vjust = -1) +\n  geom_text(aes(x = k, y = Kappa, label = round(Kappa, 3)), vjust = -1) +\n  ggtitle(\"KNN Model Performance (10-Fold CV)\")\n\n\n\nTuned\n\n\nShow/Hide Code\nload(\"dataset\\\\model\\\\knn.model_kfoldCV_mod.Rdata\")\n\nknn.predictions &lt;- predict(knn_model, newdata = test)\n\nconfusionMatrix(knn.predictions, test$good)\n\n\nConfusion Matrix and Statistics\n\n          Reference\nPrediction   0   1\n         0 935 208\n         1  26  19\n                                          \n               Accuracy : 0.803           \n                 95% CI : (0.7793, 0.8253)\n    No Information Rate : 0.8089          \n    P-Value [Acc &gt; NIR] : 0.7118          \n                                          \n                  Kappa : 0.0816          \n                                          \n Mcnemar's Test P-Value : &lt;2e-16          \n                                          \n            Sensitivity : 0.9729          \n            Specificity : 0.0837          \n         Pos Pred Value : 0.8180          \n         Neg Pred Value : 0.4222          \n             Prevalence : 0.8089          \n         Detection Rate : 0.7870          \n   Detection Prevalence : 0.9621          \n      Balanced Accuracy : 0.5283          \n                                          \n       'Positive' Class : 0               \n                                          \n\n\nShow/Hide Code\nknn.predictions &lt;- as.numeric(knn.predictions)\npred_obj &lt;- prediction(knn.predictions, test$good)\nauc_val &lt;- performance(pred_obj, \"auc\")@y.values[[1]]\nauc_val\n\n\n[1] 0.5283226\n\n\nShow/Hide Code\nroc_obj &lt;- performance(pred_obj, \"tpr\", \"fpr\")\ninvisible(plot(roc_obj, colorize = TRUE, lwd = 2,\n               xlab = \"False Positive Rate\", \n               ylab = \"True Positive Rate\",\n               main = \"ROC Curves from Tuned 10-fold CV\"))\npoints(auc_val, 1 - auc_val, \n       col = \"steelblue\", \n       pch = 21)\nabline(a = 0, b = 1)\n\n\nShow/Hide Code\nknn.kfoldCV_mod.ROC.plot &lt;- recordPlot()\n\nknn.kfoldCV_mod.plot &lt;- knn.accu.kappa.plot(knn_model) +\n  geom_text(aes(x = k, y = Accuracy, label = round(Accuracy, 3)), vjust = -1) +\n  geom_text(aes(x = k, y = Kappa, label = round(Kappa, 3)), vjust = -1) +\n  ggtitle(\"KNN Model Performance (Tuned 10-Fold CV)\")"
  },
  {
    "objectID": "src/knn.html#hold-out-cv-validation-set-approach",
    "href": "src/knn.html#hold-out-cv-validation-set-approach",
    "title": "K Nearest Neighbor Classifier",
    "section": "Hold-out CV (Validation Set Approach)",
    "text": "Hold-out CV (Validation Set Approach)\n\n\nShow/Hide Code\nload(\"dataset\\\\model\\\\knn.model_holdoutCV.Rdata\")\n\nknn.predictions &lt;- predict(knn_model, newdata = test)\n\nconfusionMatrix(knn.predictions, test$good)\n\n\nConfusion Matrix and Statistics\n\n          Reference\nPrediction   0   1\n         0 911 174\n         1  50  53\n                                         \n               Accuracy : 0.8114         \n                 95% CI : (0.788, 0.8333)\n    No Information Rate : 0.8089         \n    P-Value [Acc &gt; NIR] : 0.4297         \n                                         \n                  Kappa : 0.2293         \n                                         \n Mcnemar's Test P-Value : &lt;2e-16         \n                                         \n            Sensitivity : 0.9480         \n            Specificity : 0.2335         \n         Pos Pred Value : 0.8396         \n         Neg Pred Value : 0.5146         \n             Prevalence : 0.8089         \n         Detection Rate : 0.7668         \n   Detection Prevalence : 0.9133         \n      Balanced Accuracy : 0.5907         \n                                         \n       'Positive' Class : 0              \n                                         \n\n\nShow/Hide Code\nknn.predictions &lt;- as.numeric(knn.predictions)\npred_obj &lt;- prediction(knn.predictions, test$good)\nauc_val &lt;- performance(pred_obj, \"auc\")@y.values[[1]]\nauc_val\n\n\n[1] 0.5907255\n\n\nShow/Hide Code\nroc_obj &lt;- performance(pred_obj, \"tpr\", \"fpr\")\nplot(roc_obj, colorize = TRUE, lwd = 2,\n     xlab = \"False Positive Rate\", \n     ylab = \"True Positive Rate\",\n     main = \"ROC Curves from Hold-out CV\")\npoints(auc_val, 1 - auc_val, \n       col = \"steelblue\", \n       pch = 21)\nabline(a = 0, b = 1)\n\n\nShow/Hide Code\nknn.holdoutCV.ROC.plot &lt;- recordPlot()\n\nknn.holdoutCV.plot &lt;- knn.accu.kappa.plot(knn_model) +\n  geom_text(aes(x = k, y = Accuracy, label = round(Accuracy, 3)), vjust = -1) +\n  geom_text(aes(x = k, y = Kappa, label = round(Kappa, 3)), vjust = -1) +\n  ggtitle(\"KNN Model Performance (Hold-out CV)\")\n\n\n\nTuned\n\n\nShow/Hide Code\nload(\"dataset\\\\model\\\\knn.model_holdoutCV_mod.Rdata\")\n\nknn.predictions &lt;- predict(knn_model, newdata = test)\n\nconfusionMatrix(knn.predictions, test$good)\n\n\nConfusion Matrix and Statistics\n\n          Reference\nPrediction   0   1\n         0 949 218\n         1  12   9\n                                          \n               Accuracy : 0.8064          \n                 95% CI : (0.7828, 0.8285)\n    No Information Rate : 0.8089          \n    P-Value [Acc &gt; NIR] : 0.6046          \n                                          \n                  Kappa : 0.0416          \n                                          \n Mcnemar's Test P-Value : &lt;2e-16          \n                                          \n            Sensitivity : 0.98751         \n            Specificity : 0.03965         \n         Pos Pred Value : 0.81320         \n         Neg Pred Value : 0.42857         \n             Prevalence : 0.80892         \n         Detection Rate : 0.79882         \n   Detection Prevalence : 0.98232         \n      Balanced Accuracy : 0.51358         \n                                          \n       'Positive' Class : 0               \n                                          \n\n\nShow/Hide Code\nknn.predictions &lt;- as.numeric(knn.predictions)\npred_obj &lt;- prediction(knn.predictions, test$good)\nauc_val &lt;- performance(pred_obj, \"auc\")@y.values[[1]]\nauc_val\n\n\n[1] 0.5135803\n\n\nShow/Hide Code\nroc_obj &lt;- performance(pred_obj, \"tpr\", \"fpr\")\nplot(roc_obj, colorize = TRUE, lwd = 2,\n     xlab = \"False Positive Rate\", \n     ylab = \"True Positive Rate\",\n     main = \"ROC Curves from Tuned Hold-out CV\")\npoints(auc_val, 1 - auc_val, \n       col = \"steelblue\", \n       pch = 21)\nabline(a = 0, b = 1)\n\n\nShow/Hide Code\nknn.holdoutCV_mod.ROC.plot &lt;- recordPlot()\n\nknn.holdoutCV_mod.plot &lt;- knn.accu.kappa.plot(knn_model) + \n  geom_text(aes(x = k, y = Accuracy, label = round(Accuracy, 3)), hjust = -0.3, angle=90) +\n  geom_text(aes(x = k, y = Kappa, label = round(Kappa, 3)), hjust=-0.3, angle=90) +\n  ggtitle(\"KNN Model Performance (Tuned Hold-out CV)\")"
  },
  {
    "objectID": "src/knn.html#loocv",
    "href": "src/knn.html#loocv",
    "title": "K Nearest Neighbor Classifier",
    "section": "LOOCV",
    "text": "LOOCV\n\n\nShow/Hide Code\nload(\"dataset\\\\model\\\\knn.model_looCV.Rdata\")\n\nknn.predictions &lt;- predict(knn_model, newdata = test)\nconfusionMatrix(knn.predictions, test$good)\n\n\nConfusion Matrix and Statistics\n\n          Reference\nPrediction   0   1\n         0 921 171\n         1  40  56\n                                          \n               Accuracy : 0.8224          \n                 95% CI : (0.7994, 0.8437)\n    No Information Rate : 0.8089          \n    P-Value [Acc &gt; NIR] : 0.1258          \n                                          \n                  Kappa : 0.263           \n                                          \n Mcnemar's Test P-Value : &lt;2e-16          \n                                          \n            Sensitivity : 0.9584          \n            Specificity : 0.2467          \n         Pos Pred Value : 0.8434          \n         Neg Pred Value : 0.5833          \n             Prevalence : 0.8089          \n         Detection Rate : 0.7753          \n   Detection Prevalence : 0.9192          \n      Balanced Accuracy : 0.6025          \n                                          \n       'Positive' Class : 0               \n                                          \n\n\nShow/Hide Code\nknn.predictions &lt;- as.numeric(knn.predictions)\npred_obj &lt;- prediction(knn.predictions, test$good)\nauc_val &lt;- performance(pred_obj, \"auc\")@y.values[[1]]\nauc_val\n\n\n[1] 0.6025364\n\n\nShow/Hide Code\nroc_obj &lt;- performance(pred_obj, \"tpr\", \"fpr\")\nplot(roc_obj, colorize = TRUE, lwd = 2,\n     xlab = \"False Positive Rate\", \n     ylab = \"True Positive Rate\",\n     main = 'ROC Curves from LOOCV')\npoints(auc_val, 1 - auc_val, \n       col = \"steelblue\", \n       pch = 21)\nabline(a = 0, b = 1)\n\n\nShow/Hide Code\nknn.looCV.ROC.plot &lt;- recordPlot()\n\nknn.looCV.plot &lt;- knn.accu.kappa.plot(knn_model) + \n  geom_text(aes(x = k, y = Accuracy, label = round(Accuracy, 3)), vjust = -1) +\n  geom_text(aes(x = k, y = Kappa, label = round(Kappa, 3)), vjust = -1) +\n  ggtitle(\"KNN Model Performance (LOOCV)\")\n\n\n\nTuned\n\n\nShow/Hide Code\nload(\"dataset\\\\model\\\\knn.model_looCV_mod.Rdata\")\n\nknn.predictions &lt;- predict(knn_model, newdata = test)\nconfusionMatrix(knn.predictions, test$good)\n\n\nConfusion Matrix and Statistics\n\n          Reference\nPrediction   0   1\n         0 936 208\n         1  25  19\n                                          \n               Accuracy : 0.8039          \n                 95% CI : (0.7801, 0.8261)\n    No Information Rate : 0.8089          \n    P-Value [Acc &gt; NIR] : 0.6863          \n                                          \n                  Kappa : 0.0833          \n                                          \n Mcnemar's Test P-Value : &lt;2e-16          \n                                          \n            Sensitivity : 0.9740          \n            Specificity : 0.0837          \n         Pos Pred Value : 0.8182          \n         Neg Pred Value : 0.4318          \n             Prevalence : 0.8089          \n         Detection Rate : 0.7879          \n   Detection Prevalence : 0.9630          \n      Balanced Accuracy : 0.5288          \n                                          \n       'Positive' Class : 0               \n                                          \n\n\nShow/Hide Code\nknn.predictions &lt;- as.numeric(knn.predictions)\npred_obj &lt;- prediction(knn.predictions, test$good)\nauc_val &lt;- performance(pred_obj, \"auc\")@y.values[[1]]\nauc_val\n\n\n[1] 0.5288429\n\n\nShow/Hide Code\nroc_obj &lt;- performance(pred_obj, \"tpr\", \"fpr\")\nplot(roc_obj, colorize = TRUE, lwd = 2,\n     xlab = \"False Positive Rate\", \n     ylab = \"True Positive Rate\",\n     main = \"ROC Curves from Tuned LOOCV\")\npoints(auc_val, 1 - auc_val, \n       col = \"steelblue\", \n       pch = 21)\nabline(a = 0, b = 1)\n\n\nShow/Hide Code\nknn.looCV_mod.ROC.plot &lt;- recordPlot()\n\nknn.looCV_mod.plot &lt;- knn.accu.kappa.plot(knn_model) + \n  geom_text(aes(x = k, y = Accuracy, label = round(Accuracy, 3)), hjust = -0.3, angle=90) +\n  geom_text(aes(x = k, y = Kappa, label = round(Kappa, 3)), hjust = -0.3, angle=90) +\n  ggtitle(\"KNN Model Performance (Tuned LOOCV)\")"
  },
  {
    "objectID": "src/knn.html#repeated-cv",
    "href": "src/knn.html#repeated-cv",
    "title": "K Nearest Neighbor Classifier",
    "section": "Repeated CV",
    "text": "Repeated CV\n\n\nShow/Hide Code\nload(\"dataset\\\\model\\\\knn.model_repeatedCV.Rdata\")\n\nknn.predictions &lt;- predict(knn_model, newdata = test)\n\nconfusionMatrix(knn.predictions, test$good)\n\n\nConfusion Matrix and Statistics\n\n          Reference\nPrediction   0   1\n         0 917  83\n         1  44 144\n                                          \n               Accuracy : 0.8931          \n                 95% CI : (0.8741, 0.9101)\n    No Information Rate : 0.8089          \n    P-Value [Acc &gt; NIR] : 1.966e-15       \n                                          \n                  Kappa : 0.6299          \n                                          \n Mcnemar's Test P-Value : 0.0007464       \n                                          \n            Sensitivity : 0.9542          \n            Specificity : 0.6344          \n         Pos Pred Value : 0.9170          \n         Neg Pred Value : 0.7660          \n             Prevalence : 0.8089          \n         Detection Rate : 0.7719          \n   Detection Prevalence : 0.8418          \n      Balanced Accuracy : 0.7943          \n                                          \n       'Positive' Class : 0               \n                                          \n\n\nShow/Hide Code\nknn.predictions &lt;- as.numeric(knn.predictions)\npred_obj &lt;- prediction(knn.predictions, test$good)\nauc_val &lt;- performance(pred_obj, \"auc\")@y.values[[1]]\nauc_val\n\n\n[1] 0.7942878\n\n\nShow/Hide Code\nroc_obj &lt;- performance(pred_obj, \"tpr\", \"fpr\")\nplot(roc_obj, colorize = TRUE, lwd = 2,\n     xlab = \"False Positive Rate\", \n     ylab = \"True Positive Rate\",\n     main = \"ROC Curves from Repeated CV\")\npoints(auc_val, 1 - auc_val, \n       col = \"steelblue\", \n       pch = 21)\nabline(a = 0, b = 1)\n\n\nShow/Hide Code\nknn.repeatedCV.ROC.plot &lt;- recordPlot()\n\ndf &lt;- knn_model$results\nknn.repeatedCV.plot &lt;- ggplot(data=df, aes(x = kmax, y = Accuracy)) +\n  geom_point(aes(color = \"Accuracy\")) +\n  geom_point(aes(color = \"Kappa\")) +\n  geom_line(aes(linetype = \"Accuracy\", color = \"Accuracy\")) +\n  geom_line(aes(y = Kappa, linetype = \"Kappa\", color = \"Kappa\")) +\n  geom_text(aes(label = round(Accuracy, 3)), vjust = -1) +\n  geom_text(aes(y = Kappa, label = round(Kappa, 3)), vjust = -1) +\n  scale_color_manual(values = c(\"#98c379\", \"#e06c75\"),\n                     guide = guide_legend(override.aes = list(linetype = c(1, 0)) )) +\n  scale_linetype_manual(values=c(\"solid\", \"dotted\"),\n                        guide = guide_legend(override.aes = list(color = c(\"#98c379\", \"#e06c75\")))) +\n  labs(x = \"K value\", \n       y = \"Accuracy / Kappa\",\n       title = \"KNN Model Performance (Repeated CV)\") +\n  ylim(0, 1) +\n  theme_bw() +\n  theme(plot.title = element_text(hjust = 0.5)) +\n  guides(color = guide_legend(title = \"Metric\"),\n         linetype = guide_legend(title = \"Metric\"))\n\n\n\nTuned\n\n\nShow/Hide Code\nload(\"dataset\\\\model\\\\knn.model_repeatedCV_mod.Rdata\")\n\nknn.predictions &lt;- predict(knn_model, newdata = test)\n\nconfusionMatrix(knn.predictions, test$good)\n\n\nConfusion Matrix and Statistics\n\n          Reference\nPrediction   0   1\n         0 921 103\n         1  40 124\n                                          \n               Accuracy : 0.8796          \n                 95% CI : (0.8598, 0.8976)\n    No Information Rate : 0.8089          \n    P-Value [Acc &gt; NIR] : 3.875e-11       \n                                          \n                  Kappa : 0.5645          \n                                          \n Mcnemar's Test P-Value : 2.164e-07       \n                                          \n            Sensitivity : 0.9584          \n            Specificity : 0.5463          \n         Pos Pred Value : 0.8994          \n         Neg Pred Value : 0.7561          \n             Prevalence : 0.8089          \n         Detection Rate : 0.7753          \n   Detection Prevalence : 0.8620          \n      Balanced Accuracy : 0.7523          \n                                          \n       'Positive' Class : 0               \n                                          \n\n\nShow/Hide Code\nknn.predictions &lt;- as.numeric(knn.predictions)\npred_obj &lt;- prediction(knn.predictions, test$good)\nauc_val &lt;- performance(pred_obj, \"auc\")@y.values[[1]]\nauc_val\n\n\n[1] 0.7523161\n\n\nShow/Hide Code\nroc_obj &lt;- performance(pred_obj, \"tpr\", \"fpr\")\n\nplot(roc_obj, colorize = TRUE, lwd = 2,\n     xlab = \"False Positive Rate\", \n     ylab = \"True Positive Rate\",\n     main = \"ROC Curves from Tuned Repeated CV\")\npoints(auc_val, 1 - auc_val, \n       col = \"steelblue\", \n       pch = 21)\nabline(a = 0, b = 1)\n\n\nShow/Hide Code\nknn.repeatedCV_mod.ROC.plot &lt;- recordPlot()\n\nknn.repeatedCV_mod.plot &lt;- ggplot(knn_model) +\n  labs(x = \"K value\", \n       y = \"Accuracy\", \n       title = \"KNN Model Performance (Tuned Repeated CV)\") +\n  theme_bw() +\n  theme(plot.title = element_text(hjust = 0.5))"
  },
  {
    "objectID": "src/knn.html#summary",
    "href": "src/knn.html#summary",
    "title": "K Nearest Neighbor Classifier",
    "section": "Summary",
    "text": "Summary\n\n\nShow/Hide Code\nggarrange(knn.kfoldCV.plot,\n          knn.kfoldCV_mod.plot,\n          knn.holdoutCV.plot,\n          knn.holdoutCV_mod.plot,\n          knn.looCV.plot,\n          knn.looCV_mod.plot,\n          knn.repeatedCV.plot,\n          knn.repeatedCV_mod.plot,\n          ncol = 2, nrow = 4)\n\n\n\n\n\n\n\n\n\n\n\nShow/Hide Code\ncowplot::plot_grid(knn.kfoldCV.ROC.plot, knn.kfoldCV_mod.ROC.plot,\n                   ncol = 2, align = \"hv\", scale = 0.8)\n\n\n\n\n\n\n\n\n\nShow/Hide Code\ncowplot::plot_grid(knn.holdoutCV.ROC.plot, knn.holdoutCV_mod.ROC.plot,\n                   ncol = 2, align = \"hv\", scale = 0.8)\n\n\n\n\n\n\n\n\n\nShow/Hide Code\ncowplot::plot_grid(knn.looCV.ROC.plot, knn.looCV_mod.ROC.plot,\n                   ncol = 2, align = \"hv\", scale = 0.8)\n\n\n\n\n\n\n\n\n\nShow/Hide Code\ncowplot::plot_grid(knn.repeatedCV.ROC.plot, knn.repeatedCV_mod.ROC.plot,\n                   ncol = 2, align = \"hv\", scale = 0.8)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nResampling Method\nError Rate\nSensitivity\nSpecificity\nAUC\n\n\n\n\nK-Fold CV\n0.2273\n0.9199\n0.1883\n0.5541001\n\n\nK-Fold CV (Tuned)\n0.1995\n0.9768\n0.1004\n0.5386181\n\n\nHold-out CV\n0.2222\n0.9336\n0.1590\n0.5463051\n\n\nHold-out CV (Tuned)\n0.2022\n0.9926\n0.0251\n0.5088642\n\n\nLOOCV\n0.1717\n0.9642\n0.2887\n0.6264379\n\n\nLOOCV (Tuned)\n0.1995\n0.9768\n0.1004\n0.5386181\n\n\nRepeated CV\n0.1776\n0.9104\n0.4728\n0.6916177\n\n\nRepeated CV (Tuned)\n0.1120\n0.9547\n0.6234\n0.7890601"
  },
  {
    "objectID": "src/lda.html",
    "href": "src/lda.html",
    "title": "Quadratic Discriminant Analysis",
    "section": "",
    "text": "Show/Hide Code\n#------------#\n#----LDA-----#\n#------------#\nset.seed(1234)\ntrain_control &lt;- trainControl(method = \"cv\", number = 10)\n\nset.seed(1234)\nlda_model &lt;- train(good ~ ., \n                   data = train, \n                   method = \"lda\", \n                   trControl = train_control)\n\nsave(lda_model, file = \"dataset\\\\model\\\\lda.model_kfoldCV.Rdata\")\n\n\n#------------------#\n#----LDA (Mod)-----#\n#------------------#"
  },
  {
    "objectID": "src/lda.html#model-construction",
    "href": "src/lda.html#model-construction",
    "title": "Quadratic Discriminant Analysis",
    "section": "",
    "text": "Show/Hide Code\n#------------#\n#----LDA-----#\n#------------#\nset.seed(1234)\ntrain_control &lt;- trainControl(method = \"cv\", number = 10)\n\nset.seed(1234)\nlda_model &lt;- train(good ~ ., \n                   data = train, \n                   method = \"lda\", \n                   trControl = train_control)\n\nsave(lda_model, file = \"dataset\\\\model\\\\lda.model_kfoldCV.Rdata\")\n\n\n#------------------#\n#----LDA (Mod)-----#\n#------------------#"
  },
  {
    "objectID": "src/lda.html#k-fold-cv",
    "href": "src/lda.html#k-fold-cv",
    "title": "Quadratic Discriminant Analysis",
    "section": "K-fold CV",
    "text": "K-fold CV\n\n\nShow/Hide Code\n# Data Import\nload(\"dataset\\\\train.Rdata\")\nload(\"dataset\\\\test.Rdata\")\n\n# Model Import\nload(\"dataset\\\\model\\\\lda.model_kfoldCV.Rdata\")\n\nlda.predictions &lt;- predict(lda_model, newdata = test)\n\nconfusionMatrix(lda.predictions, test$good)\n\n\nConfusion Matrix and Statistics\n\n          Reference\nPrediction   0   1\n         0 889 144\n         1  72  83\n                                          \n               Accuracy : 0.8182          \n                 95% CI : (0.7951, 0.8397)\n    No Information Rate : 0.8089          \n    P-Value [Acc &gt; NIR] : 0.2201          \n                                          \n                  Kappa : 0.3308          \n                                          \n Mcnemar's Test P-Value : 1.359e-06       \n                                          \n            Sensitivity : 0.9251          \n            Specificity : 0.3656          \n         Pos Pred Value : 0.8606          \n         Neg Pred Value : 0.5355          \n             Prevalence : 0.8089          \n         Detection Rate : 0.7483          \n   Detection Prevalence : 0.8695          \n      Balanced Accuracy : 0.6454          \n                                          \n       'Positive' Class : 0               \n                                          \n\n\nShow/Hide Code\nlda.predictions &lt;- as.numeric(lda.predictions)\npred_obj &lt;- prediction(lda.predictions, test$good)\nauc_val &lt;- performance(pred_obj, \"auc\")@y.values[[1]]\nauc_val\n\n\n[1] 0.6453584\n\n\nShow/Hide Code\nroc_obj &lt;- performance(pred_obj, \"tpr\", \"fpr\")\nplot(roc_obj, colorize = TRUE, lwd = 2,\n     xlab = \"False Positive Rate\", \n     ylab = \"True Positive Rate\",\n     main = \"ROC Curves from Repeated CV\")\npoints(auc_val, 1 - auc_val, \n       col = \"steelblue\", \n       pch = 21)\nabline(a = 0, b = 1)\n\n\nShow/Hide Code\nlda.kfoldCV.ROC.plot &lt;- recordPlot()\n\n\n\nTuned\n\n\n\n\n\n\n\n\n\n\nResampling Method\nError Rate\nSensitivity\nSpecificity\nAUC\n\n\n\n\nLDA\n0.1919\n0.9283\n0.3305\n0.6294448\n\n\nLDA (Tuned)\n0.xxxx\n0.xxxx\n0.xxxx\n0.xxxxxxx"
  },
  {
    "objectID": "src/logit.html",
    "href": "src/logit.html",
    "title": "Logistic Regression",
    "section": "",
    "text": "Show/Hide Code\n#----------------------------#\n#----Logistic Regression-----#\n#----------------------------#\n\nset.seed(1234)\n# Define the training control object for 10-fold cross-validation\ntrain_control &lt;- trainControl(method = \"cv\", number = 10)\n\n# Train the logistic regression model using 10-fold cross-validation\nset.seed(1234)\nlogit_model &lt;- train(good ~ ., \n                     data = train, \n                     method = \"glm\", \n                     family = \"binomial\",\n                     trControl = train_control)\n\nsave(logit_model, file = \"dataset\\\\model\\\\logit.model_kfoldCV.Rdata\")\n\n\n#----------------------------------#\n#----Logistic Regression (Mod)-----#\n#----------------------------------#"
  },
  {
    "objectID": "src/logit.html#model-construction",
    "href": "src/logit.html#model-construction",
    "title": "Logistic Regression",
    "section": "",
    "text": "Show/Hide Code\n#----------------------------#\n#----Logistic Regression-----#\n#----------------------------#\n\nset.seed(1234)\n# Define the training control object for 10-fold cross-validation\ntrain_control &lt;- trainControl(method = \"cv\", number = 10)\n\n# Train the logistic regression model using 10-fold cross-validation\nset.seed(1234)\nlogit_model &lt;- train(good ~ ., \n                     data = train, \n                     method = \"glm\", \n                     family = \"binomial\",\n                     trControl = train_control)\n\nsave(logit_model, file = \"dataset\\\\model\\\\logit.model_kfoldCV.Rdata\")\n\n\n#----------------------------------#\n#----Logistic Regression (Mod)-----#\n#----------------------------------#"
  },
  {
    "objectID": "src/logit.html#k-fold-cv",
    "href": "src/logit.html#k-fold-cv",
    "title": "Logistic Regression",
    "section": "K-fold CV",
    "text": "K-fold CV\n\n\nShow/Hide Code\n# Data Import\nload(\"dataset\\\\train.Rdata\")\nload(\"dataset\\\\test.Rdata\")\n\n# Model Import\nload(\"dataset\\\\model\\\\logit.model_kfoldCV.Rdata\")\n\nlogit.predictions &lt;- predict(logit_model, newdata = test)\n\nconfusionMatrix(logit.predictions, test$good)\n\n\nConfusion Matrix and Statistics\n\n          Reference\nPrediction   0   1\n         0 899 154\n         1  62  73\n                                          \n               Accuracy : 0.8182          \n                 95% CI : (0.7951, 0.8397)\n    No Information Rate : 0.8089          \n    P-Value [Acc &gt; NIR] : 0.2201          \n                                          \n                  Kappa : 0.3041          \n                                          \n Mcnemar's Test P-Value : 5.949e-10       \n                                          \n            Sensitivity : 0.9355          \n            Specificity : 0.3216          \n         Pos Pred Value : 0.8538          \n         Neg Pred Value : 0.5407          \n             Prevalence : 0.8089          \n         Detection Rate : 0.7567          \n   Detection Prevalence : 0.8864          \n      Balanced Accuracy : 0.6285          \n                                          \n       'Positive' Class : 0               \n                                          \n\n\nShow/Hide Code\nlogit.predictions &lt;- as.numeric(logit.predictions)\npred_obj &lt;- prediction(logit.predictions, test$good)\nauc_val  &lt;- performance(pred_obj, \"auc\")@y.values[[1]]\nauc_val\n\n\n[1] 0.6285349\n\n\nShow/Hide Code\nroc_obj &lt;- performance(pred_obj, \"tpr\", \"fpr\")\nplot(roc_obj, colorize = TRUE, lwd = 2,\n     xlab = \"False Positive Rate\", \n     ylab = \"True Positive Rate\",\n     main = \"ROC Curves from Repeated CV\")\npoints(auc_val, 1 - auc_val, \n       col = \"steelblue\", \n       pch = 21)\nabline(a = 0, b = 1)\n\n\nShow/Hide Code\nlogit.kfoldCV.ROC.plot &lt;- recordPlot()\n\n\n\nTuned\n\n\nShow/Hide Code\nglm.model &lt;- glm(good ~ ., data= train,family=\"binomial\")\nglm.fit= stepAIC(glm.model, direction = 'backward')\n\n\nStart:  AIC=2263\ngood ~ fixed.acidity + volatile.acidity + citric.acid + residual.sugar + \n    chlorides + free.sulfur.dioxide + total.sulfur.dioxide + \n    density + pH + sulphates + alcohol\n\n                       Df Deviance    AIC\n- citric.acid           1   2240.1 2262.1\n&lt;none&gt;                      2239.0 2263.0\n- alcohol               1   2241.3 2263.3\n- total.sulfur.dioxide  1   2243.0 2265.0\n- chlorides             1   2252.2 2274.2\n- volatile.acidity      1   2254.3 2276.3\n- sulphates             1   2256.6 2278.6\n- free.sulfur.dioxide   1   2258.1 2280.1\n- fixed.acidity         1   2258.6 2280.6\n- density               1   2263.2 2285.2\n- residual.sugar        1   2266.6 2288.6\n- pH                    1   2295.8 2317.8\n\nStep:  AIC=2262.05\ngood ~ fixed.acidity + volatile.acidity + residual.sugar + chlorides + \n    free.sulfur.dioxide + total.sulfur.dioxide + density + pH + \n    sulphates + alcohol\n\n                       Df Deviance    AIC\n&lt;none&gt;                      2240.1 2262.1\n- alcohol               1   2242.7 2262.7\n- total.sulfur.dioxide  1   2243.7 2263.7\n- chlorides             1   2252.8 2272.8\n- sulphates             1   2257.9 2277.9\n- volatile.acidity      1   2258.4 2278.4\n- free.sulfur.dioxide   1   2258.8 2278.8\n- fixed.acidity         1   2261.2 2281.2\n- density               1   2263.7 2283.7\n- residual.sugar        1   2267.2 2287.2\n- pH                    1   2296.2 2316.2\n\n\nShow/Hide Code\n# Make predictions on test data and construct a confusion matrix\nlogit.predictions &lt;- predict(glm.fit, newdata = test,type = \"response\")\nlogit.predictions &lt;- factor(ifelse(logit.predictions &gt; 0.7, 1, 0),\n                            levels = c(0, 1))\nconfusionMatrix(logit.predictions, test$good)\n\n\nConfusion Matrix and Statistics\n\n          Reference\nPrediction   0   1\n         0 954 207\n         1   7  20\n                                          \n               Accuracy : 0.8199          \n                 95% CI : (0.7968, 0.8413)\n    No Information Rate : 0.8089          \n    P-Value [Acc &gt; NIR] : 0.1784          \n                                          \n                  Kappa : 0.1218          \n                                          \n Mcnemar's Test P-Value : &lt;2e-16          \n                                          \n            Sensitivity : 0.99272         \n            Specificity : 0.08811         \n         Pos Pred Value : 0.82171         \n         Neg Pred Value : 0.74074         \n             Prevalence : 0.80892         \n         Detection Rate : 0.80303         \n   Detection Prevalence : 0.97727         \n      Balanced Accuracy : 0.54041         \n                                          \n       'Positive' Class : 0               \n                                          \n\n\nShow/Hide Code\nlogit.predictions &lt;- as.numeric(logit.predictions)\npred_obj &lt;- prediction(logit.predictions, test$good)\nauc_val &lt;- performance(pred_obj, \"auc\")@y.values[[1]]\nauc_val\n\n\n[1] 0.5404108\n\n\nShow/Hide Code\nroc_obj &lt;- performance(pred_obj, \"tpr\", \"fpr\")\nplot(roc_obj, colorize = TRUE, lwd = 2,\n     xlab = \"False Positive Rate\", \n     ylab = \"True Positive Rate\",\n     main = \"ROC Curves from Repeated CV\")\npoints(auc_val, 1 - auc_val, \n       col = \"steelblue\", \n       pch = 21)\nabline(a = 0, b = 1)\n\n\nShow/Hide Code\nlogit.kfoldCV.ROC.plot &lt;- recordPlot()\n\n\n\n\n\n\n\n\n\n\n\n\nResampling Method\nError Rate\nSensitivity\nSpecificity\nAUC\n\n\n\n\nLogistic Regression\n0.1944\n0.9347\n0.2929\n0.6137776\n\n\nLogistic Regression (Tuned)\n0.1919\n0.98946\n0.08787\n0.5386644"
  },
  {
    "objectID": "src/naiveBayes.html",
    "href": "src/naiveBayes.html",
    "title": "Naive Bayes",
    "section": "",
    "text": "Show/Hide Code\n#--------------------#\n#----Naive Bayes-----#\n#--------------------#\nset.seed(1234)\ntrain_control &lt;- trainControl(method = \"cv\", number = 10)\n\nset.seed(1234)\nnb_model &lt;- train(good ~ ., \n               data = train, \n               method = \"naive_bayes\", \n               trControl = train_control)\n\nsave(nb_model, file = \"dataset\\\\model\\\\nb.model_kfoldCV.Rdata\")"
  },
  {
    "objectID": "src/naiveBayes.html#model-construction",
    "href": "src/naiveBayes.html#model-construction",
    "title": "Naive Bayes",
    "section": "",
    "text": "Show/Hide Code\n#--------------------#\n#----Naive Bayes-----#\n#--------------------#\nset.seed(1234)\ntrain_control &lt;- trainControl(method = \"cv\", number = 10)\n\nset.seed(1234)\nnb_model &lt;- train(good ~ ., \n               data = train, \n               method = \"naive_bayes\", \n               trControl = train_control)\n\nsave(nb_model, file = \"dataset\\\\model\\\\nb.model_kfoldCV.Rdata\")"
  },
  {
    "objectID": "src/naiveBayes.html#k-fold-cv",
    "href": "src/naiveBayes.html#k-fold-cv",
    "title": "Naive Bayes",
    "section": "K-fold CV",
    "text": "K-fold CV\n\n\nShow/Hide Code\n# Data Import\nload(\"dataset\\\\train.Rdata\")\nload(\"dataset\\\\test.Rdata\")\n\n# Model Import\nload(\"dataset\\\\model\\\\nb.model_kfoldCV.Rdata\")\n\nnb.predictions &lt;- predict(nb_model, newdata = test)\n\nconfusionMatrix(nb.predictions, test$good)\n\n\nConfusion Matrix and Statistics\n\n          Reference\nPrediction   0   1\n         0 750  73\n         1 211 154\n                                          \n               Accuracy : 0.7609          \n                 95% CI : (0.7356, 0.7849)\n    No Information Rate : 0.8089          \n    P-Value [Acc &gt; NIR] : 1               \n                                          \n                  Kappa : 0.3724          \n                                          \n Mcnemar's Test P-Value : 4.312e-16       \n                                          \n            Sensitivity : 0.7804          \n            Specificity : 0.6784          \n         Pos Pred Value : 0.9113          \n         Neg Pred Value : 0.4219          \n             Prevalence : 0.8089          \n         Detection Rate : 0.6313          \n   Detection Prevalence : 0.6928          \n      Balanced Accuracy : 0.7294          \n                                          \n       'Positive' Class : 0               \n                                          \n\n\nShow/Hide Code\nnb.predictions &lt;- as.numeric(nb.predictions)\npred_obj &lt;- prediction(nb.predictions, test$good)\nauc_val &lt;- performance(pred_obj, \"auc\")@y.values[[1]]\nauc_val\n\n\n[1] 0.7294256\n\n\nShow/Hide Code\nroc_obj &lt;- performance(pred_obj, \"tpr\", \"fpr\")\nplot(roc_obj, colorize = TRUE, lwd = 2,\n     xlab = \"False Positive Rate\", \n     ylab = \"True Positive Rate\",\n     main = \"ROC Curves from Repeated CV\")\npoints(auc_val, 1 - auc_val, \n       col = \"steelblue\", \n       pch = 21)\nabline(a = 0, b = 1)\n\n\nShow/Hide Code\nnb.kfoldCV.ROC.plot &lt;- recordPlot()\n\n\n\nTuned\n\n\n\n\n\n\n\n\n\n\nResampling Method\nError Rate\nSensitivity\nSpecificity\nAUC\n\n\n\n\nNaive Bayes\n0.2391\n0.7804\n0.6784\n0.7294256\n\n\nNaive Bayes (Tuned)\n0.xxxx\n0.xxxx\n0.xxxx\n0.xxxxxxx"
  },
  {
    "objectID": "src/nnet.html",
    "href": "src/nnet.html",
    "title": "Neural Network",
    "section": "",
    "text": "Show/Hide Code\n#-------------#\n#----NNet-----#\n#-------------#\nset.seed(1234)\ntrain_control &lt;- trainControl(method = \"cv\", number = 10)\n\nset.seed(1234)\nnnet_model &lt;- train(good ~ ., \n                    data = train, \n                    method = \"nnet\", \n                    trControl = train_control)\n\nsave(nnet_model, file = \"dataset\\\\model\\\\nnet.model_kfoldCV.Rdata\")"
  },
  {
    "objectID": "src/nnet.html#model-construction",
    "href": "src/nnet.html#model-construction",
    "title": "Neural Network",
    "section": "",
    "text": "Show/Hide Code\n#-------------#\n#----NNet-----#\n#-------------#\nset.seed(1234)\ntrain_control &lt;- trainControl(method = \"cv\", number = 10)\n\nset.seed(1234)\nnnet_model &lt;- train(good ~ ., \n                    data = train, \n                    method = \"nnet\", \n                    trControl = train_control)\n\nsave(nnet_model, file = \"dataset\\\\model\\\\nnet.model_kfoldCV.Rdata\")"
  },
  {
    "objectID": "src/nnet.html#k-fold-cv",
    "href": "src/nnet.html#k-fold-cv",
    "title": "Neural Network",
    "section": "K-fold CV",
    "text": "K-fold CV\n\n\nShow/Hide Code\n# Data Import\nload(\"dataset\\\\train.Rdata\")\nload(\"dataset\\\\test.Rdata\")\n\n# Model Import\nload(\"dataset\\\\model\\\\nnet.model_kfoldCV.Rdata\")\n\nnnet.predictions &lt;- predict(nnet_model, newdata = test)\n\nconfusionMatrix(nnet.predictions, test$good)\n\n\nConfusion Matrix and Statistics\n\n          Reference\nPrediction   0   1\n         0 895 148\n         1  66  79\n                                          \n               Accuracy : 0.8199          \n                 95% CI : (0.7968, 0.8413)\n    No Information Rate : 0.8089          \n    P-Value [Acc &gt; NIR] : 0.1784          \n                                          \n                  Kappa : 0.324           \n                                          \n Mcnemar's Test P-Value : 3.076e-08       \n                                          \n            Sensitivity : 0.9313          \n            Specificity : 0.3480          \n         Pos Pred Value : 0.8581          \n         Neg Pred Value : 0.5448          \n             Prevalence : 0.8089          \n         Detection Rate : 0.7534          \n   Detection Prevalence : 0.8779          \n      Balanced Accuracy : 0.6397          \n                                          \n       'Positive' Class : 0               \n                                          \n\n\nShow/Hide Code\nnnet.predictions &lt;- as.numeric(nnet.predictions)\npred_obj &lt;- prediction(nnet.predictions, test$good)\nauc_val &lt;- performance(pred_obj, \"auc\")@y.values[[1]]\nauc_val\n\n\n[1] 0.6396696\n\n\nShow/Hide Code\nroc_obj &lt;- performance(pred_obj, \"tpr\", \"fpr\")\nplot(roc_obj, colorize = TRUE, lwd = 2,\n     xlab = \"False Positive Rate\", \n     ylab = \"True Positive Rate\",\n     main = \"ROC Curves from Repeated CV\")\npoints(auc_val, 1 - auc_val, \n       col = \"steelblue\", \n       pch = 21)\nabline(a = 0, b = 1)\n\n\nShow/Hide Code\nnnet.kfoldCV.ROC.plot &lt;- recordPlot()\n\n\n\nTuned\n\n\n\n\n\n\n\n\n\n\nResampling Method\nError Rate\nSensitivity\nSpecificity\nAUC\n\n\n\n\nNeural Network\n0.1801\n0.9313\n0.3480\n0.6396696\n\n\nNeural Network\n0.xxxx\n0.xxxx\n0.xxxx\n0.xxxxxxx"
  },
  {
    "objectID": "src/nnet.html#summary",
    "href": "src/nnet.html#summary",
    "title": "Neural Network",
    "section": "Summary",
    "text": "Summary\n\n\nShow/Hide Code\ncowplot::plot_grid(nb.kfoldCV.ROC.plot,\n                   dc.kfoldCV.ROC.plot,\n                   rf.kfoldCV.ROC.plot,\n                   bag.kfoldCV.ROC.plot, \n                   ncol = 2, align = \"hv\", scale = 0.8)\n\n\n\n\n\n\n\n\n\nShow/Hide Code\ncowplot::plot_grid(boost.kfoldCV.ROC.plot,\n                   svm.kfoldCV.ROC.plot,\n                   xgboost.kfoldCV.ROC.plot,\n                   nnet.kfoldCV.ROC.plot,\n                   ncol = 2, align = \"hv\", scale = 0.8)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nResampling Method\nError Rate\nSensitivity\nSpecificity\nAUC\n\n\n\n\nNaive Bayes\n0.2391\n0.7804\n0.6784\n0.7294256\n\n\nCART\n0.181\n0.9032\n0.4626\n0.6828904\n\n\nRandom Forest\n0.1540\n0.9501\n0.4053\n0.6776692\n\n\nBagging\n0.1658\n0.9178\n0.4802\n0.6989851\n\n\nboost\n0.1751\n0.9251\n0.4009\n0.6629796\n\n\nXGBoost\n0.1633\n0.9428\n0.3877\n0.6652166\n\n\nSVM\n0.1675\n0.9646\n0.2731\n0.6188740\n\n\nNeural Network\n0.1801\n0.9313\n0.3480\n0.6396696"
  },
  {
    "objectID": "src/qda.html",
    "href": "src/qda.html",
    "title": "Linear Discriminant Analysis",
    "section": "",
    "text": "Show/Hide Code\n#------------#\n#----QDA-----#\n#------------#\nset.seed(1234)\ntrain_control &lt;- trainControl(method = \"cv\", number = 10)\n\nset.seed(1234)\nqda_model &lt;- train(good ~ ., \n                   data = train, \n                   method = \"qda\", \n                   trControl = train_control)\n\nsave(qda_model, file = \"dataset\\\\model\\\\qda.model_kfoldCV.Rdata\")\n\n#------------------#\n#----QDA (Mod)-----#\n#------------------#"
  },
  {
    "objectID": "src/qda.html#model-construction",
    "href": "src/qda.html#model-construction",
    "title": "Linear Discriminant Analysis",
    "section": "",
    "text": "Show/Hide Code\n#------------#\n#----QDA-----#\n#------------#\nset.seed(1234)\ntrain_control &lt;- trainControl(method = \"cv\", number = 10)\n\nset.seed(1234)\nqda_model &lt;- train(good ~ ., \n                   data = train, \n                   method = \"qda\", \n                   trControl = train_control)\n\nsave(qda_model, file = \"dataset\\\\model\\\\qda.model_kfoldCV.Rdata\")\n\n#------------------#\n#----QDA (Mod)-----#\n#------------------#"
  },
  {
    "objectID": "src/qda.html#k-fold-cv",
    "href": "src/qda.html#k-fold-cv",
    "title": "Linear Discriminant Analysis",
    "section": "K-fold CV",
    "text": "K-fold CV\n\n\nShow/Hide Code\n# Data Import\nload(\"dataset\\\\train.Rdata\")\nload(\"dataset\\\\test.Rdata\")\n\n# Model Import\nload(\"dataset\\\\model\\\\qda.model_kfoldCV.Rdata\")\n\nqda.predictions &lt;- predict(qda_model, newdata = test)\n\nconfusionMatrix(qda.predictions, test$good)\n\n\nConfusion Matrix and Statistics\n\n          Reference\nPrediction   0   1\n         0 710  52\n         1 251 175\n                                          \n               Accuracy : 0.7449          \n                 95% CI : (0.7192, 0.7695)\n    No Information Rate : 0.8089          \n    P-Value [Acc &gt; NIR] : 1               \n                                          \n                  Kappa : 0.3819          \n                                          \n Mcnemar's Test P-Value : &lt;2e-16          \n                                          \n            Sensitivity : 0.7388          \n            Specificity : 0.7709          \n         Pos Pred Value : 0.9318          \n         Neg Pred Value : 0.4108          \n             Prevalence : 0.8089          \n         Detection Rate : 0.5976          \n   Detection Prevalence : 0.6414          \n      Balanced Accuracy : 0.7549          \n                                          \n       'Positive' Class : 0               \n                                          \n\n\nShow/Hide Code\nqda.predictions &lt;- as.numeric(qda.predictions)\npred_obj &lt;- prediction(qda.predictions, test$good)\nauc_val &lt;- performance(pred_obj, \"auc\")@y.values[[1]]\nauc_val\n\n\n[1] 0.7548694\n\n\nShow/Hide Code\nroc_obj &lt;- performance(pred_obj, \"tpr\", \"fpr\")\nplot(roc_obj, colorize = TRUE, lwd = 2,\n     xlab = \"False Positive Rate\", \n     ylab = \"True Positive Rate\",\n     main = \"ROC Curves from Repeated CV\")\npoints(auc_val, 1 - auc_val, \n       col = \"steelblue\", \n       pch = 21)\nabline(a = 0, b = 1)\n\n\nShow/Hide Code\nqda.kfoldCV.ROC.plot &lt;- recordPlot()\n\n\n\nTuned\n\n\n\n\n\n\n\n\n\n\nResampling Method\nError Rate\nSensitivity\nSpecificity\nAUC\n\n\n\n\nQDA\n0.2559\n0.7418\n0.7531\n0.7474858\n\n\nQDA (Tuned)\n0.xxxx\n0.xxxx\n0.xxxx\n0.xxxxxxx"
  },
  {
    "objectID": "src/qda.html#summary",
    "href": "src/qda.html#summary",
    "title": "Linear Discriminant Analysis",
    "section": "Summary",
    "text": "Summary\n\n\nShow/Hide Code\ncowplot::plot_grid(knn.kfoldCV.ROC.plot,\n                   logit.kfoldCV.ROC.plot,\n                   lda.kfoldCV.ROC.plot,\n                   qda.kfoldCV.ROC.plot, \n                   ncol = 2, align = \"hv\", scale = 0.8)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nResampling Method\nError Rate\nSensitivity\nSpecificity\nAUC\n\n\n\n\nLogistic Regression\n0.1944\n0.9347\n0.2929\n0.6137776\n\n\nLogistic Regression (Tuned)\n0.1919\n0.98946\n0.08787\n0.5386644\n\n\n\n\n\n\n\n\n\nLDA\n0.1919\n0.9283\n0.3305\n0.6294448\n\n\nLDA (Tuned)\n0.xxxx\n0.xxxx\n0.xxxx\n0.xxxxxxx\n\n\n\n\n\n\n\n\n\nQDA\n0.2559\n0.7418\n0.7531\n0.7474858\n\n\nQDA (Tuned)\n0.xxxx\n0.xxxx\n0.xxxx\n0.xxxxxxx"
  },
  {
    "objectID": "src/randomForest.html",
    "href": "src/randomForest.html",
    "title": "Random Forest (Classification)",
    "section": "",
    "text": "Show/Hide Code\n#----------------------#\n#----Random Forest-----#\n#----------------------#\nset.seed(1234)\ntrain_control &lt;- trainControl(method = \"cv\", number = 10)\n\nset.seed(1234)\nrf_model &lt;- train(good ~ ., \n               data = train, \n               method = \"rf\", \n               trControl = train_control)\n\nsave(rf_model, file = \"dataset\\\\model\\\\rf.model_kfoldCV.Rdata\")"
  },
  {
    "objectID": "src/randomForest.html#model-construction",
    "href": "src/randomForest.html#model-construction",
    "title": "Random Forest (Classification)",
    "section": "",
    "text": "Show/Hide Code\n#----------------------#\n#----Random Forest-----#\n#----------------------#\nset.seed(1234)\ntrain_control &lt;- trainControl(method = \"cv\", number = 10)\n\nset.seed(1234)\nrf_model &lt;- train(good ~ ., \n               data = train, \n               method = \"rf\", \n               trControl = train_control)\n\nsave(rf_model, file = \"dataset\\\\model\\\\rf.model_kfoldCV.Rdata\")"
  },
  {
    "objectID": "src/randomForest.html#k-fold-cv",
    "href": "src/randomForest.html#k-fold-cv",
    "title": "Random Forest (Classification)",
    "section": "K-fold CV",
    "text": "K-fold CV\n\n\nShow/Hide Code\n# Data Import\nload(\"dataset\\\\train.Rdata\")\nload(\"dataset\\\\test.Rdata\")\n\n# Model Import\nload(\"dataset\\\\model\\\\rf.model_kfoldCV.Rdata\")\n\nrf.predictions &lt;- predict(rf_model, newdata = test)\n\nconfusionMatrix(rf.predictions, test$good)\n\n\nConfusion Matrix and Statistics\n\n          Reference\nPrediction   0   1\n         0 913 135\n         1  48  92\n                                         \n               Accuracy : 0.846          \n                 95% CI : (0.8242, 0.866)\n    No Information Rate : 0.8089         \n    P-Value [Acc &gt; NIR] : 0.0005039      \n                                         \n                  Kappa : 0.4163         \n                                         \n Mcnemar's Test P-Value : 2.053e-10      \n                                         \n            Sensitivity : 0.9501         \n            Specificity : 0.4053         \n         Pos Pred Value : 0.8712         \n         Neg Pred Value : 0.6571         \n             Prevalence : 0.8089         \n         Detection Rate : 0.7685         \n   Detection Prevalence : 0.8822         \n      Balanced Accuracy : 0.6777         \n                                         \n       'Positive' Class : 0              \n                                         \n\n\nShow/Hide Code\nrf.predictions &lt;- as.numeric(rf.predictions)\npred_obj &lt;- prediction(rf.predictions, test$good)\nauc_val &lt;- performance(pred_obj, \"auc\")@y.values[[1]]\nauc_val\n\n\n[1] 0.6776692\n\n\nShow/Hide Code\nroc_obj &lt;- performance(pred_obj, \"tpr\", \"fpr\")\nplot(roc_obj, colorize = TRUE, lwd = 2,\n     xlab = \"False Positive Rate\", \n     ylab = \"True Positive Rate\",\n     main = \"ROC Curves from Repeated CV\")\npoints(auc_val, 1 - auc_val, \n       col = \"steelblue\", \n       pch = 21)\nabline(a = 0, b = 1)\n\n\nShow/Hide Code\nrf.kfoldCV.ROC.plot &lt;- recordPlot()\n\n\n\nTuned\n\n\n\n\n\n\n\n\n\n\nResampling Method\nError Rate\nSensitivity\nSpecificity\nAUC\n\n\n\n\nRandom Forest\n0.1540\n0.9501\n0.4053\n0.6776692\n\n\nRandom Forest (Tuned)\n0.xxxx\n0.xxxx\n0.xxxx\n0.xxxxxxx"
  },
  {
    "objectID": "src/svm.html",
    "href": "src/svm.html",
    "title": "Support Vectir Machine (SVM)",
    "section": "",
    "text": "Show/Hide Code\n#------------#\n#----SVM-----#\n#------------#\nset.seed(1234)\ntrain_control &lt;- trainControl(method = \"cv\", number = 10)\n\nset.seed(1234)\nsvm_model &lt;- train(good ~ ., \n               data = train, \n               method = \"svmRadial\", \n               trControl = train_control)\n\nsave(svm_model, file = \"dataset\\\\model\\\\svm.model_kfoldCV.Rdata\")"
  },
  {
    "objectID": "src/svm.html#model-construction",
    "href": "src/svm.html#model-construction",
    "title": "Support Vectir Machine (SVM)",
    "section": "",
    "text": "Show/Hide Code\n#------------#\n#----SVM-----#\n#------------#\nset.seed(1234)\ntrain_control &lt;- trainControl(method = \"cv\", number = 10)\n\nset.seed(1234)\nsvm_model &lt;- train(good ~ ., \n               data = train, \n               method = \"svmRadial\", \n               trControl = train_control)\n\nsave(svm_model, file = \"dataset\\\\model\\\\svm.model_kfoldCV.Rdata\")"
  },
  {
    "objectID": "src/svm.html#k-fold-cv",
    "href": "src/svm.html#k-fold-cv",
    "title": "Support Vectir Machine (SVM)",
    "section": "K-fold CV",
    "text": "K-fold CV\n\n\nShow/Hide Code\n# Data Import\nload(\"dataset\\\\train.Rdata\")\nload(\"dataset\\\\test.Rdata\")\n\n# Model Import\nload(\"dataset\\\\model\\\\svm.model_kfoldCV.Rdata\")\n\nsvm.predictions &lt;- predict(svm_model, newdata = test)\n\nconfusionMatrix(svm.predictions, test$good)\n\n\nConfusion Matrix and Statistics\n\n          Reference\nPrediction   0   1\n         0 927 165\n         1  34  62\n                                        \n               Accuracy : 0.8325        \n                 95% CI : (0.81, 0.8533)\n    No Information Rate : 0.8089        \n    P-Value [Acc &gt; NIR] : 0.01995       \n                                        \n                  Kappa : 0.305         \n                                        \n Mcnemar's Test P-Value : &lt; 2e-16       \n                                        \n            Sensitivity : 0.9646        \n            Specificity : 0.2731        \n         Pos Pred Value : 0.8489        \n         Neg Pred Value : 0.6458        \n             Prevalence : 0.8089        \n         Detection Rate : 0.7803        \n   Detection Prevalence : 0.9192        \n      Balanced Accuracy : 0.6189        \n                                        \n       'Positive' Class : 0             \n                                        \n\n\nShow/Hide Code\nsvm.predictions &lt;- as.numeric(svm.predictions)\npred_obj &lt;- prediction(svm.predictions, test$good)\nauc_val &lt;- performance(pred_obj, \"auc\")@y.values[[1]]\nauc_val\n\n\n[1] 0.618874\n\n\nShow/Hide Code\nroc_obj &lt;- performance(pred_obj, \"tpr\", \"fpr\")\nplot(roc_obj, colorize = TRUE, lwd = 2,\n     xlab = \"False Positive Rate\", \n     ylab = \"True Positive Rate\",\n     main = \"ROC Curves from Repeated CV\")\npoints(auc_val, 1 - auc_val, \n       col = \"steelblue\", \n       pch = 21)\nabline(a = 0, b = 1)\n\n\nShow/Hide Code\nsvm.kfoldCV.ROC.plot &lt;- recordPlot()\n\n\n\nTuned\n\n\n\n\n\n\n\n\n\n\nResampling Method\nError Rate\nSensitivity\nSpecificity\nAUC\n\n\n\n\nSVM\n0.1675\n0.9646\n0.2731\n0.618874\n\n\nSVM (Tuned)\n0.xxxx\n0.xxxx\n0.xxxx\n0.xxxxxxx"
  },
  {
    "objectID": "src/xgboost.html",
    "href": "src/xgboost.html",
    "title": "eXtreme Gradient Boosting (XGBoost)",
    "section": "",
    "text": "Show/Hide Code\n#----------------#\n#----XGBoost-----#\n#----------------#\nset.seed(1234)\ntrain_control &lt;- trainControl(method = \"cv\", number = 10)\n\nset.seed(1234)\nxgboost_model &lt;- train(good ~ ., \n                       data = train, \n                       method = \"xgbTree\",\n                       trControl = train_control,\n                       tuneGrid = expand.grid(nrounds = 100,\n                                              max_depth = 5,\n                                              eta = 0.05,\n                                              gamma = 0,\n                                              colsample_bytree = 0.5,\n                                              min_child_weight = 1,\n                                              subsample = 0.5),\n                       verbose = FALSE,\n                       metric = \"Accuracy\")\n\nsave(xgboost_model, file = \"dataset\\\\model\\\\xgboost.model_kfoldCV.Rdata\")"
  },
  {
    "objectID": "src/xgboost.html#model-construction",
    "href": "src/xgboost.html#model-construction",
    "title": "eXtreme Gradient Boosting (XGBoost)",
    "section": "",
    "text": "Show/Hide Code\n#----------------#\n#----XGBoost-----#\n#----------------#\nset.seed(1234)\ntrain_control &lt;- trainControl(method = \"cv\", number = 10)\n\nset.seed(1234)\nxgboost_model &lt;- train(good ~ ., \n                       data = train, \n                       method = \"xgbTree\",\n                       trControl = train_control,\n                       tuneGrid = expand.grid(nrounds = 100,\n                                              max_depth = 5,\n                                              eta = 0.05,\n                                              gamma = 0,\n                                              colsample_bytree = 0.5,\n                                              min_child_weight = 1,\n                                              subsample = 0.5),\n                       verbose = FALSE,\n                       metric = \"Accuracy\")\n\nsave(xgboost_model, file = \"dataset\\\\model\\\\xgboost.model_kfoldCV.Rdata\")"
  },
  {
    "objectID": "src/xgboost.html#k-fold-cv",
    "href": "src/xgboost.html#k-fold-cv",
    "title": "eXtreme Gradient Boosting (XGBoost)",
    "section": "K-fold CV",
    "text": "K-fold CV\n\n\nShow/Hide Code\n# Data Import\nload(\"dataset\\\\train.Rdata\")\nload(\"dataset\\\\test.Rdata\")\n\n# Model Import\nload(\"dataset\\\\model\\\\xgboost.model_kfoldCV.Rdata\")\n\nxgboost.predictions &lt;- predict(xgboost_model, newdata = test)\nxgboost.predictions &lt;- ifelse(xgboost.predictions == \"X1\", 1, 0)\nxgboost.predictions &lt;- factor(xgboost.predictions, levels = c(0, 1))\nconfusionMatrix(xgboost.predictions, test$good)\n\n\nConfusion Matrix and Statistics\n\n          Reference\nPrediction   0   1\n         0 906 139\n         1  55  88\n                                          \n               Accuracy : 0.8367          \n                 95% CI : (0.8144, 0.8573)\n    No Information Rate : 0.8089          \n    P-Value [Acc &gt; NIR] : 0.007397        \n                                          \n                  Kappa : 0.3848          \n                                          \n Mcnemar's Test P-Value : 2.537e-09       \n                                          \n            Sensitivity : 0.9428          \n            Specificity : 0.3877          \n         Pos Pred Value : 0.8670          \n         Neg Pred Value : 0.6154          \n             Prevalence : 0.8089          \n         Detection Rate : 0.7626          \n   Detection Prevalence : 0.8796          \n      Balanced Accuracy : 0.6652          \n                                          \n       'Positive' Class : 0               \n                                          \n\n\nShow/Hide Code\nxgboost.predictions &lt;- as.numeric(xgboost.predictions)\npred_obj &lt;- prediction(xgboost.predictions, test$good)\nauc_val &lt;- performance(pred_obj, \"auc\")@y.values[[1]]\nauc_val\n\n\n[1] 0.6652166\n\n\nShow/Hide Code\nroc_obj &lt;- performance(pred_obj, \"tpr\", \"fpr\")\nplot(roc_obj, colorize = TRUE, lwd = 2,\n     xlab = \"False Positive Rate\", \n     ylab = \"True Positive Rate\",\n     main = \"ROC Curves from Repeated CV\")\npoints(auc_val, 1 - auc_val, \n       col = \"steelblue\", \n       pch = 21)\nabline(a = 0, b = 1)\n\n\nShow/Hide Code\nxgboost.kfoldCV.ROC.plot &lt;- recordPlot()\n\n\n\nTuned"
  }
]