{
  "hash": "20acfb541b30ddaf3676141f08076801",
  "result": {
    "markdown": "---\ntitle: \"Introduction to Statistical Learning\"\nsubtitle: \"STAT 387\"\nfig-align: center\n---\n\n\n## Preamble\n\nWelcome ðŸ‘‹! This is Group 6's Final Project, and here is our detailed work\n\nConsider the wine quality dataset from [UCI Machine Learning Respository](https://archive.ics.uci.edu/ml/datasets/Wine+Quality)[^1]. We will focus only on the data concerning white wines (and not red wines). Dichotomize the `quality` variable as `good`, which takes the value 1 if quality â‰¥ 7 axnd the value 0, otherwise. We will take `good` as response and all the 11 physiochemical characteristics of the wines in the data as predictors.\n\n[^1]: P. Cortez, A. Cerdeira, F. Almeida, T. Matos and J. Reis. Modeling wine preferences by data mining from physicochemical properties. In Decision Support Systems, Elsevier, 47(4):547-553, 2009.\n\n### Problem Statements\n\nUse 10-fold cross-validation for estimating the test error rates below and compute the estimates using `caret` package with seed set to 1234 before each computation.\n\n(a) Fit a KNN with K chosen optimally using test error rate. Report error rate, sensitivity, specificity, and AUC for the optimal KNN based on the training data. Also, report its estimated test error rate.\n(b) Repeat (a) using logistic regression.\n(c) Repeat (a) using LDA.\n(d) Repeat (a) using QDA.\n(e) Compare the results in (a)-(d). Which classifier would you recommend? Justify your answer.\n\n### Methodologies\n\n-   KNN\n-   GLM/logit/glmnet\n-   LDA\n-   QDA\n-   Regression/Multi-Regression\n-   Naive Bayes\n-   Decision Tree\n-   Classification Tree\n-   Regression Tree\n-   Classification and Regression Trees (CART)\n-   Iterative Dichotomiser 3 (ID3)\n-   C4.5\n-   Random Forest\n-   Classification\n-   Regression\n-   Bagged\n-   Boosted\n-   Gradient Boosted Trees (GBT)\n-   Extremely Randomized Trees (ExtraTrees)\n-   SVM\n\n### Data Description\n\nThis is a dataset of wine quality containing 4898 observations of 12 variables. The variables are:\n\n-   `fixed.acidity`: The amount of fixed acid in the wine ($g/dm^3$)\n-   `volatile.acidity`: The amount of volatile acid in the wine ($g/dm^4$)\n-   `citric.acid`: The amount of citric acid in the wine ($g/dm^3$)\n-   `residual.sugar`: The amount of residual sugar in the wine ($g/dm^3$)\n-   `chlorides`: The amount of salt in the wine ($g/dm^3$)\n-   `free.sulfur.dioxide`: The amount of free sulfur dioxide in the wine ($mg/dm^3$)\n-   `total.sulfur.dioxide`: The amount of total sulfur dioxide in the wine ($mg/dm^3$)\n-   `density`: The density of the wine ($g/dm^3$)\n-   `pH`: The $pH$ value of the wine\n-   `sulphates`: The amount of sulphates in the wine ($g/dm^3$)\n-   `alcohol`: The alcohol content of the wine ($\\% vol$)\n-   `quality`: The quality score of the wine ($0-10$)\n\nAfter removing the duplicate rows from our data set, we are left with 3961 observations of the above 11 variables minus `quality` column variable, and introduced a new variable `good` as our response:\n\n-   `good`: A binary variable indicating whether the wine is good (`quality` $\\geq 7$) or not (`quality` $<7$).\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Import model\nload(\"dataset\\\\knn.model_kfoldCV.Rdata\")\n\n# Make predictions on the test data using the trained model and calculate the test error rate\nknn.predictions <- predict(knn_model, newdata = test)\n\nconfusionMatrix(knn.predictions, test$good)\n\n\n# Convert predictions to a numeric vector\nknn.predictions <- as.numeric(knn.predictions)\n\n# Calculate the AUC using the performance() and auc() functions:\npred_obj <- prediction(knn.predictions, test$good)\nauc_val <- performance(pred_obj, \"auc\")@y.values[[1]]\nauc_val\n\n# Performance plot for TP and FP\nroc_obj <- performance(pred_obj, \"tpr\", \"fpr\")\nplot(roc_obj, colorize = TRUE, lwd = 2,\n     xlab = \"False Positive Rate\", \n     ylab = \"True Positive Rate\",\n     main = \"ROC Curves from K-fold CV\")\npoints(auc_val, 1 - auc_val, \n       col = \"steelblue\", \n       pch = 21)\nabline(a = 0, b = 1)\nknn.kfoldCV.ROC.plot<- recordPlot()\n\n# Accuracy and Kappa value plot\nknn.accu.kappa.plot <- function(knn.model) {\n  p <- ggplot(data=data.frame(k = knn.model$results$k,\n                              Accuracy = knn.model$results$Accuracy,\n                              Kappa = knn.model$results$Kappa)) +\n    geom_point(aes(x = k, y = Accuracy, color = \"Accuracy\")) +\n    geom_point(aes(x = k, y = Kappa, color = \"Kappa\")) +\n    geom_line(aes(x = k, y = Accuracy, linetype = \"Accuracy\", color = \"Accuracy\")) +\n    geom_line(aes(x = k, y = Kappa, linetype = \"Kappa\", color = \"Kappa\")) +\n    scale_color_manual(values = c(\"#98c379\", \"#e06c75\"),\n                       guide = guide_legend(override.aes = list(linetype = c(1, 0)) )) +\n    scale_linetype_manual(values=c(\"solid\", \"dotted\"),\n                          guide = guide_legend(override.aes = list(color = c(\"#98c379\", \"#e06c75\")))) +\n    labs(x = \"K value\", \n         y = \"Accuracy / Kappa\") +\n    ylim(0, 1) +\n    theme_bw() +\n    theme(plot.title = element_text(hjust = 0.5)) +\n    guides(color = guide_legend(title = \"Metric\"),\n           linetype = guide_legend(title = \"Metric\"))\n  return(p)\n}\n\nknn.kfoldCV.plot <- knn.accu.kappa.plot(knn_model) + \n  geom_text(aes(x = k, y = Accuracy, label = round(Accuracy, 3)), vjust = -1) +\n  geom_text(aes(x = k, y = Kappa, label = round(Kappa, 3)), vjust = -1) +\n  ggtitle(\"KNN Model Performance (10-Fold CV)\")\n```\n:::\n\n\n### Reference\n",
    "supporting": [],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}