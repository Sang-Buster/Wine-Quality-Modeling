{
  "hash": "ec91c291947bd41f525bba180df29d4a",
  "result": {
    "markdown": "---\ntitle: \"Logistic Regression\"\n---\n\n\n\n\n## K-fold CV (`caret`)\n\n::: {.cell layout-align=\"TRUE\"}\n\n```{.r .cell-code}\n#---------------------------#\n#----Model Construction-----#\n#---------------------------#\nset.seed(1234)\n# Define the training control object for 10-fold cross-validation\ntrain_control <- trainControl(method = \"cv\", number = 10)\n\n# Train the logistic regression model using 10-fold cross-validation\nset.seed(1234)\nlogit_model <- train(good ~ ., \n                     data = train, \n                     method = \"glm\", \n                     family = \"binomial\",\n                     trControl = train_control)\n\nsave(logit_model, file = \"dataset\\\\logit.model_kfoldCV.Rdata\")\n```\n:::\n\n::: {.cell layout-align=\"TRUE\"}\n\n```{.r .cell-code}\n# Data Import\nload(\"dataset\\\\wine.data_cleaned.Rdata\")\nload(\"dataset\\\\train.Rdata\")\nload(\"dataset\\\\test.Rdata\")\n\n# Function Import\nload(\"dataset\\\\function\\\\accu.kappa.plot.Rdata\")\n\n# Model Import\nload(\"dataset\\\\model\\\\logit.model_kfoldCV.Rdata\")\n\nlogit.predictions <- predict(logit_model, newdata = test)\n\nconfusionMatrix(logit.predictions, test$good)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nConfusion Matrix and Statistics\n\n          Reference\nPrediction   0   1\n         0 896 148\n         1  65  79\n                                          \n               Accuracy : 0.8207          \n                 95% CI : (0.7977, 0.8421)\n    No Information Rate : 0.8089          \n    P-Value [Acc > NIR] : 0.1596          \n                                          \n                  Kappa : 0.3259          \n                                          \n Mcnemar's Test P-Value : 1.926e-08       \n                                          \n            Sensitivity : 0.9324          \n            Specificity : 0.3480          \n         Pos Pred Value : 0.8582          \n         Neg Pred Value : 0.5486          \n             Prevalence : 0.8089          \n         Detection Rate : 0.7542          \n   Detection Prevalence : 0.8788          \n      Balanced Accuracy : 0.6402          \n                                          \n       'Positive' Class : 0               \n                                          \n```\n:::\n\n```{.r .cell-code}\nlogit.predictions <- as.numeric(logit.predictions)\npred_obj <- prediction(logit.predictions, test$good)\n\n# Compute the RMSE and MAE\nRMSE <- caret::RMSE(as.numeric(unlist(pred_obj@predictions)), as.numeric(test$good))\nMAE <- caret::MAE(as.numeric(unlist(pred_obj@predictions)), as.numeric(test$good))\n\n# Compute AUC value\nauc_val  <- performance(pred_obj, \"auc\")@y.values[[1]]\nauc_val\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] 0.6401899\n```\n:::\n\n```{.r .cell-code}\nlog.perf <- performance(pred_obj, \"tpr\", \"fpr\")\nplot(log.perf, colorize = TRUE, lwd = 2,\n     xlab = \"False Positive Rate\", \n     ylab = \"True Positive Rate\",\n     main = \"caret::glm ROC Curves\")\nabline(a = 0, b = 1)\nx_values <- as.numeric(unlist(log.perf@x.values))\ny_values <- as.numeric(unlist(log.perf@y.values))\npolygon(x = x_values, y = y_values, \n        col = rgb(0.3803922, 0.6862745, 0.9372549, alpha = 0.3),\n        border = NA)\npolygon(x = c(0, 1, 1), y = c(0, 0, 1), \n        col = rgb(0.3803922, 0.6862745, 0.9372549, alpha = 0.3),\n        border = NA)\ntext(0.6, 0.4, paste(\"AUC =\", round(auc_val, 4)))\n```\n\n```{.r .cell-code}\nlogit.kfoldCV_caret.ROC.plot <- recordPlot()\n\npander::pander(data.frame(\"Accuracy\" = logit_model$results$Accuracy, \n                          \"RMSE\" = RMSE, \n                          \"MAE\" = MAE,\n                          \"Kappa\" = logit_model$results$Kappa), \n               caption = \"caret::glm Performance (10-fold CV)\")\n```\n\n::: {.cell-output-display}\n-------------------------------------\n Accuracy    RMSE     MAE     Kappa  \n---------- -------- -------- --------\n  0.8107    0.4234   0.1793   0.3351 \n-------------------------------------\n\nTable: caret::glm Performance (10-fold CV)\n:::\n:::\n\n\n\n## K-fold CV Tuned (`caret`)\n\n::: {.cell layout-align=\"TRUE\"}\n\n```{.r .cell-code}\nglm.model <- glm(good ~ ., data= train,family=\"binomial\")\nglm.fit= stepAIC(glm.model, direction = 'backward')\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nStart:  AIC=2263\ngood ~ fixed.acidity + volatile.acidity + citric.acid + residual.sugar + \n    chlorides + free.sulfur.dioxide + total.sulfur.dioxide + \n    density + pH + sulphates + alcohol\n\n                       Df Deviance    AIC\n- citric.acid           1   2240.1 2262.1\n<none>                      2239.0 2263.0\n- alcohol               1   2241.3 2263.3\n- total.sulfur.dioxide  1   2243.0 2265.0\n- chlorides             1   2252.2 2274.2\n- volatile.acidity      1   2254.3 2276.3\n- sulphates             1   2256.6 2278.6\n- free.sulfur.dioxide   1   2258.1 2280.1\n- fixed.acidity         1   2258.6 2280.6\n- density               1   2263.2 2285.2\n- residual.sugar        1   2266.6 2288.6\n- pH                    1   2295.8 2317.8\n\nStep:  AIC=2262.05\ngood ~ fixed.acidity + volatile.acidity + residual.sugar + chlorides + \n    free.sulfur.dioxide + total.sulfur.dioxide + density + pH + \n    sulphates + alcohol\n\n                       Df Deviance    AIC\n<none>                      2240.1 2262.1\n- alcohol               1   2242.7 2262.7\n- total.sulfur.dioxide  1   2243.7 2263.7\n- chlorides             1   2252.8 2272.8\n- sulphates             1   2257.9 2277.9\n- volatile.acidity      1   2258.4 2278.4\n- free.sulfur.dioxide   1   2258.8 2278.8\n- fixed.acidity         1   2261.2 2281.2\n- density               1   2263.7 2283.7\n- residual.sugar        1   2267.2 2287.2\n- pH                    1   2296.2 2316.2\n```\n:::\n\n```{.r .cell-code}\n# Make predictions on test data and construct a confusion matrix\nlogit.predictions <- predict(glm.fit, newdata = test,type = \"response\")\nlogit.predictions <- factor(ifelse(logit.predictions > 0.7, 1, 0),\n                            levels = c(0, 1))\nconfusionMatrix(logit.predictions, test$good)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nConfusion Matrix and Statistics\n\n          Reference\nPrediction   0   1\n         0 954 207\n         1   7  20\n                                          \n               Accuracy : 0.8199          \n                 95% CI : (0.7968, 0.8413)\n    No Information Rate : 0.8089          \n    P-Value [Acc > NIR] : 0.1784          \n                                          \n                  Kappa : 0.1218          \n                                          \n Mcnemar's Test P-Value : <2e-16          \n                                          \n            Sensitivity : 0.99272         \n            Specificity : 0.08811         \n         Pos Pred Value : 0.82171         \n         Neg Pred Value : 0.74074         \n             Prevalence : 0.80892         \n         Detection Rate : 0.80303         \n   Detection Prevalence : 0.97727         \n      Balanced Accuracy : 0.54041         \n                                          \n       'Positive' Class : 0               \n                                          \n```\n:::\n\n```{.r .cell-code}\nAccuracy <- confusionMatrix(logit.predictions, test$good)$overall[[1]]\nKappa <- confusionMatrix(logit.predictions, test$good)$overall[[2]] \n\nlogit.predictions <- as.numeric(logit.predictions)\npred_obj <- prediction(logit.predictions, test$good)\n\n# Compute the RMSE and MAE\nRMSE <- caret::RMSE(as.numeric(unlist(pred_obj@predictions)), as.numeric(test$good))\nMAE <- caret::MAE(as.numeric(unlist(pred_obj@predictions)), as.numeric(test$good))\n\n# Compute AUC value\nauc_val <- performance(pred_obj, \"auc\")@y.values[[1]]\nauc_val\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] 0.5404108\n```\n:::\n\n```{.r .cell-code}\nlog.perf <- performance(pred_obj, \"tpr\", \"fpr\")\nplot(log.perf, colorize = TRUE, lwd = 2,\n     xlab = \"False Positive Rate\", \n     ylab = \"True Positive Rate\",\n     main = \"caret::glm ROC Curves with stepAIC\")\nabline(a = 0, b = 1)\nx_values <- as.numeric(unlist(log.perf@x.values))\ny_values <- as.numeric(unlist(log.perf@y.values))\npolygon(x = x_values, y = y_values, \n        col = rgb(0.3803922, 0.6862745, 0.9372549, alpha = 0.3),\n        border = NA)\npolygon(x = c(0, 1, 1), y = c(0, 0, 1), \n        col = rgb(0.3803922, 0.6862745, 0.9372549, alpha = 0.3),\n        border = NA)\ntext(0.6, 0.4, paste(\"AUC =\", round(auc_val, 4)))\n```\n\n```{.r .cell-code}\nlogit.kfoldCV_caret_tuned.ROC.plot <- recordPlot()\n\npander::pander(data.frame(\"Accuracy\" = Accuracy,\n                          \"RMSE\" = RMSE, \n                          \"MAE\" = MAE,\n                          \"Kappa\" = Kappa),\n               caption = \"caret::glm Performance (10-fold CV with stepAIC)\")\n```\n\n::: {.cell-output-display}\n-------------------------------------\n Accuracy    RMSE     MAE     Kappa  \n---------- -------- -------- --------\n  0.8199    0.4244   0.1801   0.1218 \n-------------------------------------\n\nTable: caret::glm Performance (10-fold CV with stepAIC)\n:::\n:::\n\n\n## K-fold CV (`MASS`)\n\n::: {.cell layout-align=\"TRUE\"}\n\n```{.r .cell-code}\n# Set the number of folds\nk <- 10\n\n# Randomly assign each row in the data to a fold\nset.seed(1234) # for reproducibility\nfold_indices <- sample(rep(1:k, length.out = nrow(wine.data_cleaned)))\n\n# Initialize an empty list to store the folds\nfolds <- vector(\"list\", k)\n\n# Assign each row to a fold\nfor (i in 1:k) {\n  folds[[i]] <- which(fold_indices == i)\n}\n\n#To store the error rate of each fold\nerror_rate <- numeric(k)\nrmse <- numeric(k)\nmae <- numeric(k)\nkappa <- numeric(k)\nconfusion_matrices <- vector(\"list\", k)\n\n# Loop through each fold\nfor (i in 1:k) {\n  # Extract the i-th fold as the testing set\n  test_indices <- unlist(folds[[i]])\n  \n  test <- wine.data_cleaned[test_indices, ]\n  train <- wine.data_cleaned[-test_indices, ]\n  \n  # Fit the model on the training set\n  logit_model <- glm(good ~ ., data = train, family = binomial)\n  \n  # Make predictions on the testing set and calculate the error rate\n  log.pred <- predict(logit_model, newdata = test, type = \"response\")\n  predicted_classes <- as.numeric(ifelse(log.pred > 0.7, 1, 0))\n  \n  # Compute RMSE\n  rmse[i] <- sqrt(mean((predicted_classes - test$good) ^ 2))\n  \n  # Compute MAE\n  mae[i] <- mean(abs(predicted_classes - test$good))\n  \n  # Compute MAE\n  error_rate[i] <- mean((predicted_classes> 0.7) != test$good)\n  \n  # Compute confusion matrix\n  test$good <- as.factor(test$good)\n  predicted_classes <- factor(ifelse(log.pred > 0.7, 1, 0), levels = c(0, 1))\n  confusion_matrices[[i]] <- caret::confusionMatrix(predicted_classes, test$good)\n  \n  # Compute Kappa value\n  kappa[i] <- confusion_matrices[[i]]$overall[[2]]\n  \n  # Print the error rates for each fold\n  cat(paste0(\"Fold \", i, \": \", \"OER:\", error_rate[i], \" RMSE:\", rmse[i], \" MAE:\", mae[i], \"\\n\"))\n}\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nFold 1: OER:0.198992443324937 RMSE:0.446085690562853 MAE:0.198992443324937\nFold 2: OER:0.181818181818182 RMSE:0.426401432711221 MAE:0.181818181818182\nFold 3: OER:0.20959595959596 RMSE:0.457816513022367 MAE:0.20959595959596\nFold 4: OER:0.247474747474747 RMSE:0.497468338163091 MAE:0.247474747474747\nFold 5: OER:0.174242424242424 RMSE:0.417423554968361 MAE:0.174242424242424\nFold 6: OER:0.22979797979798 RMSE:0.479372485441102 MAE:0.22979797979798\nFold 7: OER:0.184343434343434 RMSE:0.429352342888023 MAE:0.184343434343434\nFold 8: OER:0.196969696969697 RMSE:0.443812682299297 MAE:0.196969696969697\nFold 9: OER:0.161616161616162 RMSE:0.402015126103685 MAE:0.161616161616162\nFold 10: OER:0.179292929292929 RMSE:0.423429957954004 MAE:0.179292929292929\n```\n:::\n\n```{.r .cell-code}\nbest_confmat_index <- which.min(error_rate)\nbest_confmat_index\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] 9\n```\n:::\n\n```{.r .cell-code}\nbest_confmat_indexi <- which.min(rmse)\nbest_confmat_index\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] 9\n```\n:::\n\n```{.r .cell-code}\nbest_confmat_index <- which.min(mae)\nbest_confmat_index\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] 9\n```\n:::\n\n```{.r .cell-code}\nconfusion_matrices[best_confmat_index]\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[[1]]\nConfusion Matrix and Statistics\n\n          Reference\nPrediction   0   1\n         0 326  64\n         1   0   6\n                                          \n               Accuracy : 0.8384          \n                 95% CI : (0.7984, 0.8733)\n    No Information Rate : 0.8232          \n    P-Value [Acc > NIR] : 0.2365          \n                                          \n                  Kappa : 0.1337          \n                                          \n Mcnemar's Test P-Value : 3.407e-15       \n                                          \n            Sensitivity : 1.00000         \n            Specificity : 0.08571         \n         Pos Pred Value : 0.83590         \n         Neg Pred Value : 1.00000         \n             Prevalence : 0.82323         \n         Detection Rate : 0.82323         \n   Detection Prevalence : 0.98485         \n      Balanced Accuracy : 0.54286         \n                                          \n       'Positive' Class : 0               \n                                          \n```\n:::\n\n```{.r .cell-code}\n#AUC and Performance Plot\npredicted_classes <- as.numeric(predicted_classes)\npred_obj <- prediction(predicted_classes, test$good)\nauc_val  <- performance(pred_obj, \"auc\")@y.values[[1]]\nlog.perf <- performance(pred_obj,\"tpr\",\"fpr\")\nauc_val  <- performance(pred_obj, \"auc\")@y.values[[1]]\nauc_val\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] 0.5438871\n```\n:::\n\n```{.r .cell-code}\nplot(log.perf, colorize = TRUE, lwd = 2,\n     xlab = \"False Positive Rate\", \n     ylab = \"True Positive Rate\",\n     main = \"MASS::glm ROC Curves\")\nabline(a = 0, b = 1)\nx_values <- as.numeric(unlist(log.perf@x.values))\ny_values <- as.numeric(unlist(log.perf@y.values))\npolygon(x = x_values, y = y_values, \n        col = rgb(0.3803922, 0.6862745, 0.9372549, alpha = 0.3),\n        border = NA)\npolygon(x = c(0, 1, 1), y = c(0, 0, 1), \n        col = rgb(0.3803922, 0.6862745, 0.9372549, alpha = 0.3),\n        border = NA)\ntext(0.6, 0.4, paste(\"AUC =\", round(auc_val, 4)))\n```\n\n```{.r .cell-code}\nlogit.kfoldCV_MASS.ROC.plot <- recordPlot()\n\nlogit_df <- data.frame(k = 1:k,\n                       Accuracy = 1-error_rate, \n                       Kappa = kappa)\n\nlogit.kfoldCV_MASS.plot <- accu.kappa.plot(logit_df) + \n  geom_text(aes(x = k, y = Accuracy, label = round(Accuracy, 3)), vjust = -1) +\n  geom_text(aes(x = k, y = Kappa, label = round(Kappa, 3)), vjust = -1) +\n  ggtitle(\"MASS::glm Model Performance (10-Fold CV)\")\n```\n:::\n\n\n\n## Hold-out CV (`MASS`)\n\n::: {.cell layout-align=\"TRUE\"}\n\n```{.r .cell-code}\n# Set the seed for reproducibility\nset.seed(1234)\n\n# Proportion of data to use for training\ntrain_prop <- 0.7\n\n# Split the data into training and testing sets\ntrain_indices <- sample(seq_len(nrow(wine.data_cleaned)), size = round(train_prop * nrow(wine.data_cleaned)), replace = FALSE)\ntrain <- wine.data_cleaned[train_indices, ]\ntest <- wine.data_cleaned[-train_indices, ]\n\n# Fit the model on the training set\nlogit_model <- glm(good ~ ., data = train, family = binomial)\n\n# Make predictions on the testing set and calculate the error rate\nlog.pred <- predict(logit_model, newdata = test, type = \"response\")\npredicted_classes <- as.numeric(ifelse(log.pred > 0.7, 1, 0))\n\n# Compute RMSE\nrmse <- sqrt(mean((predicted_classes - test$good) ^ 2))\n\n# Compute MAE\nmae <- mean(abs(predicted_classes - test$good))\n\n# Compute error rate\nerror_rate <- mean((predicted_classes > 0.7) != test$good)\n\n# Calculate the accuracy of the predictions on the testing set\ntrain$good <- as.numeric(train$good)\ntest$good <- as.factor(test$good)\npredicted_classes <- factor(ifelse(log.pred > 0.7, 1, 0), levels = c(0, 1))\nconfusionMatrix(predicted_classes, test$good)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nConfusion Matrix and Statistics\n\n          Reference\nPrediction   0   1\n         0 938 214\n         1  11  25\n                                          \n               Accuracy : 0.8106          \n                 95% CI : (0.7871, 0.8325)\n    No Information Rate : 0.7988          \n    P-Value [Acc > NIR] : 0.1643          \n                                          \n                  Kappa : 0.1363          \n                                          \n Mcnemar's Test P-Value : <2e-16          \n                                          \n            Sensitivity : 0.9884          \n            Specificity : 0.1046          \n         Pos Pred Value : 0.8142          \n         Neg Pred Value : 0.6944          \n             Prevalence : 0.7988          \n         Detection Rate : 0.7896          \n   Detection Prevalence : 0.9697          \n      Balanced Accuracy : 0.5465          \n                                          \n       'Positive' Class : 0               \n                                          \n```\n:::\n\n```{.r .cell-code}\nkappa <- confusionMatrix(predicted_classes, test$good)$overall[[2]]\n\n#AUC and Performance Plot\npredicted_classes <- as.numeric(predicted_classes)\npred_obj <- prediction(predicted_classes, test$good)\nlog.perf <- performance(pred_obj,\"tpr\",\"fpr\")\nauc_val <- performance(pred_obj, \"auc\")@y.values[[1]]\nauc_val\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] 0.5465057\n```\n:::\n\n```{.r .cell-code}\nplot(log.perf, colorize = TRUE, lwd = 2,\n     xlab = \"False Positive Rate\", \n     ylab = \"True Positive Rate\",\n     main = \"MASS::glm ROC Curves with Hold-out CV\")\nabline(a = 0, b = 1)\nx_values <- as.numeric(unlist(log.perf@x.values))\ny_values <- as.numeric(unlist(log.perf@y.values))\npolygon(x = x_values, y = y_values, \n        col = rgb(0.3803922, 0.6862745, 0.9372549, alpha = 0.3),\n        border = NA)\npolygon(x = c(0, 1, 1), y = c(0, 0, 1), \n        col = rgb(0.3803922, 0.6862745, 0.9372549, alpha = 0.3),\n        border = NA)\ntext(0.6, 0.4, paste(\"AUC =\", round(auc_val, 4)))\n```\n\n```{.r .cell-code}\nlogit.holdoutCV_MASS.ROC.plot <- recordPlot()\n\npander::pander(data.frame(\"Accuracy\" = 1 - error_rate, \n                          \"RMSE\" = rmse, \n                          \"MAE\" = mae,\n                          \"Kappa\" = kappa))\n```\n\n::: {.cell-output-display}\n-------------------------------------\n Accuracy    RMSE     MAE     Kappa  \n---------- -------- -------- --------\n  0.8106    0.4352   0.1894   0.1363 \n-------------------------------------\n:::\n:::\n\n\n## Summary\n\n::: {.cell layout-align=\"TRUE\"}\n\n```{.r .cell-code}\ncowplot::plot_grid(logit.kfoldCV_caret.ROC.plot,\n                   logit.kfoldCV_caret_tuned.ROC.plot,\n                   logit.kfoldCV_MASS.ROC.plot,\n                   logit.holdoutCV_MASS.ROC.plot,\n                   ncol = 2, align = \"hv\", scale = 0.8)\n```\n\n::: {.cell-output-display}\n![](logit_files/figure-html/logit.ROC_curve-1.png){fig-align='TRUE' width=1440}\n:::\n:::\n\n\n\n| Resampling Method                                | Error Rate | Sensitivity | Specificity | AUC       |\n| ------------------------------------------------ | ---------- | ----------- | ----------- | --------- |\n| Logistic Regression (`caret`)                    | 0.1793     | 0.9324      | 0.3480      | 0.6401899 |\n| Logistic Regression (`caret` tuned with stepAIC) | 0.1801     | 0.9927      | 0.0881      | 0.5404108 |\n| Logistic Regression (`MASS` 10-fold CV)          | 0.1616     | 1.0000      | 0.0857      | 0.5438871 |\n| Logistic Regression (`MASS` Hold-out CV)         | 0.1894     | 0.9884      | 0.1046      | 0.5465057 |\n\n\n::: {.cell layout-align=\"TRUE\"}\n\n:::\n",
    "supporting": [
      "logit_files"
    ],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}